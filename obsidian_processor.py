import os
import re
import PyPDF2
import markdown
import json
import yaml  # Added PyYAML for better frontmatter parsing
import psutil
import colorama
from colorama import Fore, Style
import time
import threading
import gc
import sys
from datetime import datetime
from bs4 import BeautifulSoup
import config
from embeddings import EmbeddingModel
from tqdm import tqdm
from functools import lru_cache
from progress_monitor_cmd import ProgressMonitor

# Windowsì—ì„œ ìƒ‰ìƒ í‘œì‹œë¥¼ ìœ„í•œ colorama ì´ˆê¸°í™”
colorama.init()

class ObsidianProcessor:
    def __init__(self, milvus_manager):
        self.vault_path = config.OBSIDIAN_VAULT_PATH
        self.milvus_manager = milvus_manager
        self.embedding_model = EmbeddingModel()
        self.next_id = self._get_next_id()
        
        # GPU ì‚¬ìš© ì„¤ì •
        self.use_gpu = config.USE_GPU
        self.device_idx = config.GPU_DEVICE_ID if hasattr(config, 'GPU_DEVICE_ID') else 0
        
        # ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ ë‹¨ìœ„)
        self.processing_timeout = 300  # ê¸°ë³¸ê°’: 5ë¶„
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜
        self.embedding_in_progress = False
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": None,
            "current_file": "",
            "estimated_time_remaining": "",
            "percentage": 0,
            "is_full_reindex": False,
            "cpu_percent": 0,
            "memory_percent": 0,
            "gpu_percent": 0,
            "gpu_memory_used": 0,
            "gpu_memory_total": 0,
            "current_batch_size": 0
        }
        
        # ENHANCED: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì œí•œ ë° ë™ì  ìµœì í™” ì„¤ì •
        self.max_cpu_percent = 85
        self.max_memory_percent = 80
        self.resource_check_interval = 2
        self.last_resource_check = 0
        
        # ë™ì  ë°°ì¹˜ í¬ê¸°: embedding_modelì—ì„œ ìµœì ê°’ ê°€ì ¸ì˜¤ê¸°
        try:
            if hasattr(self.embedding_model, 'batch_optimizer'):
                self.dynamic_batch_size = self.embedding_model.batch_optimizer.current_batch_size
                self.min_batch_size = self.embedding_model.batch_optimizer.min_batch_size
                self.max_batch_size = self.embedding_model.batch_optimizer.max_batch_size
                print(f"ğŸš€ Using optimized batch sizes from embedding model: {self.dynamic_batch_size} (range: {self.min_batch_size}-{self.max_batch_size})")
            else:
                # í´ë°±: ê¸°ë³¸ ì„¤ì •
                self.dynamic_batch_size = getattr(config, 'BATCH_SIZE', 32)
                self.min_batch_size = max(1, self.dynamic_batch_size // 2)
                self.max_batch_size = self.dynamic_batch_size * 4
                print(f"âš ï¸ Using fallback batch sizes: {self.dynamic_batch_size} (range: {self.min_batch_size}-{self.max_batch_size})")
        except Exception as e:
            print(f"Error initializing dynamic batch sizes: {e}")
            # ì•ˆì „ í´ë°±
            self.dynamic_batch_size = 32
            self.min_batch_size = 8
            self.max_batch_size = 128
        
        # ì§„í–‰ë¥  ë° ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì ìƒì„±
        self.monitor = ProgressMonitor(self)
        
        # OPTIMIZATION: Session cache for verification results
        self.verification_cache = {}
        
        # OPTIMIZATION: Performance thresholds for smart decision making
        self.FAST_SKIP_THRESHOLD = 0.1  # Files with time diff < 0.1s are very likely unchanged
        self.FAST_PROCESS_THRESHOLD = 2.0  # Files with time diff > 2.0s are definitely changed
        
    def _get_next_id(self):
        """ë‹¤ìŒ ID ê°’ ê°€ì ¸ì˜¤ê¸°"""
        results = self.milvus_manager.query("id >= 0", output_fields=["id"], limit=1)
        if not results:
            return 1
        return max([r['id'] for r in results]) + 1
        
    def _create_ascii_bar(self, percent, width=20):
        """í¼ì„¼íŠ¸ ê°’ì„ ë°›ì•„ ASCII ê·¸ë˜í”„ ë°” ìƒì„±"""
        # ê°’ì´ ìœ íš¨í•œ ë²”ìœ„ì¸ì§€ í™•ì¸
        if not isinstance(percent, (int, float)) or percent < 0:
            percent = 0
        elif percent > 100:
            percent = 100
            
        # ì±„ì›Œì§ˆ ê¸¸ì´ ê³„ì‚° (ë°˜ì˜¬ë¦¼í•˜ì—¬ ìµœì†Œ 1ì¹¸ì€ í‘œì‹œ)
        filled_length = max(1, int(width * percent / 100)) if percent > 0 else 0
        
        # ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë‹¤ë¥¸ ë¬¸ì ì‚¬ìš©
        if percent > 90:
            bar_char = '#'  # ë§¤ìš° ë†’ìŒ
        elif percent > 70:
            bar_char = '='  # ë†’ìŒ
        elif percent > 50:
            bar_char = '-'  # ì¤‘ê°„
        elif percent > 30:
            bar_char = '.'  # ë‚®ìŒ
        elif percent > 0:
            bar_char = 'Â·'  # ë§¤ìš° ë‚®ìŒ
        else:
            bar_char = ' '  # 0%
        
        # ê·¸ë˜í”„ ë°” ìƒì„± (ìµœëŒ€ ê¸¸ì´ ì œí•œ)
        filled_length = min(filled_length, width)
        bar = bar_char * filled_length + ' ' * (width - filled_length)
        return bar
        
    def _check_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ (ENHANCED)"""
        current_time = time.time()
        
        if current_time - self.last_resource_check < self.resource_check_interval:
            return self.dynamic_batch_size
            
        self.last_resource_check = current_time
        
        try:
            # ENHANCED: embedding_modelì˜ ë™ì  ë°°ì¹˜ ìµœì í™” ì‚¬ìš©
            if hasattr(self.embedding_model, 'system_monitor'):
                system_status = self.embedding_model.system_monitor.get_system_status()
                memory_percent = system_status.get('memory_percent', 50)
                cpu_percent = system_status.get('cpu_percent', 50)
                gpu_percent = system_status.get('gpu_percent', 0)
                
                # embedding_modelì˜ batch_optimizerë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •
                if hasattr(self.embedding_model, 'batch_optimizer'):
                    optimal_batch = self.embedding_model.batch_optimizer.adjust_batch_size({
                        'memory_percent': memory_percent,
                        'cpu_percent': cpu_percent,
                        'gpu_percent': gpu_percent,
                        'processing_time': 1.0
                    })
                    
                    # ë™ì  ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
                    self.dynamic_batch_size = optimal_batch
                    
                    # ì§„í–‰ë¥  ì •ë³´ ì—…ë°ì´íŠ¸
                    self.embedding_progress["current_batch_size"] = self.dynamic_batch_size
                    
                    print(f"ğŸ“ˆ Dynamic batch size adjusted to: {self.dynamic_batch_size} (Memory: {memory_percent:.1f}%, GPU: {gpu_percent:.1f}%)")
            
            # ProgressMonitor ì—…ë°ì´íŠ¸
            if hasattr(self.monitor, '_update_system_resources'):
                self.monitor._update_system_resources()
                
        except Exception as e:
            print(f"Error in enhanced system resource check: {e}")
        
        return self.dynamic_batch_size
        
    def _update_progress_stats(self):
        """ì„ë² ë”© ì§„í–‰ë¥ ê³¼ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ë©”ì†Œë“œ (íŒŒì¼ í¬ê¸° ê¸°ë°˜)"""
        if not self.embedding_in_progress:
            return
            
        # ì§„í–‰ë¥  ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
        total_size = self.embedding_progress["total_size"]
        processed_size = self.embedding_progress["processed_size"]
        
        # íŒŒì¼ ê°œìˆ˜ë„ í•¨ê»˜ í‘œì‹œí•˜ê¸° ìœ„í•´ ìœ ì§€
        total_files = self.embedding_progress["total_files"]
        processed_files = self.embedding_progress["processed_files"]
        
        if total_size <= 0:
            self.embedding_progress["percentage"] = 0
            self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
            return
            
        # ì§„í–‰ë„ëŠ” íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        if total_size > 0:
            percentage = min(99, int((processed_size / total_size) * 100))  # 100%ëŠ” ì™„ì „íˆ ì™„ë£Œë˜ì—ˆì„ ë•Œë§Œ í‘œì‹œ
            self.embedding_progress["percentage"] = percentage
        
        # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
        if processed_size > 0 and self.embedding_progress["start_time"] is not None:
            elapsed_time = time.time() - self.embedding_progress["start_time"]
            bytes_per_second = processed_size / elapsed_time if elapsed_time > 0 else 0
            
            if bytes_per_second > 0:
                remaining_size = total_size - processed_size
                remaining_seconds = remaining_size / bytes_per_second
                
                # ì˜ˆìƒ ì‹œê°„ í¬ë§·íŒ…
                if remaining_seconds < 60:
                    time_str = f"{int(remaining_seconds)}ì´ˆ"
                elif remaining_seconds < 3600:
                    minutes = int(remaining_seconds / 60)
                    seconds = int(remaining_seconds % 60)
                    time_str = f"{minutes}ë¶„ {seconds}ì´ˆ"
                else:
                    hours = int(remaining_seconds / 3600)
                    minutes = int((remaining_seconds % 3600) / 60)
                    time_str = f"{hours}ì‹œê°„ {minutes}ë¶„"
                    
                self.embedding_progress["estimated_time_remaining"] = time_str
            else:
                self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
        else:
            self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            memory_percent = memory_info.percent
            
            self.embedding_progress["cpu_percent"] = cpu_percent
            self.embedding_progress["memory_percent"] = memory_percent
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¬´ì‹œ
            pass
        
    def start_monitoring(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitor.start()
        
    def stop_monitoring(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitor.stop()
    
    def process_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ë° ìƒ‰ì¸ (ìµœì í™” ë° ì•ˆì „ì¥ì¹˜ ì¶”ê°€)"""
        if not os.path.exists(file_path):
            error_msg = f"Error: File not found: {file_path}"
            print(error_msg)
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return False
            
        file_size = os.path.getsize(file_path)
        file_name = os.path.basename(file_path)
        
        # ì¶œë ¥ ì¤„ì´ê¸° - ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì„
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.embedding_in_progress = True
        
        # í˜„ì¬ íŒŒì¼ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ê¸°ì¡´ ì´ íŒŒì¼ ìˆ˜/í¬ê¸° ìœ ì§€
        current_progress = self.embedding_progress.copy()
        
        # í˜„ì¬ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì§„í–‰ ì •ë³´ ì´ˆê¸°í™” (ì „ì²´ ì§„í–‰ë¥ ì€ ìœ ì§€)
        self.embedding_progress = {
            "total_files": current_progress.get("total_files", 1), # ê¸°ì¡´ ì´ íŒŒì¼ ìˆ˜ ìœ ì§€
            "processed_files": current_progress.get("processed_files", 0), # ê¸°ì¡´ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ ìœ ì§€
            "total_size": current_progress.get("total_size", file_size), # ê¸°ì¡´ ì´ í¬ê¸° ìœ ì§€
            "processed_size": current_progress.get("processed_size", 0), # ê¸°ì¡´ ì²˜ë¦¬ëœ í¬ê¸° ìœ ì§€
            "start_time": current_progress.get("start_time", time.time()),
            "current_file": file_name,
            "estimated_time_remaining": current_progress.get("estimated_time_remaining", "Calculating..."),
            "percentage": current_progress.get("percentage", 0),
            "is_full_reindex": current_progress.get("is_full_reindex", False),
            "cpu_percent": current_progress.get("cpu_percent", 0),
            "memory_percent": current_progress.get("memory_percent", 0),
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì „ì—­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        processing_completed = threading.Event()
        processing_result = {"success": False}
        
        def process_with_timeout():
            try:
                # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                self.start_monitoring()
                
                try:
                    # íŒŒì¼ì—ì„œ ì²­í¬ ì¶”ì¶œ
                    chunks, metadata = self._extract_chunks_from_file(file_path)
                    if not chunks or not metadata:
                        # ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì´ë¯€ë¡œ ì¶œë ¥ ì¤„ì„
                        processing_result["success"] = False
                        self.monitor.last_processed_status = f"{Fore.RED}Fail{Fore.RESET}"
                        return
                    
                    # ì²­í¬ ì¶”ì¶œ ì„±ê³µ ë©”ì‹œì§€ ì œê±° - ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì„
                    
                    # ì„ë² ë”© ì§„í–‰ ì •ë³´ ì—…ë°ì´íŠ¸
                    self.embedding_progress["current_file"] = file_name
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    self._check_memory_usage("Before embedding generation")
                    
                    # ENHANCED: ì²­í¬ì— ëŒ€í•œ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì†ë„ ëŒ€í­ ê°œì„ !)
                    print(f"ğŸš€ Processing {len(chunks)} chunks with FORCED batch embedding...")
                    
                    # STEP 1: ë°°ì¹˜ í¬ê¸° í™•ì¸ ë° ìµœì í™”
                    optimal_batch_size = self._check_system_resources()
                    if hasattr(self.embedding_model, 'batch_optimizer'):
                        current_batch_size = self.embedding_model.batch_optimizer.current_batch_size
                        print(f"ğŸ“¦ Current optimal batch size: {current_batch_size}")
                    
                    vectors = []
                    batch_success = False
                    
                    try:
                        # STEP 2: ê°•ì œ ë°°ì¹˜ ì²˜ë¦¬ (í´ë°± ì—†ì´)
                        print(f"ğŸ”¥ FORCING batch processing for {len(chunks)} chunks...")
                        start_time = time.time()
                        
                        # ë°°ì¹˜ ì²˜ë¦¬ ê°•ì œ ì‹¤í–‰
                        vectors = self.embedding_model.get_embeddings_batch_adaptive(chunks)
                        
                        batch_time = time.time() - start_time
                        
                        # ê²°ê³¼ ê²€ì¦
                        if vectors and len(vectors) == len(chunks):
                            batch_success = True
                            print(f"âœ… BATCH SUCCESS: {len(chunks)} chunks in {batch_time:.2f}s ({len(chunks)/batch_time:.1f} chunks/sec)")
                            print(f"ğŸ¯ GPU utilization should be HIGH during this process")
                        else:
                            print(f"âŒ BATCH FAILED: Expected {len(chunks)} vectors, got {len(vectors) if vectors else 0}")
                            
                    except Exception as e:
                        print(f"âŒ BATCH PROCESSING ERROR: {e}")
                        import traceback
                        print(f"ğŸ“ Error details: {traceback.format_exc()}")
                    
                    # STEP 3: ë°°ì¹˜ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ê°œë³„ ì²˜ë¦¬
                    if not batch_success:
                        print(f"âš ï¸ Falling back to individual processing (this should be rare)...")
                        vectors = []
                        individual_start = time.time()
                        
                        for i, chunk in enumerate(chunks):
                            try:
                                vector = self.embedding_model.get_embedding(chunk)
                                vectors.append(vector)
                            except Exception as e:
                                print(f"Error embedding chunk {i}: {e}")
                                vectors.append([0] * config.VECTOR_DIM)
                        
                        individual_time = time.time() - individual_start
                        print(f"ğŸŒ Individual processing completed in {individual_time:.2f}s ({len(chunks)/individual_time:.1f} chunks/sec)")
                    
                    # STEP 4: ì„±ëŠ¥ í†µê³„ ì¶œë ¥
                    if batch_success:
                        print(f"ğŸ† PERFORMANCE: Batch processing achieved {len(chunks)/batch_time:.1f} chunks/second")
                        print(f"ğŸ’ª Expected GPU usage: HIGH during batch processing")
                    else:
                        print(f"ğŸš¨ WARNING: Batch processing failed - investigating...")
                    
                    # ë©”íƒ€ë°ì´í„° ë§¤í•‘ ì¤€ë¹„
                    chunk_file_map = [metadata] * len(chunks)
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    self._check_memory_usage("Before saving to Milvus")
                    
                    # ë²¡í„° ì €ì¥
                    success = self._save_vectors_to_milvus(vectors, chunks, chunk_file_map)
                    
                    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ëª…ì‹œì  ë³€ìˆ˜ í•´ì œ
                    del chunks
                    del vectors
                    del chunk_file_map
                    del metadata
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    if 'torch' in sys.modules:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    
                    # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ë° ìƒíƒœ í‘œì‹œ ì—…ë°ì´íŠ¸
                    processing_result["success"] = success
                    
                    # ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if success:
                        self.monitor.last_processed_status = f"{Fore.GREEN}Success{Fore.RESET}"
                    else:
                        self.monitor.last_processed_status = f"{Fore.RED}Fail{Fore.RESET}"
                    
                except Exception as e:
                    print(f"Error processing file {file_name}: {e}")
                    processing_result["success"] = False
                    # ëª¨ë‹ˆí„°ë§ì€ finally ë¸”ë¡ì—ì„œ ì¤‘ì§€ë¨
                
            finally:
                # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
                self.stop_monitoring()

                # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì™„ë£Œ
                # í˜„ì¬ íŒŒì¼ì˜ í¬ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ ì—…ë°ì´íŠ¸
                if "total_size" in self.embedding_progress and file_size > 0:
                    # ì´ë¯¸ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²˜ë¦¬ëœ í¬ê¸° ì¶”ê°€
                    if not hasattr(self, '_processed_this_file') or not self._processed_this_file:
                        self.embedding_progress["processed_size"] += file_size
                        self._processed_this_file = True
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if self.embedding_progress["total_size"] > 0:
                        percentage = min(99, int((self.embedding_progress["processed_size"] / self.embedding_progress["total_size"]) * 100))
                        self.embedding_progress["percentage"] = percentage
                
                self.embedding_in_progress = False
                
                # ì´ë²¤íŠ¸ ì„¤ì •í•˜ì—¬ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ì•Œë¦¼
                processing_completed.set()
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬ ì‹¤í–‰
        processing_thread = threading.Thread(target=process_with_timeout)
        processing_thread.daemon = True
        processing_thread.start()
        
        # ì„ì‹œ ì†ì„± ì´ˆê¸°í™” (íŒŒì¼ê°„ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
        self._processed_this_file = False
        
        # íƒ€ì„ì•„ì›ƒ ì ìš©
        completed = processing_completed.wait(timeout=self.processing_timeout)
        
        if not completed:
            print(f"Error: Processing timed out after {self.processing_timeout} seconds")
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ (íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ)
            self.stop_monitoring()
            self.embedding_in_progress = False
            return False
        
        return processing_result["success"]
    
    def _check_memory_usage(self, stage=""):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° í•„ìš”ì‹œ ê°•ì œ ì •ë¦¬"""
        memory_info = psutil.virtual_memory()
        memory_percent = memory_info.percent
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë©”ì‹œì§€ ì¶œë ¥ ì œê±°
        # print(f"Memory usage at {stage}: {memory_percent:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ê°•ì œ ì •ë¦¬
        if memory_percent > 90:
            # print(f"Warning: High memory usage detected ({memory_percent:.1f}%), forcing cleanup")
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # 2ì°¨ ë©”ëª¨ë¦¬ í™•ì¸
            memory_info = psutil.virtual_memory()
            # print(f"Memory usage after cleanup: {memory_info.percent:.1f}%")
    
    def _extract_chunks_from_file(self, file_path):
        """íŒŒì¼ì—ì„œ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ìµœì í™”ëœ ë©”ì†Œë“œ"""
        # íŒŒì¼ ê²½ë¡œ ê²€ì¦
        if not os.path.exists(file_path) or not os.path.isfile(file_path):
            return None, None
            
        try:
            rel_path = os.path.relpath(file_path, self.vault_path)
            file_name = os.path.basename(file_path)
            file_ext = os.path.splitext(file_path)[1].lower()[1:] if '.' in file_path else ''
            
            # íŒŒì¼ëª… ê²€ì¦ - ë¹„ì–´ìˆê±°ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not file_name or file_name.startswith('.'):
                return None, None
            
            # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬ (ë‹¤ë¥¸ íŒŒì¼ì€ ë²¡í„° ì„ë² ë”© ì œì™¸)
            if file_ext.lower() not in ['pdf', 'md']:
                print(f"Skipping non-supported file type: {file_ext} - {file_path}")
                return None, None
                
            # íŒŒì¼ ìƒì„±/ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
            file_stats = os.stat(file_path)
            created_at = str(file_stats.st_ctime)
            updated_at = str(file_stats.st_mtime)
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                if file_ext == 'pdf':
                    content, title, tags = self._extract_pdf(file_path)
                elif file_ext == 'md':
                    content, title, tags = self._extract_markdown(file_path)
                else:
                    return None, None
            except Exception as e:
                print(f"Error extracting content from {file_path}: {e}")
                return None, None
            
            # ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not content or not content.strip():
                return None, None
            
            # ì²­í¬ë¡œ ë¶„í• 
            chunks = self._split_into_chunks(content)
            if not chunks:
                return None, None
                
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ - contentëŠ” ì²« ë²ˆì§¸ ì²­í¬ì—ë§Œ ì €ì¥
            metadata = {
                "rel_path": rel_path,
                "title": title,
                "content": content,  # ì²­í¬ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°ë¨
                "file_ext": file_ext,
                "is_pdf": file_ext.lower() == 'pdf',
                "tags": tags,
                "created_at": created_at,
                "updated_at": updated_at
            }
            
            return chunks, metadata
            
        except Exception as e:
            error_msg = f"Error processing file {file_path}: {e}"
            print(error_msg)
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return None, None
    
    def _extract_markdown(self, file_path):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìµœì í™”)"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                
            # ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ # í—¤ë”© ë˜ëŠ” íŒŒì¼ëª…)
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else os.path.basename(file_path).replace('.md', '')
            
            # YAML í”„ë¡ íŠ¸ë§¤í„° ë° íƒœê·¸ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
            tags = []
            yaml_match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
            
            if yaml_match:
                try:
                    # ì•ˆì „í•œ YAML íŒŒì‹±
                    frontmatter_text = yaml_match.group(1)
                    # íŠ¹ìˆ˜ ë¬¸ì ë° ì•…ì„± ë¬¸ìì—´ ì œê±° (security)
                    frontmatter_text = re.sub(r'[^\w\s\-\[\]:#\'",._{}]+', ' ', frontmatter_text)
                    
                    try:
                        # YAML íŒŒì‹± ì‹œë„
                        frontmatter = yaml.safe_load(frontmatter_text)
                        if isinstance(frontmatter, dict):
                            # íƒœê·¸ ì¶”ì¶œ
                            if 'tags' in frontmatter:
                                tags_data = frontmatter['tags']
                                if isinstance(tags_data, list):
                                    tags = [str(tag).strip() for tag in tags_data if tag]
                                elif isinstance(tags_data, str):
                                    tags = [tags_data.strip()]
                    except Exception as yaml_err:
                        error_msg = f"YAML parsing error: {yaml_err}, falling back to regex for {os.path.basename(file_path)}"
                        print(error_msg)
                        if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                            self.monitor.add_error_log(error_msg)
                        # YAML íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ í´ë°±
                        tag_match = re.search(r'tags:\s*\[(.*?)\]', frontmatter_text, re.DOTALL)
                        if tag_match:
                            tags_str = tag_match.group(1)
                            tags = [tag.strip().strip("'\"") for tag in tags_str.split(',') if tag.strip()]
                        else:
                            tag_lines = re.findall(r'tags:\s*\n((?:\s*-\s*.+\n)+)', frontmatter_text)
                            if tag_lines:
                                for line in tag_lines[0].split('\n'):
                                    tag_match = re.match(r'\s*-\s*(.+)', line)
                                    if tag_match:
                                        tags.append(tag_match.group(1).strip().strip("'\""))
                except Exception as e:
                    print(f"Error processing frontmatter: {e}")
            
            # ì¸ë¼ì¸ íƒœê·¸ ì¶”ì¶œ (#íƒœê·¸)
            inline_tags = re.findall(r'#([a-zA-Z0-9_-]+)', content)
            tags.extend(inline_tags)
            
            # ì¤‘ë³µ íƒœê·¸ ì œê±°
            tags = list(set([tag for tag in tags if tag and isinstance(tag, str)]))
            
            # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ (LaTeX ìˆ˜ì‹ ë“± ì²˜ë¦¬)
            # $~$ ê°™ì€ ìˆ˜ì‹ ê¸°í˜¸ë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            content = re.sub(r'\$~\$', ' ', content)
            content = re.sub(r'\${2}.*?\${2}', ' ', content, flags=re.DOTALL)  # ë¸”ë¡ ìˆ˜ì‹ ì²˜ë¦¬
            content = re.sub(r'\$.*?\$', ' ', content)  # ì¸ë¼ì¸ ìˆ˜ì‹ ì²˜ë¦¬
            
            # ë¶ˆí•„ìš”í•œ ì—¬ëŸ¬ ì¤„ ê³µë°± ì œê±°
            content = re.sub(r'\n{3,}', '\n\n', content)
            
            # í›„í–‰ ê³µë°± ì œê±°
            content = content.rstrip()
            
            return content, title, tags
            
        except Exception as e:
            print(f"Error in _extract_markdown: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return "", os.path.basename(file_path).replace('.md', ''), []
    
    def _extract_pdf(self, file_path):
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )"""
        title = os.path.basename(file_path).replace('.pdf', '')
        content = ""
        
        try:
            with open(file_path, 'rb') as file:
                # ì•ˆì „í•˜ê²Œ PDF ì½ê¸° ì‹œë„
                try:
                    reader = PyPDF2.PdfReader(file)
                    
                    # PDF ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    metadata = reader.metadata
                    if metadata and '/Title' in metadata and metadata['/Title']:
                        title = metadata['/Title']
                    
                    # í˜ì´ì§€ ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )
                    for i, page in enumerate(reader.pages):
                        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ 10í˜ì´ì§€ë§ˆë‹¤ ì •ë¦¬
                        if i > 0 and i % 10 == 0:
                            gc.collect()
                            
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                content += page_text + "\n\n"
                        except Exception as e:
                            print(f"Error extracting text from page {i}: {e}")
                
                except Exception as e:
                    print(f"Error reading PDF: {e}")
        
        except Exception as e:
            print(f"Error opening PDF file: {e}")
        
        # ë¹ˆ ë‚´ìš©ì¸ ê²½ìš° í™•ì¸
        if not content.strip():
            error_msg = f"Warning: No content extracted from PDF {file_path} - likely a scanned document"
            print(f"\n{Fore.YELLOW}{error_msg}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            # ìŠ¤ìº”ë³¸ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” PDFëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            return None, None, None
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸ (ìŠ¤ìº”ë³¸ PDF ê°ì§€)
        if len(content) > 0:
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸ - ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¨
            # ìŠ¤ìº”ë³¸ PDFëŠ” ì¢…ì¢… ì˜ë¯¸ ì—†ëŠ” ë¬¸ìë‚˜ ê¸°í˜¸ê°€ ë§ì´ í¬í•¨ë¨
            meaningful_chars = sum(1 for c in content if c.isalnum() or c.isspace())
            total_chars = len(content)
            quality_ratio = meaningful_chars / total_chars if total_chars > 0 else 0
            
            # í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ì€ ê²½ìš° (ìŠ¤ìº”ë³¸ PDFë¡œ ê°„ì£¼)
            if quality_ratio < 0.5 and total_chars > 100:
                error_msg = f"Warning: Low quality text extracted from PDF {file_path} (quality: {quality_ratio:.2f}) - likely a scanned document"
                print(f"\n{Fore.YELLOW}{error_msg}{Style.RESET_ALL}")
                if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                    self.monitor.add_error_log(error_msg)
                return None, None, None
        
        return content, title, []  # PDFëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íƒœê·¸ê°€ ì—†ìŒ
    
    def _split_into_chunks(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì„±ëŠ¥ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìµœì í™”)"""
        if not text:
            return []
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - íŠ¹ìˆ˜ ë¬¸ì ë° ë°˜ë³µë˜ëŠ” ê³µë°± ì •ë¦¬
        # ì´ ë‹¨ê³„ì—ì„œ íŠ¹ìˆ˜ ê¸°í˜¸ë‚˜ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” íŒ¨í„´ì„ ì²˜ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r'\.{3,}', '...', text)  # ì—¬ëŸ¬ ì (...)ì„ í•˜ë‚˜ë¡œ í†µí•©
        
        # ğŸ”§ ENHANCED: ì•ˆì „ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì†ë„ì™€ ì•ˆì •ì„± ê· í˜•)
        max_safe_length = 80000  # 8ë§Œ ìë¡œ ì¡°ì • (ì†ë„ ê°œì„  ìœ„í•´)
        if len(text) > max_safe_length:
            print(f"ğŸš¨ WARNING: Text too long ({len(text)} chars), truncating to {max_safe_length} for Milvus compatibility")
            text = text[:max_safe_length]
            
        # ì‚¬ì „ ê²€ì‚¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
        text_length = len(text)
        chunk_size = config.CHUNK_SIZE
        chunk_overlap = config.CHUNK_OVERLAP
        chunk_min_size = config.CHUNK_MIN_SIZE
            
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        if text_length < chunk_size:
            return [text] if text_length >= chunk_min_size else []
        
        # ì²­í¬ ë¶„í• 
        chunks = []
        start = 0
        
        # ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë¡œì»¬ ë³€ìˆ˜ ìºì‹±
        text_find = text.find
        text_rfind = text.rfind
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ì¥ì¹˜
        max_iterations = text_length * 2  # ê·¹ë‹¨ì ì¸ ê²½ìš°ì—ë„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ë„ë¡
        iteration_count = 0
        
        while start < text_length and iteration_count < max_iterations:
            iteration_count += 1
            
            # ì²­í¬ í¬ê¸° ê³„ì‚° (ìµœì†Œ í¬ê¸° ë³´ì¥)
            end = min(start + chunk_size, text_length)
            
            # ì˜ë¯¸ ë‹¨ìœ„(ë¬¸ì¥, ë¬¸ë‹¨) ê²½ê³„ì—ì„œ ë¶„í• 
            if end < text_length:
                # ë¬¸ë‹¨ ê²½ê³„ ì°¾ê¸°
                paragraph_end = text_find('\n\n', start, end)
                if paragraph_end != -1:
                    end = paragraph_end + 2
                else:
                    # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
                    sentence_end = max(
                        text_rfind('. ', start, end),
                        text_rfind('? ', start, end),
                        text_rfind('! ', start, end),
                        text_rfind('.\n', start, end),
                        text_rfind('?\n', start, end),
                        text_rfind('!\n', start, end)
                    )
                    
                    if sentence_end != -1:
                        end = sentence_end + 2
                    else:
                        # ë‹¨ì–´ ê²½ê³„ ì°¾ê¸°
                        space_pos = text_rfind(' ', start, end)
                        if space_pos != -1:
                            end = space_pos + 1
                        # ê³µë°±ì„ ì°¾ì§€ ëª»í•˜ë©´ ê·¸ëƒ¥ ì²­í¬ í¬ê¸° ì‚¬ìš©
            
            # ì²­í¬ ì¶”ì¶œ
            chunk = text[start:end].strip()
            
            # ìœ íš¨í•œ ì²­í¬ë§Œ ì¶”ê°€
            if len(chunk) >= chunk_min_size:
                chunks.append(chunk)
            
            # ì§„í–‰ ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start == end:
                print(f"Warning: Chunking stuck at position {start}, breaking")
                break
                
            # ë‹¤ìŒ ì²­í¬ë¡œ ì´ë™ (ì˜¤ë²„ë© ì ìš©)
            start = max(start + 1, end - chunk_overlap)  # ìµœì†Œ 1ì ì´ìƒ ì§„í–‰ ë³´ì¥
            
            # ì‹œì‘ ìœ„ì¹˜ê°€ í…ìŠ¤íŠ¸ ëì„ ë„˜ì–´ê°€ëŠ” ê²½ìš° ì¤‘ì§€
            if start >= text_length:
                break
        
        # ë¬´í•œ ë£¨í”„ ê²€ì‚¬
        if iteration_count >= max_iterations:
            print("Warning: Max iterations reached in chunking, potential infinite loop avoided")
        
        # ì¤‘ë³µ ì²­í¬ ì œê±°
        unique_chunks = []
        seen = set()
        for chunk in chunks:
            # ì§§ì€ chunkëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
            if len(chunk) < 100:
                unique_chunks.append(chunk)
            else:
                # ê¸´ ì²­í¬ëŠ” í•´ì‹œë¥¼ í†µí•´ ì¤‘ë³µ ê²€ì‚¬
                chunk_hash = hash(chunk)
                if chunk_hash not in seen:
                    seen.add(chunk_hash)
                    unique_chunks.append(chunk)
        
        # ğŸ”§ ENHANCED: GPU/CPU ì„±ëŠ¥ì— ë”°ë¥¸ ë™ì  ì²­í¬ ê°œìˆ˜ ì œí•œ
        if hasattr(self, 'embedding_model') and hasattr(self.embedding_model, 'hardware_profiler'):
            profile = self.embedding_model.hardware_profiler.performance_profile
            
            # GPU ì„±ëŠ¥ì— ë”°ë¥¸ ì²­í¬ ìˆ˜ ê²°ì •
            if 'professional_gpu' in profile:
                max_chunks = 200  # Tesla, A100, H100 ë“±
            elif 'flagship_gpu' in profile:
                max_chunks = 150  # RTX 5090, RX 7900 XTX ë“±
            elif 'ultra_high_end_gpu' in profile:
                max_chunks = 120  # RTX 5080, RTX 4080 ë“±
            elif 'high_end_gpu' in profile:
                max_chunks = 100  # RTX 5070, RTX 4070 ë“±
            elif 'mid_range_gpu' in profile:
                max_chunks = 80   # RTX 3060, RTX 2080 ë“±
            elif 'low_mid_gpu' in profile:
                max_chunks = 60   # RTX 2060, GTX 1660 Ti ë“±
            elif 'low_end_gpu' in profile:
                max_chunks = 50   # GTX 1650, RX 580 ë“±
            elif 'very_low_end_gpu' in profile:
                max_chunks = 40   # GTX 1050 ë“±
            elif 'high_end_cpu' in profile:
                max_chunks = 60   # ê³ ì„±ëŠ¥ CPU
            elif 'mid_range_cpu' in profile:
                max_chunks = 40   # ì¤‘ê¸‰ CPU
            else:
                max_chunks = 30   # ì €ì„±ëŠ¥ CPU
            
            # í•˜ë“œì›¨ì–´ ì„±ëŠ¥ì´ ì¢‹ë”ë¼ë„ ìµœëŒ€ 100ê°œë¡œ ì œí•œ (ì•ˆì •ì„±)
            max_chunks = min(max_chunks, 100)
            
            print(f"ğŸš€ Dynamic chunk limit based on {profile}: {max_chunks} chunks")
        else:
            # í´ë°±: ê¸°ë³¸ ì œí•œ
            max_chunks = 80
            print(f"âš ï¸ Using fallback chunk limit: {max_chunks} chunks")
        if len(unique_chunks) > max_chunks:
            print(f"ğŸš¨ WARNING: Too many chunks ({len(unique_chunks)}), limiting to {max_chunks} for Milvus stability")
            unique_chunks = unique_chunks[:max_chunks]
        
        # ğŸ”§ FINAL CHUNK SAFETY: ê° ì²­í¬ì˜ ê¸¸ì´ë„ ìµœì¢… í™•ì¸ (ì†ë„ì™€ ì•ˆì •ì„± ê· í˜•)
        safe_chunks = []
        max_chunk_length = 12000  # ê°œë³„ ì²­í¬ ìµœëŒ€ 12K ì (ì†ë„ ê°œì„ )
        for chunk in unique_chunks:
            if len(chunk) > max_chunk_length:
                print(f"ğŸš¨ CHUNK TOO LONG: {len(chunk)} chars, truncating to {max_chunk_length}")
                chunk = chunk[:max_chunk_length]
            safe_chunks.append(chunk)
        
        unique_chunks = safe_chunks
            
        return unique_chunks
    
    def _save_vectors_to_milvus(self, vectors, chunks, chunk_file_map):
        """ë²¡í„°ì™€ ì²­í¬ ë°ì´í„°ë¥¼ Milvusì— ì €ì¥í•˜ëŠ” ìµœì í™”ëœ ë©”ì†Œë“œ (ë¬¸ìì—´ ê¸¸ì´ ì œí•œ ê°•í™”)"""
        if not vectors or not chunks or not chunk_file_map or len(vectors) != len(chunks):
            return False
            
        try:
            # ê° ì²­í¬ì™€ ë²¡í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ Milvusì— ì‚½ì…
            # íŒŒì¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ì¶”ì 
            file_chunk_indices = {}
            
            # ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ëœ í•­ëª© ìˆ˜ ì¶”ì 
            success_count = 0
            
            # ê° ì²­í¬ì™€ ë²¡í„° ì²˜ë¦¬
            for i, (vector, chunk, metadata) in enumerate(zip(vectors, chunks, chunk_file_map)):
                # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
                if i > 0 and i % 20 == 0:
                    self._check_memory_usage(f"Milvus insertion {i}/{len(chunks)}")
                
                rel_path = metadata["rel_path"]
                
                # íŒŒì¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ì¶”ì 
                if rel_path not in file_chunk_indices:
                    file_chunk_indices[rel_path] = 0
                chunk_index = file_chunk_indices[rel_path]
                file_chunk_indices[rel_path] += 1
                
                # íƒœê·¸ JSON ë³€í™˜ (ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ)
                try:
                    tags_json = json.dumps(metadata["tags"]) if metadata["tags"] else "[]"
                except:
                    tags_json = "[]"
                
                # ğŸ”§ FIXED: ìµœëŒ€ ë¬¸ìì—´ ê¸¸ì´ (ë” ì•ˆì „í•œ ë§ˆì§„)
                MAX_STRING_LENGTH = 32000  # Milvus ì œí•œ 65535ë³´ë‹¤ ì¶©ë¶„íˆ ì•ˆì „í•˜ê²Œ ì„¤ì •
                MAX_CONTENT_LENGTH = 16000  # content í•„ë“œëŠ” ë” ì§§ê²Œ
                MAX_CHUNK_LENGTH = 16000    # chunk_text í•„ë“œë„ ë” ì§§ê²Œ
                
                # ğŸ”§ ENHANCED: ê°•í™”ëœ ë¬¸ìì—´ ì•ˆì „ ìë¥´ê¸° í•¨ìˆ˜
                def safe_truncate(text, max_len=MAX_STRING_LENGTH):
                    if not isinstance(text, str):
                        return str(text) if text is not None else ""
                    if not text:
                        return ""
                    # UTF-8 ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œë„ í™•ì¸
                    text_bytes = text.encode('utf-8', errors='ignore')[:max_len//2]
                    truncated = text_bytes.decode('utf-8', errors='ignore')
                    # ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì ê¸¸ì´ë„ í™•ì¸
                    return truncated[:max_len] if len(truncated) > max_len else truncated
                
                # ê° í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ì‚½ì…
                single_data = {
                    "id": self.next_id,
                    "path": safe_truncate(rel_path, 500),
                    "title": safe_truncate(metadata["title"], 500) if metadata["title"] else "",
                    # ì²« ë²ˆì§¸ ì²­í¬ì¼ ë•Œë§Œ ì „ì²´ ë‚´ìš© ì €ì¥, ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ë¬¸ìì—´
                    "content": safe_truncate(metadata["content"], MAX_CONTENT_LENGTH) if chunk_index == 0 else "",  # content ê¸¸ì´ ì œí•œ
                    "chunk_text": safe_truncate(chunk, MAX_CHUNK_LENGTH),  # chunk_text ê¸¸ì´ ì œí•œ ê°•í™”
                    "chunk_index": chunk_index,
                    "file_type": safe_truncate(metadata["file_ext"], 10),
                    "tags": safe_truncate(tags_json, 1000),
                    "created_at": safe_truncate(metadata["created_at"], 30),
                    "updated_at": safe_truncate(metadata["updated_at"], 30),
                    "vector": vector
                }
                
                # ğŸ”§ ENHANCED: ê°•í™”ëœ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (ì•ˆì „ ì¥ì¹˜)
                valid_data = True
                for key, value in single_data.items():
                    if key != "vector" and isinstance(value, str):
                        # ëª¨ë“  ë¬¸ìì—´ í•„ë“œì— ëŒ€í•´ ê°•ì œ ê¸¸ì´ ì œí•œ
                        if key == "content":
                            max_field_len = MAX_CONTENT_LENGTH
                        elif key == "chunk_text":
                            max_field_len = MAX_CHUNK_LENGTH
                        else:
                            max_field_len = MAX_STRING_LENGTH
                        
                        if len(value) > max_field_len:
                            print(f"ğŸš¨ CRITICAL: Field {key} too long ({len(value)} chars), forcing truncation to {max_field_len}")
                            single_data[key] = value[:max_field_len]
                
                # ğŸ”§ FINAL SAFETY: ëª¨ë“  ë¬¸ìì—´ì´ ì•ˆì „í•œ ê¸¸ì´ì¸ì§€ ìµœì¢… í™•ì¸
                for key, value in single_data.items():
                    if key != "vector" and isinstance(value, str) and len(value) > 16000:
                        print(f"ğŸš¨ EMERGENCY: Field {key} still too long after all checks ({len(value)} chars), emergency truncation")
                        single_data[key] = value[:10000]  # ì‘ê¸‰ ì²˜ì¹˜ - ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ 10Kë¡œ ì œí•œ
                
                # ë‹¨ì¼ í•­ëª© ì‚½ì…
                try:
                    if valid_data:
                        self.milvus_manager.insert_data(single_data)
                        success_count += 1
                        # 10ê°œ í•­ëª©ë§ˆë‹¤ flush - ë©”ëª¨ë¦¬ ê´€ë¦¬
                        if success_count % 10 == 0:
                            self.milvus_manager.collection.flush()
                except Exception as e:
                    print(f"Error inserting item {self.next_id}: {e}")
                
                self.next_id += 1
            
            # ìµœì¢… flush
            self.milvus_manager.collection.flush()
            
            print(f"Successfully inserted {success_count} out of {len(chunks)} items")
            return success_count > 0
            
        except Exception as e:
            print(f"Error saving vectors to Milvus: {e}")
            return False
    
    def _fast_decision_engine(self, file_path, file_mtime, existing_mtime, file_size):
        """OPTIMIZATION: 3-Tier fast decision making for file processing"""
        rel_path = os.path.relpath(file_path, self.vault_path)
        
        # Cache key for this file
        cache_key = f"{rel_path}:{file_mtime}:{file_size}"
        
        # Check cache first
        if cache_key in self.verification_cache:
            cached_result = self.verification_cache[cache_key]
            print(f"{Fore.BLUE}[CACHED] {rel_path}: {cached_result['decision']} ({cached_result['reason']}){Style.RESET_ALL}")
            return cached_result['decision'], cached_result['reason']
        
        # Calculate time difference
        time_diff = abs(file_mtime - existing_mtime) if existing_mtime > 0 else float('inf')
        
        decision = None
        reason = ""
        
        # TIER 1: Lightning Fast Decisions (90%+ of files)
        if time_diff > self.FAST_PROCESS_THRESHOLD:
            # File definitely modified - immediate process
            decision = "PROCESS"
            reason = f"definitely modified (time_diff: {time_diff:.2f}s)"
            print(f"{Fore.GREEN}[FAST-PROCESS] {rel_path}: {reason}{Style.RESET_ALL}")
            
        elif time_diff < self.FAST_SKIP_THRESHOLD:
            # File very likely unchanged - immediate skip (low risk)
            decision = "SKIP"
            reason = f"very likely unchanged (time_diff: {time_diff:.2f}s)"
            print(f"{Fore.CYAN}[FAST-SKIP] {rel_path}: {reason}{Style.RESET_ALL}")
            
        else:
            # TIER 2: Smart Batch Check for ambiguous cases
            decision = "VERIFY"
            reason = f"ambiguous timestamp (time_diff: {time_diff:.2f}s) - needs verification"
            print(f"{Fore.YELLOW}[NEED-VERIFY] {rel_path}: {reason}{Style.RESET_ALL}")
        
        # Cache the result
        result = {"decision": decision, "reason": reason}
        self.verification_cache[cache_key] = result
        
        return decision, reason

    def _batch_existence_check(self, suspect_files):
        """OPTIMIZATION: Batch check multiple files at once"""
        if not suspect_files:
            return {}
            
        try:
            # Build batch query for multiple files
            paths = [os.path.relpath(fp, self.vault_path) for fp, _ in suspect_files]
            path_conditions = " or ".join([f"path == '{path}'" for path in paths])
            
            # Single query to check all suspect files
            results = self.milvus_manager.query(
                expr=f"({path_conditions})",
                output_fields=["path", "id"],
                limit=len(paths) * 10  # Assume max 10 chunks per file
            )
            
            # Count chunks per file
            file_chunk_counts = {}
            for result in results:
                path = result.get("path")
                if path:
                    file_chunk_counts[path] = file_chunk_counts.get(path, 0) + 1
            
            print(f"{Fore.CYAN}[BATCH-CHECK] Verified {len(suspect_files)} files in single query{Style.RESET_ALL}")
            
            return file_chunk_counts
            
        except Exception as e:
            print(f"{Fore.YELLOW}[WARNING] Batch check failed: {e}{Style.RESET_ALL}")
            return {}
    
    def _normalize_timestamp(self, timestamp_value):
        """Normalize timestamp to float for reliable comparison"""
        try:
            if timestamp_value is None:
                return 0.0
            
            if isinstance(timestamp_value, (int, float)):
                return float(timestamp_value)
            
            if isinstance(timestamp_value, str):
                # Handle empty strings
                if not timestamp_value.strip():
                    return 0.0
                
                # Try direct float conversion
                try:
                    return float(timestamp_value)
                except ValueError:
                    # Try parsing as datetime string if needed
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp_value.replace('Z', '+00:00'))
                        return dt.timestamp()
                    except:
                        print(f"{Fore.YELLOW}[WARNING] Could not parse timestamp: {timestamp_value}{Style.RESET_ALL}")
                        return 0.0
            
            print(f"{Fore.YELLOW}[WARNING] Unknown timestamp type: {type(timestamp_value)}{Style.RESET_ALL}")
            return 0.0
            
        except Exception as e:
            print(f"{Fore.YELLOW}[WARNING] Error normalizing timestamp {timestamp_value}: {e}{Style.RESET_ALL}")
            return 0.0
    
    def process_updated_files(self):
        """ë³¼íŠ¸ì˜ ìƒˆë¡œìš´ íŒŒì¼ ë˜ëŠ” ìˆ˜ì •ëœ íŒŒì¼ë§Œ ì²˜ë¦¬ + ì‚­ì œëœ íŒŒì¼ ì •ë¦¬ (ì¦ë¶„ ì„ë² ë”©) - ENHANCED VERSION"""
        print(f"\n{Fore.CYAN}[DEBUG] Starting FIXED process_updated_files with deleted file cleanup{Style.RESET_ALL}")
        print(f"Processing new/modified files AND cleaning up deleted files in {self.vault_path}")
        
        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.vault_path):
            error_msg = f"Error: Obsidian vault path not found: {self.vault_path}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
        
        print(f"\n{Fore.CYAN}[DEBUG] Vault path exists: {self.vault_path}{Style.RESET_ALL}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ í™•ì¸
        try:
            test_file = os.path.join(self.vault_path, "test_access.txt")
            with open(test_file, 'w') as f:
                f.write("test access")
            os.remove(test_file)
            print(f"{Fore.CYAN}[DEBUG] File system access test passed{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error: Cannot write to vault directory: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False
            return 0
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
        self.embedding_in_progress = True
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": time.time(),
            "current_file": "",
            "estimated_time_remaining": "Calculating...",
            "percentage": 0,
            "is_full_reindex": False,  # ì¦ë¶„ ì„ë² ë”©ì€ ì „ì²´ ì¬ì¸ë±ì‹±ì´ ì•„ë‹˜
            "cpu_percent": 0,
            "memory_percent": 0,
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì§„í–‰ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.embedding_progress["processed_size"] = 0
        self.embedding_progress["processed_files"] = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° ì „ Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Checking Milvus connection...{Style.RESET_ALL}")
            test_query = self.milvus_manager.query("id >= 0", limit=1)
            print(f"\n{Fore.CYAN}[DEBUG] Milvus connection successful. Query result: {test_query}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error connecting to Milvus: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Testing embedding model...{Style.RESET_ALL}")
            test_vector = self.embedding_model.get_embedding("Test embedding model")
            print(f"\n{Fore.CYAN}[DEBUG] Embedding model OK, vector dimension: {len(test_vector)}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error with embedding model: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # Get existing file information - IMPROVED VERSION (more robust)
        existing_files_info = {}
        
        try:
            print(f"{Fore.CYAN}[DEBUG] Querying existing files from Milvus (improved timestamp check)...{Style.RESET_ALL}")
            max_limit = 16000
            offset = 0
            
            while True:
                # Get only path and updated_at for timestamp comparison
                results = self.milvus_manager.query(
                    output_fields=["path", "updated_at"],
                    limit=max_limit,
                    offset=offset,
                    expr="id >= 0"
                )
                
                if not results:
                    break
                    
                for doc in results:
                    path = doc.get("path")
                    updated_at = doc.get('updated_at')
                    
                    if path and path not in existing_files_info:
                        # Store normalized timestamp
                        normalized_timestamp = self._normalize_timestamp(updated_at)
                        existing_files_info[path] = normalized_timestamp
                
                offset += max_limit
                if len(results) < max_limit:
                    break
                
                # Memory management
                gc.collect()
                
            print(f"{Fore.CYAN}[DEBUG] Found {len(existing_files_info)} unique files in DB{Style.RESET_ALL}")
            
        except Exception as e:
            print(f"Warning: Error fetching existing files: {e}")
        
        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ ë° ì‚­ì œëœ íŒŒì¼ íƒì§€
        print("Scanning files for changes and detecting deleted files...")
        files_to_process = []
        skipped_count = 0
        total_files_count = 0
        total_files_size = 0
        new_or_modified_count = 0
        new_or_modified_size = 0
        
        # í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œì˜ íŒŒì¼ë“¤ ìˆ˜ì§‘
        fs_files = set()
        
        print(f"{Fore.CYAN}[DEBUG] Walking through directory: {self.vault_path}{Style.RESET_ALL}")
        
        for root, _, files in os.walk(self.vault_path):
            # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
            if os.path.basename(root).startswith(('.', '_')):
                print(f"{Fore.CYAN}[DEBUG] Skipping hidden directory: {root}{Style.RESET_ALL}")
                continue
                
            for file in files:
                # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                    total_files_count += 1
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, self.vault_path)
                    
                    # íŒŒì¼ ì‹œìŠ¤í…œ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                    fs_files.add(rel_path)
                    
                    try:
                        # íŒŒì¼ í¬ê¸° ë° ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
                        file_size = os.path.getsize(full_path)
                        total_files_size += file_size
                        file_mtime = os.path.getmtime(full_path)
                        
                        # OPTIMIZED: Use fast decision engine
                        file_mtime = os.path.getmtime(full_path)
                        existing_mtime = self._normalize_timestamp(existing_files_info.get(rel_path, 0))
                        
                        decision, reason = self._fast_decision_engine(full_path, file_mtime, existing_mtime, file_size)
                        
                        if decision == "PROCESS":
                            new_or_modified_count += 1
                            new_or_modified_size += file_size
                            
                            if rel_path in existing_files_info:
                                self.milvus_manager.mark_for_deletion(rel_path)
                            
                            files_to_process.append((full_path, file_size))
                            
                        elif decision == "SKIP":
                            skipped_count += 1
                            
                        elif decision == "VERIFY":
                            # Will be handled in batch verification below
                            files_to_process.append((full_path, file_size))
                    except Exception as e:
                        print(f"Warning: Error checking file {rel_path}: {e}")
        
        # ENHANCED: ì‚­ì œëœ íŒŒì¼ íƒì§€ ë° ì •ë¦¬
        print(f"\n{Fore.MAGENTA}[DELETED FILES CLEANUP] Detecting deleted files...{Style.RESET_ALL}")
        deleted_files = set(existing_files_info.keys()) - fs_files
        
        if deleted_files:
            print(f"{Fore.YELLOW}Found {len(deleted_files)} deleted files:{Style.RESET_ALL}")
            
            # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ í‘œì‹œ
            display_count = min(5, len(deleted_files))
            for i, file_path in enumerate(list(deleted_files)[:display_count]):
                print(f"{Fore.YELLOW}  {i+1}. {file_path}{Style.RESET_ALL}")
            
            if len(deleted_files) > display_count:
                print(f"{Fore.YELLOW}  ... and {len(deleted_files) - display_count} more files{Style.RESET_ALL}")
            
            print(f"\n{Fore.CYAN}Starting automatic cleanup of deleted files...{Style.RESET_ALL}")
            
            try:
                # ì‚­ì œëœ íŒŒì¼ë“¤ì˜ ì„ë² ë”© ì •ë¦¬
                cleanup_count = self.cleanup_deleted_embeddings(list(deleted_files))
                
                if cleanup_count > 0:
                    print(f"{Fore.GREEN}âœ… Successfully cleaned up {cleanup_count} deleted files{Style.RESET_ALL}")
                else:
                    print(f"{Fore.YELLOW}âš ï¸ No deleted files were cleaned up{Style.RESET_ALL}")
                    
            except Exception as e:
                print(f"{Fore.RED}Error during deleted files cleanup: {e}{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}âœ… No deleted files found - database is in sync{Style.RESET_ALL}")
        
        # ì‚­ì œ í‘œì‹œëœ íŒŒì¼ë“¤ ì¼ê´„ ì‚­ì œ (ìˆ˜ì •ëœ íŒŒì¼ë“¤ì˜ ì´ì „ ë²„ì „)
        self.milvus_manager.execute_pending_deletions()
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸ - ì´ ê°’ë“¤ì´ ì „ì²´ ì§„í–‰ë„ì˜ ë¶„ëª¨ê°€ ë¨
        self.embedding_progress["total_files"] = new_or_modified_count
        self.embedding_progress["total_size"] = new_or_modified_size
        
        # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
        print(f"\n{Fore.CYAN}[SUMMARY] File processing summary:{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Total files scanned: {total_files_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] New or modified files: {new_or_modified_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Skipped files (unchanged): {skipped_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Deleted files cleaned up: {len(deleted_files)}{Style.RESET_ALL}")
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì •ë³´ ì¶œë ¥
        print(f"Total files to process: {new_or_modified_count} files ({new_or_modified_size/(1024*1024):.2f} MB)")
        
        # OPTIMIZATION: Performance summary
        total_scanned = total_files_count
        if total_scanned > 0:
            print(f"\n{Fore.CYAN}[PERFORMANCE SUMMARY]{Style.RESET_ALL}")
            print(f"Files scanned: {total_scanned}")
            print(f"Processing decisions: {len(files_to_process)}/{total_scanned} ({len(files_to_process)/total_scanned*100:.1f}%)")
            print(f"Skipped decisions: {skipped_count}/{total_scanned} ({skipped_count/total_scanned*100:.1f}%)")
            print(f"Cache entries: {len(self.verification_cache)}")
        
        # ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        if not files_to_process:
            print(f"{Fore.GREEN}[INFO] No new or modified files found. Nothing to process.{Style.RESET_ALL}")
            if len(deleted_files) > 0:
                print(f"{Fore.GREEN}[INFO] However, {len(deleted_files)} deleted files were cleaned up.{Style.RESET_ALL}")
            self.embedding_in_progress = False
            return len(deleted_files)  # Return count of cleaned up files
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
        self.start_monitoring()
        
        try:
            # ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            current_batch_size = self._check_system_resources()
            
            # íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
            processed_count = self._process_file_batch(files_to_process, current_batch_size)
            
            # ì„ë² ë”© ì™„ë£Œ í‘œì‹œ
            self.embedding_in_progress = False
            print(f"\n{Fore.GREEN}[SUCCESS] Incremental embedding & deleted cleanup completed successfully!{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Processed {processed_count} new/modified files{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Cleaned up {len(deleted_files)} deleted files{Style.RESET_ALL}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return processed_count + len(deleted_files)
            
        except Exception as e:
            error_msg = f"Error in process_updated_files: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ë² ë”© ì¤‘ì§€
            self.embedding_in_progress = False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return 0
        finally:
            # í•­ìƒ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.stop_monitoring()
            print(f"\n{Fore.CYAN}[DEBUG] Exiting process_updated_files{Style.RESET_ALL}")
    
    def process_all_files(self):
        """ë³¼íŠ¸ì˜ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë° ì„±ëŠ¥ ê°œì„ )
        ì „ì²´ ì„ë² ë”© - ëª¨ë“  íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹"""
        print(f"\n{Fore.CYAN}[DEBUG] Starting process_all_files (FULL REINDEXING){Style.RESET_ALL}")
        print(f"Processing all files in {self.vault_path} - All files will be reprocessed")
        
        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.vault_path):
            error_msg = f"Error: Obsidian vault path not found: {self.vault_path}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
        
        print(f"\n{Fore.CYAN}[DEBUG] Vault path exists: {self.vault_path}{Style.RESET_ALL}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ í™•ì¸
        try:
            test_file = os.path.join(self.vault_path, "test_access.txt")
            with open(test_file, 'w') as f:
                f.write("test access")
            os.remove(test_file)
            print(f"{Fore.CYAN}[DEBUG] File system access test passed{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error: Cannot write to vault directory: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False
            return 0
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
        self.embedding_in_progress = True
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": time.time(),
            "current_file": "",
            "estimated_time_remaining": "Calculating...",
            "percentage": 0,
            "is_full_reindex": True,
            "cpu_percent": 0,
            "memory_percent": 0,
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì§„í–‰ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.embedding_progress["processed_size"] = 0
        self.embedding_progress["processed_files"] = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° ì „ Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Checking Milvus connection...{Style.RESET_ALL}")
            test_query = self.milvus_manager.query("id >= 0", limit=1)
            print(f"\n{Fore.CYAN}[DEBUG] Milvus connection successful. Query result: {test_query}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error connecting to Milvus: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Testing embedding model...{Style.RESET_ALL}")
            test_vector = self.embedding_model.get_embedding("Test embedding model")
            print(f"\n{Fore.CYAN}[DEBUG] Embedding model OK, vector dimension: {len(test_vector)}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error with embedding model: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚°
        print("Calculating total files and size...")
        total_files_count = 0
        total_files_size = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ê³„ì‚°
        for root, _, files in os.walk(self.vault_path):
            # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
            if os.path.basename(root).startswith(('.', '_')):
                continue
                
            for file in files:
                # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                    total_files_count += 1
                    file_path = os.path.join(root, file)
                    try:
                        file_size = os.path.getsize(file_path)
                        total_files_size += file_size
                    except Exception as e:
                        print(f"Error getting file size for {file_path}: {e}")
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸ - ì´ ê°’ë“¤ì´ ì „ì²´ ì§„í–‰ë„ì˜ ë¶„ëª¨ê°€ ë¨
        self.embedding_progress["total_files"] = total_files_count
        self.embedding_progress["total_size"] = total_files_size
        print(f"Found {total_files_count} files with total size of {total_files_size/(1024*1024):.2f} MB")
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
        self.start_monitoring()
        
        try:
            # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            print(f"\n{Fore.CYAN}[DEBUG] Collecting files to process...{Style.RESET_ALL}")
            
            # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            existing_files_info = {}
            try:
                print(f"{Fore.CYAN}[DEBUG] Querying existing files from Milvus...{Style.RESET_ALL}")
                max_limit = 16000
                offset = 0
                
                while True:
                    results = self.milvus_manager.query(
                        output_fields=["path", "updated_at"],
                        limit=max_limit,
                        offset=offset,
                        expr="id >= 0"
                    )
                    
                    if not results:
                        break
                        
                    for doc in results:
                        path = doc.get("path")
                        if path and path not in existing_files_info:
                            existing_files_info[path] = doc.get('updated_at')
                    
                    offset += max_limit
                    if len(results) < max_limit:
                        break
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬
                    gc.collect()
            except Exception as e:
                print(f"Warning: Error fetching existing files: {e}")
            
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            print("Scanning files...")
            files_to_process = []
            skipped_count = 0
            
            # ì´ë¯¸ ë¯¸ë¦¬ ê³„ì‚°í•œ ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸°ë¥¼ ì‚¬ìš©
            # ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
            total_files = self.embedding_progress["total_files"]
            total_size = self.embedding_progress["total_size"]
            
            print(f"{Fore.CYAN}[DEBUG] Walking through directory: {self.vault_path}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Total files to process: {total_files} files ({total_size/(1024*1024):.2f} MB){Style.RESET_ALL}")
            file_count = 0
            
            for root, _, files in os.walk(self.vault_path):
                # ë””ë ‰í† ë¦¬ë³„ ë¡œê·¸ ì¶”ê°€
                print(f"{Fore.CYAN}[DEBUG] Scanning directory: {root}{Style.RESET_ALL}")
                
                # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
                if os.path.basename(root).startswith(('.', '_')):
                    print(f"{Fore.CYAN}[DEBUG] Skipping hidden directory: {root}{Style.RESET_ALL}")
                    continue
                    
                for file in files:
                    # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                    if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                        file_count += 1
                        # íŒŒì¼ ê²½ë¡œ
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, self.vault_path)
                        print(f"{Fore.CYAN}[DEBUG] Found file {file_count}: {rel_path}{Style.RESET_ALL}")
                        
                        # ë¬¸ì œê°€ ìˆëŠ” íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
                        if "(shorter version(2)).md" in full_path:
                            print(f"Found problematic file: {full_path}")
                            # ì´ íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
                            try:
                                # íŒŒì¼ ë‚´ìš© í™•ì¸
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                
                                # íŒŒì¼ ë‚´ìš©ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ê²½ìš° ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìê°€ ë§ì€ ê²½ìš°
                                if content.count('$~') > 10:
                                    print("File contains many math placeholders, cleaning...")
                                    # íŒŒì¼ ì •ë¦¬ ë° ì €ì¥
                                    content = re.sub(r'\$~\$\n+', '\n', content)
                                    content = re.sub(r'\n{3,}', '\n\n', content)
                                    content = content.rstrip()
                                    
                                    # ì •ë¦¬ëœ íŒŒì¼ ì €ì¥
                                    with open(full_path + ".cleaned", 'w', encoding='utf-8') as f:
                                        f.write(content)
                                    print(f"Cleaned file saved to {full_path}.cleaned")
                            except Exception as e:
                                print(f"Error analyzing problematic file: {e}")
                        
                        # ì „ì²´ ì„ë² ë”©ì—ì„œëŠ” ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬ (ìˆ˜ì • ì‹œê°„ ë¹„êµ ì—†ìŒ)
                        try:
                            # ì„ë² ë”© ì§„í–‰ ì •ë³´ì— ì „ì²´ ì¬ì²˜ë¦¬ ëª¨ë“œ í‘œì‹œ (ì´ë¯¸ main.pyì—ì„œ ì„¤ì •ë¨)
                            
                            # PDF íŒŒì¼ ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸ (í•„ìš”ì— ë”°ë¼ ê±´ë„ˆë›° ìˆ˜ ìˆìŒ)
                            if file.lower().endswith('.pdf') and getattr(config, 'SKIP_PDF_IN_FULL_EMBEDDING', False):
                                print(f"Skipping PDF in full embedding: {rel_path}")
                                skipped_count += 1
                                continue
                                
                            # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì‚­ì œ í‘œì‹œ
                            if rel_path in existing_files_info:
                                print(f"Reprocessing existing file: {rel_path}")
                                self.milvus_manager.mark_for_deletion(rel_path)
                        except Exception as e:
                            print(f"Warning: Error checking file {rel_path}: {e}")
                        
                        # íŒŒì¼ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                        file_size = os.path.getsize(full_path)
                        
                        # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                        files_to_process.append((full_path, file_size))
                        
                        # íŒŒì¼ ëª©ë¡ì´ ë„ˆë¬´ ì»¤ì§€ë©´ ì¤‘ê°„ì— ì²˜ë¦¬
                        if len(files_to_process) >= 10000:
                            print(f"Reached 10,000 files, processing batch...")
                            current_batch_size = self._check_system_resources()
                            self._process_file_batch(files_to_process, current_batch_size)
                            files_to_process = []
                            
                            # ë©”ëª¨ë¦¬ ì •ë¦¬
                            gc.collect()
                            self.embedding_model.clear_cache()
            
            # ì‚­ì œ í‘œì‹œëœ íŒŒì¼ë“¤ë§Œ ì¼ê´„ ì‚­ì œ
            # ì „ì²´ ì¬ìƒ‰ì¸ ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ main.pyì—ì„œ recreate_choiceì— ë”°ë¼ ì»´ë ‰ì…˜ì„ ì¬ìƒì„±í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì‚­ì œ ì‘ì—…ì„ í•˜ì§€ ì•ŠìŒ
            self.milvus_manager.execute_pending_deletions()
            
            # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
            print(f"{Fore.CYAN}[DEBUG] Files found in this scan: {file_count}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Files to process: {len(files_to_process)}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Skipped files: {skipped_count}{Style.RESET_ALL}")
            
            # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì •ë³´ ì¶œë ¥
            print(f"Total files to process: {total_files} files ({total_size/(1024*1024):.2f} MB)")
            
            # ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
            if not files_to_process:
                print(f"{Fore.YELLOW}[WARNING] No files found to process. Creating test file for demonstration.{Style.RESET_ALL}")
                # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
                test_dir = os.path.join(self.vault_path, "test")
                os.makedirs(test_dir, exist_ok=True)
                test_file = os.path.join(test_dir, "test_file.md")
                
                # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‘ì„±
                with open(test_file, "w", encoding="utf-8") as f:
                    f.write("# Test File\n\nThis is a test file for embedding process demonstration.\n\n## Section 1\n\nSome content here.\n\n## Section 2\n\nMore content here.")
                
                # íŒŒì¼ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                file_size = os.path.getsize(test_file)
                
                # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                files_to_process.append((test_file, file_size))
                
                # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸
                self.embedding_progress["total_files"] = 1
                self.embedding_progress["total_size"] = file_size
                
                print(f"Created test file: {test_file} ({file_size} bytes)")
            
            # íŒŒì¼ ë°°ì¹˜ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
            if files_to_process:
                first_five = files_to_process[:5]
                print(f"{Fore.CYAN}[DEBUG] First 5 files in batch: {[os.path.basename(fp) for fp, _ in first_five]}{Style.RESET_ALL}")
            
            # ë‚¨ì€ íŒŒì¼ ì²˜ë¦¬
            if files_to_process:
                print(f"{Fore.CYAN}[DEBUG] Processing files...{Style.RESET_ALL}")
                current_batch_size = self._check_system_resources()
                processed_count = self._process_file_batch(files_to_process, current_batch_size)
                print(f"{Fore.CYAN}[DEBUG] Batch processing completed. Processed {processed_count} files.{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}[WARNING] No files to process after filtering!{Style.RESET_ALL}")
                # íŒŒì¼ì´ ì—†ì–´ë„ ì„ë² ë”© ì§„í–‰ ìƒíƒœë¥¼ ìœ ì§€í•˜ì—¬ ì§„í–‰ë°”ê°€ í‘œì‹œë˜ë„ë¡ í•¨
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            self.embedding_model.clear_cache()
            
            print(f"Successfully processed files, skipped {skipped_count} unchanged files")
            
            # ëª…ì‹œì ìœ¼ë¡œ ì„±ê³µ ì½”ë“œ ë°˜í™˜
            return 1
            
        except Exception as e:
            error_msg = f"Error in process_all_files: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.stop_monitoring()
            
            # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì™„ë£Œ
            self.embedding_in_progress = False
            self.embedding_progress["percentage"] = 100
            print(f"\n{Fore.CYAN}[DEBUG] Exiting process_all_files with status: {0 if 'error_msg' in locals() else 1}{Style.RESET_ALL}")
    
    def _process_file_batch(self, files_to_process, batch_size):
        """íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§"""
        if not files_to_process:
            print(f"{Fore.YELLOW}[DEBUG] No files to process in batch.{Style.RESET_ALL}")
            return 0
            
        processed_count = 0
        total_files = len(files_to_process)
        total_size = sum(file_size for _, file_size in files_to_process)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ì—…ë°ì´íŠ¸
        print(f"{Fore.CYAN}[DEBUG] Processing batch: {total_files} files in this batch ({total_size/(1024*1024):.2f} MB){Style.RESET_ALL}")
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ í™•ì¸
        self.embedding_in_progress = True
        
        # íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
        print(f"{Fore.CYAN}[DEBUG] First 5 files in batch: {[os.path.basename(fp) for fp, _ in files_to_process[:5]]}{Style.RESET_ALL}")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        total_size_mb = total_size / (1024 * 1024)
        with tqdm(total=total_size_mb, desc="Indexing files", unit="MB", ncols=100) as pbar:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, total_files, batch_size):
                batch = files_to_process[i:i+batch_size]
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                self._check_memory_usage(f"Before processing batch {i//batch_size+1}/{(total_files+batch_size-1)//batch_size}")
                
                # ë°°ì¹˜ ì²˜ë¦¬
                for file_item in batch:
                    try:
                        # íŒŒì¼ ê²½ë¡œì™€ í¬ê¸° ë¶„ë¦¬
                        file_path, file_size = file_item
                        
                        # í˜„ì¬ ì²˜ë¦¬ì¤‘ì¸ íŒŒì¼ í‘œì‹œ
                        rel_path = os.path.relpath(file_path, self.vault_path)
                        self.embedding_progress["current_file"] = rel_path
                        self.embedding_progress["processed_files"] += 1
                        
                        # íŒŒì¼ í¬ê¸°ë¥¼ ì¶”ê°€í•˜ê³  processed_sizeëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                        # ì¤‘ìš”: process_fileê°€ ì´ë¯¸ processed_sizeë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                        # ëŒ€ì‹  ì´ë¯¸ ì¶”ê°€ë˜ì–´ ìˆëŠ”ì§€ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸ ì„¤ì •
                        self._processed_this_file = True
                        
                        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
                        success = self.process_file(file_path)
                        if success:
                            processed_count += 1
                            
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        file_size_mb = file_size / (1024 * 1024)
                        pbar.update(file_size_mb)
                        
                    except Exception as e:
                        print(f"Error processing file in batch: {e}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                self.embedding_model.clear_cache()
        
        return processed_count
    
    def detect_deleted_files(self):
        """ì‚­ì œëœ íŒŒì¼ íƒì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        from colorama import Fore, Style
        
        print(f"{Fore.CYAN}Scanning Milvus database for file paths...{Style.RESET_ALL}")
        
        # 1. Milvusì—ì„œ ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)
        db_files = set()
        offset = 0
        max_limit = 16000
        total_db_files = 0
        
        try:
            while True:
                results = self.milvus_manager.query(
                    expr="id >= 0",
                    output_fields=["path"],
                    limit=max_limit,
                    offset=offset
                )
                
                if not results:
                    break
                    
                for doc in results:
                    path = doc.get("path")
                    if path and path not in db_files:
                        db_files.add(path)
                        total_db_files += 1
                
                offset += max_limit
                if len(results) < max_limit:
                    break
                    
                # ì§„í–‰ìƒí™© í‘œì‹œ
                if total_db_files % 1000 == 0 and total_db_files > 0:
                    print(f"{Fore.CYAN}Found {total_db_files} files in database so far...{Style.RESET_ALL}")
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                gc.collect()
                
        except Exception as e:
            print(f"{Fore.RED}Error querying Milvus database: {e}{Style.RESET_ALL}")
            return []
        
        print(f"{Fore.GREEN}Found {len(db_files)} unique files in Milvus database{Style.RESET_ALL}")
        
        # 2. í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº”
        print(f"{Fore.CYAN}Scanning file system...{Style.RESET_ALL}")
        fs_files = set()
        total_fs_files = 0
        
        try:
            for root, _, files in os.walk(self.vault_path):
                # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
                if os.path.basename(root).startswith(('.', '_')):
                    continue
                    
                for file in files:
                    # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                    if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, self.vault_path)
                        fs_files.add(rel_path)
                        total_fs_files += 1
                        
                        # ì§„í–‰ìƒí™© í‘œì‹œ
                        if total_fs_files % 1000 == 0:
                            print(f"{Fore.CYAN}Scanned {total_fs_files} files in file system...{Style.RESET_ALL}")
        
        except Exception as e:
            print(f"{Fore.RED}Error scanning file system: {e}{Style.RESET_ALL}")
            return []
        
        print(f"{Fore.GREEN}Found {len(fs_files)} files in file system{Style.RESET_ALL}")
        
        # 3. ì‚­ì œëœ íŒŒì¼ ì°¾ê¸°
        deleted_files = db_files - fs_files
        
        if deleted_files:
            print(f"{Fore.YELLOW}Found {len(deleted_files)} deleted files{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}No deleted files found{Style.RESET_ALL}")
        
        return list(deleted_files)
    
    def cleanup_deleted_embeddings(self, deleted_files):
        """ì‚­ì œëœ íŒŒì¼ë“¤ì˜ embedding ì œê±°"""
        from colorama import Fore, Style
        
        if not deleted_files:
            print(f"{Fore.GREEN}No files to clean up{Style.RESET_ALL}")
            return 0
        
        print(f"{Fore.CYAN}Starting cleanup of {len(deleted_files)} deleted files...{Style.RESET_ALL}")
        
        success_count = 0
        error_count = 0
        
        try:
            # ë°°ì¹˜ ì‚­ì œë¥¼ ìœ„í•´ pending_deletionsì— ì¶”ê°€
            for file_path in deleted_files:
                self.milvus_manager.mark_for_deletion(file_path)
            
            print(f"{Fore.CYAN}Executing batch deletion...{Style.RESET_ALL}")
            
            # ë°°ì¹˜ ì‚­ì œ ì‹¤í–‰
            self.milvus_manager.execute_pending_deletions()
            
            # ì‚­ì œ ê²°ê³¼ í™•ì¸
            print(f"{Fore.CYAN}Verifying deletion results...{Style.RESET_ALL}")
            
            # ì‚­ì œ í›„ ê²€ì¦
            remaining_files = []
            for file_path in deleted_files:
                try:
                    # íŒŒì¼ì´ ì—¬ì „íˆ DBì— ìˆëŠ”ì§€ í™•ì¸
                    results = self.milvus_manager.query(
                        expr=f"path == '{file_path}'",
                        output_fields=["path"],
                        limit=1
                    )
                    
                    if results:
                        remaining_files.append(file_path)
                        error_count += 1
                    else:
                        success_count += 1
                        
                except Exception as e:
                    print(f"{Fore.YELLOW}Warning: Could not verify deletion of {file_path}: {e}{Style.RESET_ALL}")
                    error_count += 1
            
            # ê²°ê³¼ ë³´ê³ 
            print(f"\n{Fore.GREEN}Cleanup Results:{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Successfully removed: {success_count} files{Style.RESET_ALL}")
            
            if error_count > 0:
                print(f"{Fore.YELLOW}âš ï¸ Failed to remove: {error_count} files{Style.RESET_ALL}")
                if remaining_files:
                    print(f"{Fore.YELLOW}Files that could not be deleted:{Style.RESET_ALL}")
                    for file_path in remaining_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"{Fore.YELLOW}  - {file_path}{Style.RESET_ALL}")
                    if len(remaining_files) > 5:
                        print(f"{Fore.YELLOW}  ... and {len(remaining_files) - 5} more{Style.RESET_ALL}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            return success_count
            
        except Exception as e:
            print(f"{Fore.RED}Error during cleanup: {e}{Style.RESET_ALL}")
            import traceback
            print(f"{Fore.RED}Stack trace: {traceback.format_exc()}{Style.RESET_ALL}")
            return 0