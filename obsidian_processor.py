import os
import re
import PyPDF2
import markdown
import json
import yaml  # Added PyYAML for better frontmatter parsing
import psutil
import colorama
from colorama import Fore, Style
import time
import threading
import gc
import sys
from datetime import datetime
from bs4 import BeautifulSoup
import config
from embeddings import EmbeddingModel
from tqdm import tqdm
from functools import lru_cache
from progress_monitor_cmd import ProgressMonitor

# Import centralized logger
from logger import get_logger

# Initialize module logger
logger = get_logger(__name__)

# Windowsì—ì„œ ìƒ‰ìƒ í‘œì‹œë¥¼ ìœ„í•œ colorama ì´ˆê¸°í™”
colorama.init()

class ObsidianProcessor:
    def __init__(self, milvus_manager):
        logger.info("Initializing ObsidianProcessor")
        
        self.vault_path = config.OBSIDIAN_VAULT_PATH
        logger.info(f"Using Obsidian vault path: {self.vault_path}")
        
        self.milvus_manager = milvus_manager
        self.embedding_model = EmbeddingModel()
        self.next_id = self._get_next_id()
        logger.debug(f"Next ID initialized to: {self.next_id}")
        
        # GPU ì‚¬ìš© ì„¤ì •
        self.use_gpu = config.USE_GPU
        self.device_idx = config.GPU_DEVICE_ID if hasattr(config, 'GPU_DEVICE_ID') else 0
        logger.info(f"GPU settings - Use GPU: {self.use_gpu}, Device index: {self.device_idx}")
        
        # ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ ë‹¨ìœ„)
        self.processing_timeout = 300  # ê¸°ë³¸ê°’: 5ë¶„
        logger.debug(f"Processing timeout set to: {self.processing_timeout} seconds")
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜
        self.embedding_in_progress = False
        logger.info("ObsidianProcessor initialization complete")
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": None,
            "current_file": "",
            "estimated_time_remaining": "",
            "percentage": 0,
            "is_full_reindex": False,
            "cpu_percent": 0,
            "memory_percent": 0,
            "gpu_percent": 0,
            "gpu_memory_used": 0,
            "gpu_memory_total": 0,
            "current_batch_size": 0
        }
        
        # ENHANCED: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì œí•œ ë° ë™ì  ìµœì í™” ì„¤ì •
        self.max_cpu_percent = 85
        self.max_memory_percent = 80
        self.resource_check_interval = 2
        self.last_resource_check = 0
        
        # ë™ì  ë°°ì¹˜ í¬ê¸°: embedding_modelì—ì„œ ìµœì ê°’ ê°€ì ¸ì˜¤ê¸°
        try:
            if hasattr(self.embedding_model, 'batch_optimizer'):
                self.dynamic_batch_size = self.embedding_model.batch_optimizer.current_batch_size
                self.min_batch_size = self.embedding_model.batch_optimizer.min_batch_size
                self.max_batch_size = self.embedding_model.batch_optimizer.max_batch_size
                print(f"ğŸš€ Using optimized batch sizes from embedding model: {self.dynamic_batch_size} (range: {self.min_batch_size}-{self.max_batch_size})")
            else:
                # í´ë°±: ê¸°ë³¸ ì„¤ì •
                self.dynamic_batch_size = getattr(config, 'BATCH_SIZE', 32)
                self.min_batch_size = max(1, self.dynamic_batch_size // 2)
                self.max_batch_size = self.dynamic_batch_size * 4
                print(f"âš ï¸ Using fallback batch sizes: {self.dynamic_batch_size} (range: {self.min_batch_size}-{self.max_batch_size})")
        except Exception as e:
            print(f"Error initializing dynamic batch sizes: {e}")
            # ì•ˆì „ í´ë°±
            self.dynamic_batch_size = 32
            self.min_batch_size = 8
            self.max_batch_size = 128
        
        # ì§„í–‰ë¥  ë° ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì ìƒì„±
        self.monitor = ProgressMonitor(self)
        
        # OPTIMIZATION: Session cache for verification results
        self.verification_cache = {}
        
        # OPTIMIZATION: Performance thresholds for smart decision making
        self.FAST_SKIP_THRESHOLD = 0.1  # Files with time diff < 0.1s are very likely unchanged
        self.FAST_PROCESS_THRESHOLD = 2.0  # Files with time diff > 2.0s are definitely changed
        
    def _get_next_id(self):
        """ë‹¤ìŒ ID ê°’ ê°€ì ¸ì˜¤ê¸° (ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬)"""
        try:
            # ì¿¼ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            results = self.milvus_manager.query("id >= 0", output_fields=["id"], limit=1)
            
            # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìœ¼ë©´ 1ë¡œ ì‹œì‘
            if not results or len(results) == 0:
                logger.debug("No existing IDs found, starting with ID 1")
                return 1
            
            # ê²°ê³¼ì—ì„œ ID ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
            valid_ids = []
            for r in results:
                try:
                    # IDê°€ ì‹¤ì œ ì •ìˆ˜ì¸ì§€ í™•ì¸
                    if 'id' in r and r['id'] is not None and isinstance(r['id'], (int, float)):
                        valid_ids.append(int(r['id']))
                    else:
                        logger.warning(f"Skipping invalid ID format: {r}")
                except Exception as id_err:
                    logger.warning(f"Error processing ID entry: {r}, error: {id_err}")
            
            # ìœ íš¨í•œ IDê°€ ìˆìœ¼ë©´ ìµœëŒ€ê°’ + 1 ë°˜í™˜
            if valid_ids:
                next_id = max(valid_ids) + 1
                logger.debug(f"Found valid IDs, next ID will be: {next_id}")
                return next_id
            else:
                logger.warning("No valid IDs found, starting with ID 1")
                return 1
                
        except Exception as e:
            # ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬í•˜ê³  ì•ˆì „í•˜ê²Œ 1 ë°˜í™˜
            logger.error(f"Error getting next ID: {e}, using default ID 1")
            return 1
        
    def _create_ascii_bar(self, percent, width=20):
        """í¼ì„¼íŠ¸ ê°’ì„ ë°›ì•„ ASCII ê·¸ë˜í”„ ë°” ìƒì„±"""
        # ê°’ì´ ìœ íš¨í•œ ë²”ìœ„ì¸ì§€ í™•ì¸
        if not isinstance(percent, (int, float)) or percent < 0:
            percent = 0
        elif percent > 100:
            percent = 100
            
        # ì±„ì›Œì§ˆ ê¸¸ì´ ê³„ì‚° (ë°˜ì˜¬ë¦¼í•˜ì—¬ ìµœì†Œ 1ì¹¸ì€ í‘œì‹œ)
        filled_length = max(1, int(width * percent / 100)) if percent > 0 else 0
        
        # ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë‹¤ë¥¸ ë¬¸ì ì‚¬ìš©
        if percent > 90:
            bar_char = '#'  # ë§¤ìš° ë†’ìŒ
        elif percent > 70:
            bar_char = '='  # ë†’ìŒ
        elif percent > 50:
            bar_char = '-'  # ì¤‘ê°„
        elif percent > 30:
            bar_char = '.'  # ë‚®ìŒ
        elif percent > 0:
            bar_char = 'Â·'  # ë§¤ìš° ë‚®ìŒ
        else:
            bar_char = ' '  # 0%
        
        # ê·¸ë˜í”„ ë°” ìƒì„± (ìµœëŒ€ ê¸¸ì´ ì œí•œ)
        filled_length = min(filled_length, width)
        bar = bar_char * filled_length + ' ' * (width - filled_length)
        return bar
        
    def _check_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ (ENHANCED)"""
        current_time = time.time()
        
        if current_time - self.last_resource_check < self.resource_check_interval:
            return self.dynamic_batch_size
            
        self.last_resource_check = current_time
        
        try:
            # ENHANCED: embedding_modelì˜ ë™ì  ë°°ì¹˜ ìµœì í™” ì‚¬ìš©
            if hasattr(self.embedding_model, 'system_monitor'):
                system_status = self.embedding_model.system_monitor.get_system_status()
                memory_percent = system_status.get('memory_percent', 50)
                cpu_percent = system_status.get('cpu_percent', 50)
                gpu_percent = system_status.get('gpu_percent', 0)
                
                # embedding_modelì˜ batch_optimizerë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •
                if hasattr(self.embedding_model, 'batch_optimizer'):
                    optimal_batch = self.embedding_model.batch_optimizer.adjust_batch_size({
                        'memory_percent': memory_percent,
                        'cpu_percent': cpu_percent,
                        'gpu_percent': gpu_percent,
                        'processing_time': 1.0
                    })
                    
                    # ë™ì  ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
                    self.dynamic_batch_size = optimal_batch
                    
                    # ì§„í–‰ë¥  ì •ë³´ ì—…ë°ì´íŠ¸
                    self.embedding_progress["current_batch_size"] = self.dynamic_batch_size
                    
                    print(f"ğŸ“ˆ Dynamic batch size adjusted to: {self.dynamic_batch_size} (Memory: {memory_percent:.1f}%, GPU: {gpu_percent:.1f}%)")
            
            # ProgressMonitor ì—…ë°ì´íŠ¸
            if hasattr(self.monitor, '_update_system_resources'):
                self.monitor._update_system_resources()
                
        except Exception as e:
            print(f"Error in enhanced system resource check: {e}")
        
        return self.dynamic_batch_size
        
    def _update_progress_stats(self):
        """ì„ë² ë”© ì§„í–‰ë¥ ê³¼ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ë©”ì†Œë“œ (íŒŒì¼ í¬ê¸° ê¸°ë°˜)"""
        if not self.embedding_in_progress:
            return
            
        # ì§„í–‰ë¥  ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
        total_size = self.embedding_progress["total_size"]
        processed_size = self.embedding_progress["processed_size"]
        
        # íŒŒì¼ ê°œìˆ˜ë„ í•¨ê»˜ í‘œì‹œí•˜ê¸° ìœ„í•´ ìœ ì§€
        total_files = self.embedding_progress["total_files"]
        processed_files = self.embedding_progress["processed_files"]
        
        if total_size <= 0:
            self.embedding_progress["percentage"] = 0
            self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
            return
            
        # ì§„í–‰ë„ëŠ” íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        if total_size > 0:
            percentage = min(99, int((processed_size / total_size) * 100))  # 100%ëŠ” ì™„ì „íˆ ì™„ë£Œë˜ì—ˆì„ ë•Œë§Œ í‘œì‹œ
            self.embedding_progress["percentage"] = percentage
        
        # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
        if processed_size > 0 and self.embedding_progress["start_time"] is not None:
            elapsed_time = time.time() - self.embedding_progress["start_time"]
            bytes_per_second = processed_size / elapsed_time if elapsed_time > 0 else 0
            
            if bytes_per_second > 0:
                remaining_size = total_size - processed_size
                remaining_seconds = remaining_size / bytes_per_second
                
                # ì˜ˆìƒ ì‹œê°„ í¬ë§·íŒ…
                if remaining_seconds < 60:
                    time_str = f"{int(remaining_seconds)}ì´ˆ"
                elif remaining_seconds < 3600:
                    minutes = int(remaining_seconds / 60)
                    seconds = int(remaining_seconds % 60)
                    time_str = f"{minutes}ë¶„ {seconds}ì´ˆ"
                else:
                    hours = int(remaining_seconds / 3600)
                    minutes = int((remaining_seconds % 3600) / 60)
                    time_str = f"{hours}ì‹œê°„ {minutes}ë¶„"
                    
                self.embedding_progress["estimated_time_remaining"] = time_str
            else:
                self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
        else:
            self.embedding_progress["estimated_time_remaining"] = "ê³„ì‚° ì¤‘..."
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            memory_percent = memory_info.percent
            
            self.embedding_progress["cpu_percent"] = cpu_percent
            self.embedding_progress["memory_percent"] = memory_percent
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¬´ì‹œ
            pass
        
    def start_monitoring(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitor.start()
        
    def stop_monitoring(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitor.stop()
    
    def process_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ë° ìƒ‰ì¸ (ìµœì í™” ë° ì•ˆì „ì¥ì¹˜ ì¶”ê°€)"""
        if not os.path.exists(file_path):
            error_msg = f"Error: File not found: {file_path}"
            print(error_msg)
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return False
            
        file_size = os.path.getsize(file_path)
        file_name = os.path.basename(file_path)
        
        # ì¶œë ¥ ì¤„ì´ê¸° - ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì„
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.embedding_in_progress = True
        
        # í˜„ì¬ íŒŒì¼ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ê¸°ì¡´ ì´ íŒŒì¼ ìˆ˜/í¬ê¸° ìœ ì§€
        current_progress = self.embedding_progress.copy()
        
        # í˜„ì¬ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì§„í–‰ ì •ë³´ ì´ˆê¸°í™” (ì „ì²´ ì§„í–‰ë¥ ì€ ìœ ì§€)
        self.embedding_progress = {
            "total_files": current_progress.get("total_files", 1), # ê¸°ì¡´ ì´ íŒŒì¼ ìˆ˜ ìœ ì§€
            "processed_files": current_progress.get("processed_files", 0), # ê¸°ì¡´ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ ìœ ì§€
            "total_size": current_progress.get("total_size", file_size), # ê¸°ì¡´ ì´ í¬ê¸° ìœ ì§€
            "processed_size": current_progress.get("processed_size", 0), # ê¸°ì¡´ ì²˜ë¦¬ëœ í¬ê¸° ìœ ì§€
            "start_time": current_progress.get("start_time", time.time()),
            "current_file": file_name,
            "estimated_time_remaining": current_progress.get("estimated_time_remaining", "Calculating..."),
            "percentage": current_progress.get("percentage", 0),
            "is_full_reindex": current_progress.get("is_full_reindex", False),
            "cpu_percent": current_progress.get("cpu_percent", 0),
            "memory_percent": current_progress.get("memory_percent", 0),
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì „ì—­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        processing_completed = threading.Event()
        processing_result = {"success": False}
        
        def process_with_timeout():
            try:
                # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                self.start_monitoring()
                
                try:
                    # íŒŒì¼ì—ì„œ ì²­í¬ ì¶”ì¶œ
                    chunks, metadata = self._extract_chunks_from_file(file_path)
                    if not chunks or not metadata:
                        # ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì´ë¯€ë¡œ ì¶œë ¥ ì¤„ì„
                        processing_result["success"] = False
                        self.monitor.last_processed_status = f"{Fore.RED}Fail{Fore.RESET}"
                        return
                    
                    # ì²­í¬ ì¶”ì¶œ ì„±ê³µ ë©”ì‹œì§€ ì œê±° - ì§„í–‰ë¥  í‘œì‹œì—ì„œ í‘œì‹œí•  ê²ƒì„
                    
                    # ì„ë² ë”© ì§„í–‰ ì •ë³´ ì—…ë°ì´íŠ¸
                    self.embedding_progress["current_file"] = file_name
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    self._check_memory_usage("Before embedding generation")
                    
                    # ENHANCED: ì²­í¬ì— ëŒ€í•œ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì†ë„ ëŒ€í­ ê°œì„ !)
                    logger.info(f"Processing {len(chunks)} chunks with batch embedding for file: {file_name}")
                    print(f"ğŸš€ Processing {len(chunks)} chunks with FORCED batch embedding...")
                    
                    # Check for special characters in file path that might need careful handling
                    has_special_chars = any(c in file_path for c in "'\"()[]{},;")
                    if has_special_chars:
                        logger.debug(f"File path contains special characters: {file_path}")
                    
                    # STEP 1: ë°°ì¹˜ í¬ê¸° í™•ì¸ ë° ìµœì í™”
                    optimal_batch_size = self._check_system_resources()
                    if hasattr(self.embedding_model, 'batch_optimizer'):
                        current_batch_size = self.embedding_model.batch_optimizer.current_batch_size
                        logger.debug(f"Current optimal batch size: {current_batch_size} for {len(chunks)} chunks")
                        print(f"ğŸ“¦ Current optimal batch size: {current_batch_size}")
                    
                    vectors = []
                    batch_success = False
                    
                    try:
                        # STEP 2: ê°•ì œ ë°°ì¹˜ ì²˜ë¦¬ (í´ë°± ì—†ì´)
                        logger.debug(f"Starting batch processing for {len(chunks)} chunks")
                        print(f"ğŸ”¥ FORCING batch processing for {len(chunks)} chunks...")
                        start_time = time.time()
                        
                        # ë°°ì¹˜ ì²˜ë¦¬ ê°•ì œ ì‹¤í–‰
                        vectors = self.embedding_model.get_embeddings_batch_adaptive(chunks)
                        
                        batch_time = time.time() - start_time
                        logger.debug(f"Batch processing completed in {batch_time:.2f} seconds")
                        
                        # ê²°ê³¼ ê²€ì¦
                        if vectors and len(vectors) == len(chunks):
                            batch_success = True
                            logger.info(f"Batch processing succeeded: {len(chunks)} chunks in {batch_time:.2f}s ({len(chunks)/batch_time:.1f} chunks/sec)")
                            print(f"âœ… BATCH SUCCESS: {len(chunks)} chunks in {batch_time:.2f}s ({len(chunks)/batch_time:.1f} chunks/sec)")
                            print(f"ğŸ¥ GPU utilization should be HIGH during this process")
                        else:
                            logger.warning(f"Batch processing failed: Expected {len(chunks)} vectors, got {len(vectors) if vectors else 0}")
                            print(f"âŒ BATCH FAILED: Expected {len(chunks)} vectors, got {len(vectors) if vectors else 0}")
                            
                    except Exception as e:
                        # Check if timeout-related error (handling the processing_timeout attribute)
                        if "timeout" in str(e).lower():
                            logger.error(f"Batch processing timed out after {self.processing_timeout} seconds: {e}", exc_info=True)
                        else:
                            logger.error(f"Batch processing error: {e}", exc_info=True)
                            
                        print(f"âŒ BATCH PROCESSING ERROR: {e}")
                        import traceback
                        print(f"ğŸ“ Error details: {traceback.format_exc()}")
                    
                    # STEP 3: ë°°ì¹˜ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ê°œë³„ ì²˜ë¦¬
                    if not batch_success:
                        logger.warning(f"Falling back to individual processing for {len(chunks)} chunks")
                        print(f"âš ï¸ Falling back to individual processing (this should be rare)...")
                        vectors = []
                        individual_start = time.time()
                        
                        successful_chunks = 0
                        failed_chunks = 0
                        
                        for i, chunk in enumerate(chunks):
                            try:
                                vector = self.embedding_model.get_embedding(chunk)
                                vectors.append(vector)
                                successful_chunks += 1
                            except Exception as e:
                                logger.error(f"Error embedding chunk {i} in individual processing: {e}")
                                print(f"Error embedding chunk {i}: {e}")
                                # Use zero vector as fallback
                                vectors.append([0] * config.VECTOR_DIM)
                                failed_chunks += 1
                        
                        individual_time = time.time() - individual_start
                        logger.info(f"Individual processing completed: {successful_chunks} succeeded, {failed_chunks} failed, took {individual_time:.2f}s")
                        print(f"ğŸŒ Individual processing completed in {individual_time:.2f}s ({len(chunks)/individual_time:.1f} chunks/sec)")
                    
                    # STEP 4: ì„±ëŠ¥ í†µê³„ ì¶œë ¥
                    if batch_success:
                        logger.info(f"Performance: Batch processing achieved {len(chunks)/batch_time:.1f} chunks/second")
                        print(f"ğŸ† PERFORMANCE: Batch processing achieved {len(chunks)/batch_time:.1f} chunks/second")
                        print(f"ğŸ’ª Expected GPU usage: HIGH during batch processing")
                    else:
                        logger.warning("Batch processing failed - review logs for details")
                        print(f"ğŸ”¨ WARNING: Batch processing failed - investigating...")
                    
                    # Check for special characters in file path before Milvus operations
                    if has_special_chars:
                        logger.info(f"Preparing to insert file with special characters into Milvus: {file_path}")
                    
                    # ë©”íƒ€ë°ì´í„° ë§¤í•‘ ì¤€ë¹„
                    chunk_file_map = [metadata] * len(chunks)
                    logger.debug(f"Prepared {len(chunks)} chunk-file mappings with metadata")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    self._check_memory_usage("Before saving to Milvus")
                    
                    # ë²¡í„° ì €ì¥
                    logger.info(f"Saving {len(vectors)} vectors to Milvus for file: {file_name}")
                    try:
                        success = self._save_vectors_to_milvus(vectors, chunks, chunk_file_map)
                        if success:
                            logger.info(f"Successfully saved vectors to Milvus for file: {file_name}")
                        else:
                            logger.error(f"Failed to save vectors to Milvus for file: {file_name}")
                            # ì¶”ê°€ ì§„ë‹¨ ì •ë³´ ë¡œê¹…
                            logger.error(f"  File details - Size: {os.path.getsize(file_path) if os.path.exists(file_path) else 'N/A'} bytes")
                            logger.error(f"  Path characteristics - Has special chars: {any(c in file_path for c in '[](){}#$%^&*')}")
                            logger.error(f"  Path starts with number: {bool(re.match(r'^\d', os.path.basename(file_path)))}")
                            logger.error(f"  Current memory usage: {psutil.Process().memory_info().rss / (1024 * 1024):.1f} MB")
                    except Exception as e:
                        error_type = type(e).__name__
                        error_msg = str(e)
                        logger.error(f"Error saving vectors to Milvus for file: {file_name}")
                        logger.error(f"  Error type: {error_type}, Message: {error_msg}")
                        
                        # ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
                        logger.error(f"  File details - Path: {file_path}")
                        logger.error(f"  Chunks: {len(chunks) if chunks else 0}, Vectors: {len(vectors) if vectors else 0}")
                        
                        # ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” íŠ¹ë³„í•œ ì¡°ê±´ ê²€ì‚¬
                        if "DataNotMatchException" in error_type or "schema" in error_msg.lower():
                            logger.error("  Possible schema mismatch issue - Check collection fields")
                            
                            # ìƒì„¸ ì§„ë‹¨ - ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª… ë¬¸ì œ í™•ì¸
                            base_name = os.path.basename(file_path)
                            if re.match(r'^\d', base_name):
                                logger.error(f"  CRITICAL: File '{base_name}' starts with a number - this is likely causing the schema issue")
                                logger.error("  SOLUTION: Add 'file_' prefix to filenames and 'Title_' prefix to titles starting with numbers")
                                
                            # 'id' í•„ë“œ ê´€ë ¨ ë¬¸ì œ í™•ì¸
                            if "id" in error_msg.lower():
                                logger.error("  'id' field issue detected in error message - verify schema compatibility")
                                # ì²«ë²ˆì§¸ ì²­í¬ ë°ì´í„° êµ¬ì¡° í™•ì¸
                                if chunk_file_map and len(chunk_file_map) > 0:
                                    first_chunk = chunk_file_map[0] if chunk_file_map else None
                                    if first_chunk:
                                        logger.error(f"  First chunk metadata keys: {list(first_chunk.keys() if first_chunk else [])}")
                        elif "timeout" in error_msg.lower():
                            logger.error("  Possible timeout issue - Check network or increase processing_timeout")
                        elif "memory" in error_msg.lower():
                            logger.error("  Possible memory issue - Check available system resources")
                            
                        logger.error(f"  Stack trace:", exc_info=True)
                        success = False
                    
                    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ëª…ì‹œì  ë³€ìˆ˜ í•´ì œ
                    logger.debug("Explicitly releasing memory for large variables")
                    del chunks
                    del vectors
                    del chunk_file_map
                    del metadata
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    logger.debug("Running garbage collection and clearing GPU cache")
                    gc.collect()
                    if 'torch' in sys.modules:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    
                    # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ë° ìƒíƒœ í‘œì‹œ ì—…ë°ì´íŠ¸
                    processing_result["success"] = success
                    
                    # ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if success:
                        logger.info(f"Successfully processed file: {file_path}")
                        self.monitor.last_processed_status = f"{Fore.GREEN}Success{Fore.RESET}"
                    else:
                        logger.warning(f"Failed to process file: {file_path}")
                        self.monitor.last_processed_status = f"{Fore.RED}Fail{Fore.RESET}"
                    
                except Exception as e:
                    # Check if it's a timeout issue
                    if "timeout" in str(e).lower():
                        logger.error(f"Processing timed out for file: {file_path} after {self.processing_timeout} seconds", exc_info=True)
                    # Check if it's related to special characters in the path
                    elif any(c in file_path for c in "'\"()[]{},;"):
                        logger.error(f"Error processing file with special characters: {file_path}: {e}", exc_info=True)
                    else:
                        logger.error(f"Error processing file {file_name}: {e}", exc_info=True)
                        
                    print(f"Error processing file {file_name}: {e}")
                    processing_result["success"] = False
                    # ëª¨ë‹ˆí„°ë§ì€ finally ë¸”ë¡ì—ì„œ ì¤‘ì§€ë¨
                
            finally:
                # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
                logger.debug("Stopping resource monitoring")
                self.stop_monitoring()

                # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì™„ë£Œ
                # í˜„ì¬ íŒŒì¼ì˜ í¬ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ ì—…ë°ì´íŠ¸
                if "total_size" in self.embedding_progress and file_size > 0:
                    # ì´ë¯¸ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²˜ë¦¬ëœ í¬ê¸° ì¶”ê°€
                    if not hasattr(self, '_processed_this_file') or not self._processed_this_file:
                        logger.debug(f"Updating processed size: +{file_size} bytes")
                        self.embedding_progress["processed_size"] += file_size
                        self._processed_this_file = True
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if self.embedding_progress["total_size"] > 0:
                        percentage = min(99, int((self.embedding_progress["processed_size"] / self.embedding_progress["total_size"]) * 100))
                        self.embedding_progress["percentage"] = percentage
                
                self.embedding_in_progress = False
                
                # ì´ë²¤íŠ¸ ì„¤ì •í•˜ì—¬ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ì•Œë¦¼
                processing_completed.set()
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬ ì‹¤í–‰
        processing_thread = threading.Thread(target=process_with_timeout)
        processing_thread.daemon = True
        processing_thread.start()
        
        # ì„ì‹œ ì†ì„± ì´ˆê¸°í™” (íŒŒì¼ê°„ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
        self._processed_this_file = False
        
        # íƒ€ì„ì•„ì›ƒ ì ìš©
        logger.debug(f"Waiting for processing to complete with timeout of {self.processing_timeout} seconds")
        completed = processing_completed.wait(timeout=self.processing_timeout)
        
        if not completed:
            # Check if this file has special characters in its path
            has_special_chars = any(c in file_path for c in "'\"()[]{},;")
            if has_special_chars:
                logger.error(f"Processing timed out for file with special characters: {file_path} after {self.processing_timeout} seconds")
            else:
                logger.error(f"Processing timed out after {self.processing_timeout} seconds for file: {file_path}")
                
            print(f"Error: Processing timed out after {self.processing_timeout} seconds")
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ (íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ)
            logger.debug("Stopping resource monitoring due to timeout")
            self.stop_monitoring()
            self.embedding_in_progress = False
            return False
        
        logger.debug(f"Processing completed successfully within timeout period ({self.processing_timeout}s)")
        
        return processing_result["success"]
    
    def _check_memory_usage(self, stage=""):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° í•„ìš”ì‹œ ê°•ì œ ì •ë¦¬"""
        memory_info = psutil.virtual_memory()
        memory_percent = memory_info.percent
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë©”ì‹œì§€ ì¶œë ¥ ì œê±°
        # print(f"Memory usage at {stage}: {memory_percent:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ê°•ì œ ì •ë¦¬
        if memory_percent > 90:
            # print(f"Warning: High memory usage detected ({memory_percent:.1f}%), forcing cleanup")
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # 2ì°¨ ë©”ëª¨ë¦¬ í™•ì¸
            memory_info = psutil.virtual_memory()
            # print(f"Memory usage after cleanup: {memory_info.percent:.1f}%")
    
    def _extract_chunks_from_file(self, file_path):
        """íŒŒì¼ì—ì„œ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ìµœì í™”ëœ ë©”ì†Œë“œ"""
        # íŒŒì¼ ê²½ë¡œ ê²€ì¦
        if not os.path.exists(file_path) or not os.path.isfile(file_path):
            logger.warning(f"File does not exist or is not a file: {file_path}")
            return None, None
            
        try:
            rel_path = os.path.relpath(file_path, self.vault_path)
            file_name = os.path.basename(file_path)
            file_ext = os.path.splitext(file_path)[1].lower()[1:] if '.' in file_path else ''
            
            # Check for special characters in file path that might need careful handling
            has_special_chars = any(c in file_path for c in "'\"()[]{},;")
            if has_special_chars:
                logger.debug(f"Extracting chunks from file with special characters in path: {rel_path}")
            
            logger.debug(f"Extracting chunks from file: {rel_path} (extension: {file_ext})")
            
            # íŒŒì¼ëª… ê²€ì¦ - ë¹„ì–´ìˆê±°ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not file_name or file_name.startswith('.'):
                logger.warning(f"Invalid filename detected: {file_name}")
                return None, None
            
            # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬ (ë‹¤ë¥¸ íŒŒì¼ì€ ë²¡í„° ì„ë² ë”© ì œì™¸)
            if file_ext.lower() not in ['pdf', 'md']:
                logger.info(f"Skipping non-supported file type: {file_ext} - {rel_path}")
                print(f"Skipping non-supported file type: {file_ext} - {file_path}")
                return None, None
                
            # íŒŒì¼ ìƒì„±/ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
            file_stats = os.stat(file_path)
            created_at = str(file_stats.st_ctime)
            updated_at = str(file_stats.st_mtime)
            logger.debug(f"File stats: created={created_at}, updated={updated_at} for {rel_path}")
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                # Log special attention for files with special characters
                if has_special_chars:
                    logger.info(f"Attempting to extract content from file with special characters: {rel_path}")
                    
                if file_ext == 'pdf':
                    logger.debug(f"Extracting content from PDF file: {rel_path}")
                    content, title, tags = self._extract_pdf(file_path)
                elif file_ext == 'md':
                    logger.debug(f"Extracting content from Markdown file: {rel_path}")
                    content, title, tags = self._extract_markdown(file_path)
                else:
                    return None, None
                    
                # Log successful extraction
                logger.debug(f"Successfully extracted content from {rel_path}, title: '{title}', tags: {tags}")
                
            except Exception as e:
                # Check if it's an Excalidraw file (known to have special characters)
                if "excalidraw" in file_path.lower():
                    logger.error(f"Error extracting content from Excalidraw file: {rel_path}: {e}", exc_info=True)
                elif has_special_chars:
                    logger.error(f"Error extracting content from file with special characters: {rel_path}: {e}", exc_info=True)
                else:
                    logger.error(f"Error extracting content from {rel_path}: {e}", exc_info=True)
                    
                print(f"Error extracting content from {file_path}: {e}")
                return None, None
            
            # ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not content or not content.strip():
                logger.warning(f"Empty content extracted from {rel_path}")
                return None, None
            
            # ì²­í¬ë¡œ ë¶„í• 
            logger.debug(f"Splitting content into chunks for {rel_path}")
            try:
                chunks = self._split_into_chunks(content)
                if not chunks:
                    logger.warning(f"No chunks generated from {rel_path}")
                    return None, None
                    
                logger.info(f"Successfully generated {len(chunks)} chunks from {rel_path}")
                
            except Exception as e:
                logger.error(f"Error splitting content into chunks for {rel_path}: {e}", exc_info=True)
                return None, None
                
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ - contentëŠ” ì²« ë²ˆì§¸ ì²­í¬ì—ë§Œ ì €ì¥
            try:
                # Check if we need to handle special characters in paths for Milvus
                path_for_milvus = rel_path
                
                # Add logging for special character detection in path that might affect Milvus
                if has_special_chars:
                    logger.debug(f"Preparing metadata for file with special characters: {rel_path}")
                
                metadata = {
                    "rel_path": path_for_milvus,  # Use the potentially sanitized path
                    "title": title,
                    "content": content,  # ì²­í¬ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°ë¨
                    "file_ext": file_ext,
                    "is_pdf": file_ext.lower() == 'pdf',
                    "tags": tags,
                    "created_at": created_at,
                    "updated_at": updated_at,
                    "path": rel_path  # Store the original path as well
                }
                
                logger.debug(f"Created metadata for {rel_path} with {len(chunks)} chunks")
                
                # 2ì°¨ ì²˜ë¦¬ìš© ì„ì‹œì €ì¥ ì œê±°
                metadata.pop('content', None)
                
                return chunks, metadata
                
            except Exception as e:
                logger.error(f"Error preparing metadata for {rel_path}: {e}", exc_info=True)
                return None, None
            
        except Exception as e:
            # Check if it's an Excalidraw file or has special characters
            if "excalidraw" in file_path.lower():
                logger.error(f"Error extracting chunks from Excalidraw file: {file_path}: {e}", exc_info=True)
            elif has_special_chars and 'has_special_chars' in locals():
                logger.error(f"Error extracting chunks from file with special characters: {file_path}: {e}", exc_info=True)
            else:
                logger.error(f"Error extracting chunks from {file_path}: {e}", exc_info=True)
                
            print(f"Error extracting chunks from {file_path}: {e}")
            return None, None
    
    def _extract_markdown(self, file_path):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìµœì í™”)"""
        # Check for special characters in the file path
        rel_path = os.path.relpath(file_path, self.vault_path) if hasattr(self, 'vault_path') else file_path
        has_special_chars = any(c in file_path for c in "'\"()[]{},;")
        is_excalidraw = "excalidraw" in file_path.lower()
        
        if has_special_chars:
            logger.debug(f"Extracting markdown from file with special characters: {rel_path}")
        if is_excalidraw:
            logger.debug(f"Processing Excalidraw file: {rel_path}")
            
        try:
            # Log file open operation for tracking potential file access issues
            logger.debug(f"Opening markdown file: {rel_path}")
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
            except UnicodeDecodeError:
                logger.warning(f"UTF-8 decode error for {rel_path}, trying with alternative encodings")
                with open(file_path, 'r', encoding='latin-1') as file:
                    content = file.read()
                    
            # ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ # í—¤ë”© ë˜ëŠ” íŒŒì¼ëª…)
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else os.path.basename(file_path).replace('.md', '')
            logger.debug(f"Extracted title: '{title}' from {rel_path}")
            
            # YAML í”„ë¡ íŠ¸ë§¤í„° ë° íƒœê·¸ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
            tags = []
            
            # ì²˜ë¦¬ ì „ ì›ë³¸ ì½˜í…ì¸  ë³´ì¡´
            original_content = content
            
            # ì½˜í…ì¸ ì—ì„œ í”„ë¡ íŠ¸ë§¤í„° êµ¬ë¬¸ ì¶”ì¶œ
            yaml_match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
            
            if yaml_match:
                # ì›ë˜ í”„ë¡ íŠ¸ë§¤í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                original_frontmatter = yaml_match.group(1)
                
                # í”„ë¡ íŠ¸ë§¤í„° ì „ì²˜ë¦¬ (YAML êµ¬ë¬¸ ë¬¸ì œ ìˆ˜ì •)
                lines = original_frontmatter.split('\n')
                processed_lines = []
                
                for line in lines:
                    if not line.strip() or line.startswith('#'):
                        processed_lines.append(line)
                        continue
                        
                    # 1. í‚¤:ê°’ í˜•íƒœì¸ì§€ í™•ì¸
                    if ':' in line:
                        # í‚¤ì™€ ê°’ ë¶„ë¦¬
                        key_value = line.split(':', 1)
                        key = key_value[0].strip()
                        value = key_value[1].strip() if len(key_value) > 1 else ''
                        
                        # 2. title í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                        if key.lower() == 'title':
                            # ê°’ì— ì½œë¡ ì´ ìˆê±°ë‚˜ íŠ¹ìˆ˜ ë¬¸ìê°€ ìˆëŠ”ì§€ í™•ì¸
                            if ':' in value or any(c in value for c in '&@#%'):
                                # ì´ë¯¸ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì ¸ ìˆì§€ ì•Šìœ¼ë©´ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
                                if not (value.startswith('"') and value.endswith('"')) and not (value.startswith('\'') and value.endswith('\'')):
                                    value = f'"{value}"'
                                line = f"{key}: {value}"
                                logger.debug(f"Quoted title with special chars: {value}")
                        
                        # 3. íƒœê·¸ í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                        elif key.lower() == 'tags':
                            # ë°°ì—´ í˜•íƒœë¡œ í‘œí˜„ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
                            if not value.startswith('[') and not value.startswith('-'):
                                line = f"{key}: [{value}]"
                        
                        # 4. ì¼ë°˜ í•„ë“œì— ì½œë¡ ì´ í¬í•¨ëœ ê²½ìš°
                        elif ':' in value and not (value.startswith('"') or value.startswith('\'')):
                            value = f'"{value}"'
                            line = f"{key}: {value}"
                            
                        # 5. ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê°’ ì²˜ë¦¬
                        elif value and value[0].isdigit() and not (value.startswith('"') or value.startswith('\'')):
                            value = f'"{value}"'
                            line = f"{key}: {value}"
                    
                    processed_lines.append(line)
                
                # ìµœì¢… ì²˜ë¦¬ëœ í”„ë¡ íŠ¸ë§¤í„° í…ìŠ¤íŠ¸
                frontmatter_text = '\n'.join(processed_lines)
                
                # ì•ˆì „ì„ ìœ„í•œ ì¶”ê°€ í•„í„°ë§ (URL ì•ˆì „ ë¬¸ì ìœ ì§€í•˜ë©´ì„œ ì•…ì„± ë¬¸ìì—´ë§Œ ì œê±°)
                # URLì— ì‚¬ìš©ë˜ëŠ” ë¬¸ì +/# ë“±ì„ ìœ ì§€
                frontmatter_text = re.sub(r'[^\w\s\-\[\]:#\'\",._{}@&%/\\(\)\+=]+', ' ', frontmatter_text)
                
                try:
                    # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê°’ê³¼ ì½œë¡ ì´ í¬í•¨ëœ ê°’ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ YAML ìˆ˜ì •
                    lines = frontmatter_text.split('\n')
                    fixed_lines = []
                    
                    for line in lines:
                        if not line.strip() or line.strip().startswith('#'):
                            fixed_lines.append(line)
                            continue
                            
                        # í‚¤-ê°’ ë¼ì¸ì¸ì§€ í™•ì¸
                        if ':' in line:
                            parts = line.split(':', 1)  # ì²« ë²ˆì§¸ ì½œë¡ ì—ì„œë§Œ ë¶„ë¦¬
                            key_part = parts[0].strip()
                            value_part = parts[1].strip() if len(parts) > 1 else ''
                            
                            # ê°’ì— ì½œë¡ ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            if ':' in value_part and not (value_part.startswith('"') or value_part.startswith('\'')):
                                # title í•„ë“œì— ì½œë¡ ì´ ìˆëŠ” ê²½ìš° (ì˜ˆ: title: Robots and jobs: Evidence from US labor markets)
                                value_part = f'"{value_part}"'
                                line = f'{key_part}: {value_part}'
                                logger.debug(f"Added quotes to value with colon: {value_part}")
                            
                            # PDF íŒŒì¼ ì°¸ì¡° ì²˜ë¦¬ (ì˜ˆ: [PDF] 2008.pdf)
                            elif '[PDF]' in value_part and re.search(r'\d+\.pdf', value_part):
                                value_part = f'"{value_part}"'
                                line = f'{key_part}: {value_part}'
                            
                            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê°’ì— ë”°ì˜´í‘œ ì¶”ê°€
                            elif value_part and value_part[0].isdigit() and not (value_part.startswith('"') or value_part.startswith('\'')):
                                value_part = f'"{value_part}"'
                                line = f'{key_part}: {value_part}'
                                
                            # URL í˜•ì‹ ë¬¸ì œ ì‚¬ì „ ì²˜ë¦¬ (share_link í¬í•¨)
                            elif any(url_field in key_part.lower() for url_field in ['url', 'link', 'share_link', 'source']):
                                # https: ë¬¸ì œ ìˆ˜ì • (ìŠ¬ë˜ì‹œê°€ ë¹ ì§„ URL)
                                if 'https:' in value_part and not 'https://' in value_part:
                                    value_part = value_part.replace('https:', 'https://')
                                # http: ë¬¸ì œë„ ê°™ì´ ìˆ˜ì •
                                if 'http:' in value_part and not 'http://' in value_part:
                                    value_part = value_part.replace('http:', 'http://')
                                    
                                # URLì— ê³µë°±ì´ ìˆìœ¼ë©´ ë”°ì˜´í‘œë¡œ ë¬¶ê¸°
                                if not (value_part.startswith('"') or value_part.startswith('\'')):
                                    if ' ' in value_part or '+' in value_part or '#' in value_part:
                                        value_part = f'"{value_part}"'
                                line = f'{key_part}: {value_part}'
                                logger.debug(f"Special handling for URL field {key_part}: {value_part[:30]}...")
                                
                            # title í•„ë“œì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬ (ì¼ë°˜ì ìœ¼ë¡œ ì½œë¡ ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
                            elif key_part.lower() == 'title' and not (value_part.startswith('"') or value_part.startswith('\'')):
                                # ì´ë¯¸ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì ¸ ìˆì§€ ì•Šìœ¼ë©´ ë¬´ì¡°ê±´ ë”°ì˜´í‘œ ì¶”ê°€
                                value_part = f'"{value_part}"'
                                line = f'{key_part}: {value_part}'
                                
                        fixed_lines.append(line)
                    
                    frontmatter_fixed = '\n'.join(fixed_lines)
                    logger.debug(f"Fixed potential formatting issues in frontmatter for {rel_path}")
                    
                    # YAML íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ í´ë°± (í–¥ìƒëœ ë²„ì „)
                    frontmatter = {}
                    
                    # íŠ¹ìˆ˜ URL í•„ë“œë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ (ì¼ë°˜ ì •ê·œì‹ìœ¼ë¡œëŠ” URL ì²˜ë¦¬ê°€ ì–´ë ¤ì›€)
                    url_fields = ['url', 'link', 'share_link', 'source']
                    url_pattern = re.compile(r'^((?:' + '|'.join(url_fields) + ')(?:_[\w]+)?)\s*:\s*(.+)$', re.IGNORECASE)
                    
                    # ë¨¼ì € URL í•„ë“œ ì¶”ì¶œ ì‹œë„
                    for line in frontmatter_text.split('\n'):
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                            
                        url_match = url_pattern.match(line)
                        if url_match:
                            key = url_match.group(1).strip()
                            value = url_match.group(2).strip()
                            # URL ê°’ì—ì„œ ë”°ì˜´í‘œ ì œê±°
                            if (value.startswith('"') and value.endswith('"')) or (value.startswith('\'') and value.endswith('\'')):
                                value = value[1:-1]
                            frontmatter[key] = value
                            logger.debug(f"Extracted URL field with regex: {key}: {value[:30]}...")
                    
                    # ë‚˜ë¨¸ì§€ ì¼ë°˜ í‚¤-ê°’ ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹
                    pattern = re.compile(r'^([\w\-]+)\s*:\s*(.+)$')
                    
                    # ë‚˜ë¨¸ì§€ ì¼ë°˜ í‚¤-ê°’ ì¶”ì¶œ
                    for line in frontmatter_text.split('\n'):
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                            
                        match = pattern.match(line)
                        if match:
                            key = match.group(1).strip()
                            value = match.group(2).strip()
                            # íƒœê·¸ í•„ë“œ ì²˜ë¦¬ëŠ” ë³„ë„ë¡œ ì§„í–‰
                            if key.lower() == 'tags':
                                if value.startswith('[') and value.endswith(']'):
                                    tag_list = value[1:-1].split(',')
                                    frontmatter['tags'] = [tag.strip().strip('"\'\'') for tag in tag_list if tag.strip()]
                                else:
                                    frontmatter['tags'] = [value] if value else []
                            else:
                                frontmatter[key] = value
                    
                    # íƒœê·¸ ì¶”ì¶œ
                    if 'tags' in frontmatter:
                        tags_data = frontmatter['tags']
                        if isinstance(tags_data, list):
                            tags = [str(tag).strip() for tag in tags_data if tag]
                        elif isinstance(tags_data, str):
                            tags = [tags_data.strip()]
                        logger.debug(f"Extracted {len(tags)} tags from frontmatter for {rel_path}")
                        
                    # URL í•„ë“œê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì²˜ë¦¬
                    if 'url' in frontmatter and isinstance(frontmatter['url'], str):
                        url_value = frontmatter['url']
                        if not url_value.startswith(('http://', 'https://')):
                            # URL í˜•ì‹ ìë™ ìˆ˜ì •
                            if url_value.startswith('www.'):
                                frontmatter['url'] = f"https://{url_value}"
                                logger.debug(f"Fixed URL format in frontmatter: {frontmatter['url']}")
                            elif ' ' in url_value and not url_value.startswith('"'):
                                # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ URLì˜ ê³µë°± ë¬¸ì œ ë°©ì§€
                                frontmatter['url'] = f"\"{url_value}\""
                except yaml.YAMLError as yaml_err:
                    logger.error(f"YAML parsing error in {rel_path}: {yaml_err}")
                except Exception as e:
                    # Special handling for files with special characters
                    if has_special_chars:
                        logger.warning(f"Special character handling for frontmatter in {rel_path}: {e}")
                        logger.warning(f"YAML parsing error in file with special characters: {rel_path}")
                    elif is_excalidraw:
                        logger.warning(f"YAML parsing error in Excalidraw file: {rel_path}")
                    else:
                        logger.error(f"Error processing frontmatter in {rel_path}: {e}")
                        logger.warning(f"YAML parsing error: falling back to regex for {rel_path}")
                        
                    error_msg = f"YAML parsing error: falling back to regex for {os.path.basename(file_path)}"
                    print(error_msg)
                    if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                        self.monitor.add_error_log(error_msg)
                    # YAML íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ í´ë°±
                    tag_match = re.search(r'tags:\s*\[(.*?)\]', frontmatter_text, re.DOTALL)
                    if tag_match:
                        tags_str = tag_match.group(1)
                        tags = [tag.strip().strip("'\"") for tag in tags_str.split(',') if tag.strip()]
                    else:
                        tag_lines = re.findall(r'tags:\s*\n((?:\s*-\s*.+\n)+)', frontmatter_text)
                        if tag_lines:
                            # YAML í˜•ì‹ì˜ íƒœê·¸ ì²˜ë¦¬ (ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
                            for line in tag_lines[0].split('\n'):
                                tag_item = re.match(r'\s*-\s*(.+)', line)
                                if tag_item:
                                    tags.append(tag_item.group(1).strip().strip("'\""))
                
                # $~$ ê°™ì€ ìˆ˜ì‹ ê¸°í˜¸ë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                content = re.sub(r'\$~\$', ' ', content)
                content = re.sub(r'\${2}.*?\${2}', ' ', content, flags=re.DOTALL)  # ë¸”ë¡ ìˆ˜ì‹ ì²˜ë¦¬
                content = re.sub(r'\$.*?\$', ' ', content)  # ì¸ë¼ì¸ ìˆ˜ì‹ ì²˜ë¦¬
            
            # ë¶ˆí•„ìš”í•œ ì—¬ëŸ¬ ì¤„ ê³µë°± ì œê±°
            content = re.sub(r'\n{3,}', '\n\n', content)
            
            # í›„í–‰ ê³µë°± ì œê±°
            content = content.rstrip()
        except Exception as e:
            logger.warning(f"Error during content cleanup for {rel_path}: {e}")
        
        logger.info(f"Successfully extracted markdown from {rel_path}: {len(content)} chars, {len(tags)} tags")
        return content, title, tags
        
    def _extract_pdf(self, file_path):
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì†ìƒëœ PDF ì²˜ë¦¬ ê°œì„ )"""
        rel_path = os.path.relpath(file_path, self.vault_path) if hasattr(self, 'vault_path') else file_path
        has_special_chars = any(c in file_path for c in "'\"()[]{},;")
        
        if has_special_chars:
            logger.debug(f"Extracting content from PDF file with special characters: {rel_path}")
            
        try:
            # Log file open operation for tracking potential file access issues
            logger.debug(f"Opening PDF file: {rel_path}")
            
            # Extract text from PDF using PyPDF2
            content = ""
            with open(file_path, 'rb') as file:
                try:
                    # ì•ˆì „í•œ PDF ì½ê¸° ì‹œë„
                    pdf_reader = PyPDF2.PdfReader(file)
                    
                    # '/Root' KeyError ê°™ì€ ë¬¸ì œë¥¼ ê°ì§€í•˜ê¸° ìœ„í•œ ì•ˆì „ ì²˜ë¦¬
                    try:
                        num_pages = len(pdf_reader.pages)
                    except (KeyError, AttributeError, TypeError) as struct_err:
                        # PDF êµ¬ì¡° ë¬¸ì œ (ì†ìƒë˜ê±°ë‚˜ ì•”í˜¸í™”ëœ PDF)
                        logger.error(f"PDF structure error in {rel_path}: {struct_err} - likely corrupted or encrypted PDF")
                        return None, None, None  # ì†ìƒëœ PDFëŠ” None ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›€
                    
                    # í˜ì´ì§€ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                    if num_pages == 0:
                        logger.warning(f"PDF file {rel_path} has 0 pages")
                        return None, None, None
                    
                    # Extract text from each page
                    for page_num in range(num_pages):
                        try:
                            page = pdf_reader.pages[page_num]
                            page_text = page.extract_text()
                            if page_text:
                                content += page_text + "\n\n"
                        except Exception as page_err:
                            # íŠ¹ì • í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                            logger.warning(f"Error extracting text from page {page_num} in {rel_path}: {page_err}")
                            continue
                    
                    # ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                    if not content.strip():
                        logger.warning(f"Extracted empty content from PDF {rel_path} - might be a scanned document")
                        return None, None, None
                        
                    # Clean up the content
                    content = content.strip()
                    # Remove excessive newlines
                    content = re.sub(r'\n{3,}', '\n\n', content)
                    
                except KeyError as key_err:
                    # íŠ¹ì • í‚¤ê°€ ì—†ëŠ” ë¬¸ì œ ('/Root' ë“±)
                    logger.error(f"PyPDF2 KeyError processing {rel_path}: {key_err} - PDF may be corrupted")
                    return None, None, None  # ì†ìƒëœ PDFëŠ” None ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›€
                except Exception as pdf_err:
                    logger.error(f"PyPDF2 error processing {rel_path}: {pdf_err}")
                    return None, None, None  # ê¸°íƒ€ ì˜¤ë¥˜ë„ None ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›€
            
            # Use filename as title (remove extension)
            title = os.path.basename(file_path)
            if title.lower().endswith('.pdf'):
                title = title[:-4]
                
            # PDFs don't have tags in our system, so return empty list
            tags = []
            
            logger.info(f"Successfully extracted content from PDF {rel_path}: {len(content)} chars")
            return content, title, tags
            
        except Exception as e:
            logger.error(f"Error processing PDF file {os.path.basename(file_path)}: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì™¸ë¥¼ ì „íŒŒí•˜ì§€ ì•Šê³  None ë°˜í™˜
            return None, None, None
        
    def _extract_pdf_content(self, file_path):
        """PDF íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        content = ""
        title = os.path.basename(file_path).replace('.pdf', '')
        rel_path = os.path.relpath(file_path, self.vault_path)
        
        try:
            with open(file_path, 'rb') as file:
                # ì•ˆì „í•˜ê²Œ PDF ì½ê¸° ì‹œë„
                try:
                    reader = PyPDF2.PdfReader(file)
                    
                    # PDF ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    metadata = reader.metadata
                    if metadata and '/Title' in metadata and metadata['/Title']:
                        title = metadata['/Title']
                        logger.debug(f"Extracted title '{title}' from PDF metadata for {rel_path}")
                    
                    # í˜ì´ì§€ ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )
                    logger.debug(f"Extracting text from {len(reader.pages)} pages in {rel_path}")
                    for i, page in enumerate(reader.pages):
                        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ 10í˜ì´ì§€ë§ˆë‹¤ ì •ë¦¬
                        if i > 0 and i % 10 == 0:
                            gc.collect()
                            logger.debug(f"Garbage collection performed after processing {i} pages")
                            
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                content += page_text + "\n\n"
                        except Exception as e:
                            logger.warning(f"Error extracting text from page {i} in {rel_path}: {e}")
                
                except Exception as e:
                    logger.error(f"Error reading PDF {rel_path}: {e}", exc_info=True)
        
        except Exception as e:
            logger.error(f"Error opening PDF file {rel_path}: {e}", exc_info=True)
        
        # ë¹ˆ ë‚´ìš©ì¸ ê²½ìš° í™•ì¸
        if not content.strip():
            error_msg = f"Warning: No content extracted from PDF {rel_path} - likely a scanned document"
            logger.warning(error_msg)
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            # ìŠ¤ìº”ë³¸ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” PDFëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            return None, None, None
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸ (ìŠ¤ìº”ë³¸ PDF ê°ì§€)
        if len(content) > 0:
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸ - ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¨
            # ìŠ¤ìº”ë³¸ PDFëŠ” ì¢…ì¢… ì˜ë¯¸ ì—†ëŠ” ë¬¸ìë‚˜ ê¸°í˜¸ê°€ ë§ì´ í¬í•¨ë¨
            meaningful_chars = sum(1 for c in content if c.isalnum() or c.isspace())
            total_chars = len(content)
            quality_ratio = meaningful_chars / total_chars if total_chars > 0 else 0
            
            # í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ì€ ê²½ìš° (ìŠ¤ìº”ë³¸ PDFë¡œ ê°„ì£¼)
            if quality_ratio < 0.5 and total_chars > 100:
                error_msg = f"Warning: Low quality text extracted from PDF {file_path} (quality: {quality_ratio:.2f}) - likely a scanned document"
                print(f"\n{Fore.YELLOW}{error_msg}{Style.RESET_ALL}")
                if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                    self.monitor.add_error_log(error_msg)
                return None, None, None
        
        return content, title, []  # PDFëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íƒœê·¸ê°€ ì—†ìŒ
    
    def _split_into_chunks(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì„±ëŠ¥ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìµœì í™”)"""
        if not text:
            return []
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - íŠ¹ìˆ˜ ë¬¸ì ë° ë°˜ë³µë˜ëŠ” ê³µë°± ì •ë¦¬
        # ì´ ë‹¨ê³„ì—ì„œ íŠ¹ìˆ˜ ê¸°í˜¸ë‚˜ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” íŒ¨í„´ì„ ì²˜ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r'\.{3,}', '...', text)  # ì—¬ëŸ¬ ì (...)ì„ í•˜ë‚˜ë¡œ í†µí•©
        
        # ğŸ”§ ENHANCED: ì•ˆì „ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì†ë„ì™€ ì•ˆì •ì„± ê· í˜•)
        # Use safe document length from config - no truncation, just warning
        max_document_length = getattr(config, 'MAX_DOCUMENT_LENGTH', 2000000)  # 2M chars
        if len(text) > max_document_length:
            print(f"Warning: Document very long ({len(text)} chars), processing may take longer")
            # Don't truncate - let chunking handle large documents
            
        # ì‚¬ì „ ê²€ì‚¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
        text_length = len(text)
        chunk_size = config.CHUNK_SIZE
        chunk_overlap = config.CHUNK_OVERLAP
        chunk_min_size = config.CHUNK_MIN_SIZE
            
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        if text_length < chunk_size:
            return [text] if text_length >= chunk_min_size else []
        
        # ì²­í¬ ë¶„í• 
        chunks = []
        start = 0
        
        # ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë¡œì»¬ ë³€ìˆ˜ ìºì‹±
        text_find = text.find
        text_rfind = text.rfind
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ì¥ì¹˜
        max_iterations = text_length * 2  # ê·¹ë‹¨ì ì¸ ê²½ìš°ì—ë„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ë„ë¡
        iteration_count = 0
        
        while start < text_length and iteration_count < max_iterations:
            iteration_count += 1
            
            # ì²­í¬ í¬ê¸° ê³„ì‚° (ìµœì†Œ í¬ê¸° ë³´ì¥)
            end = min(start + chunk_size, text_length)
            
            # ì˜ë¯¸ ë‹¨ìœ„(ë¬¸ì¥, ë¬¸ë‹¨) ê²½ê³„ì—ì„œ ë¶„í• 
            if end < text_length:
                # ë¬¸ë‹¨ ê²½ê³„ ì°¾ê¸°
                paragraph_end = text_find('\n\n', start, end)
                if paragraph_end != -1:
                    end = paragraph_end + 2
                else:
                    # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
                    sentence_end = max(
                        text_rfind('. ', start, end),
                        text_rfind('? ', start, end),
                        text_rfind('! ', start, end),
                        text_rfind('.\n', start, end),
                        text_rfind('?\n', start, end),
                        text_rfind('!\n', start, end)
                    )
                    
                    if sentence_end != -1:
                        end = sentence_end + 2
                    else:
                        # ë‹¨ì–´ ê²½ê³„ ì°¾ê¸°
                        space_pos = text_rfind(' ', start, end)
                        if space_pos != -1:
                            end = space_pos + 1
                        # ê³µë°±ì„ ì°¾ì§€ ëª»í•˜ë©´ ê·¸ëƒ¥ ì²­í¬ í¬ê¸° ì‚¬ìš©
            
            # ì²­í¬ ì¶”ì¶œ
            chunk = text[start:end].strip()
            
            # ìœ íš¨í•œ ì²­í¬ë§Œ ì¶”ê°€
            if len(chunk) >= chunk_min_size:
                chunks.append(chunk)
            
            # ì§„í–‰ ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start == end:
                print(f"Warning: Chunking stuck at position {start}, breaking")
                break
                
            # ë‹¤ìŒ ì²­í¬ë¡œ ì´ë™ (ì˜¤ë²„ë© ì ìš©)
            start = max(start + 1, end - chunk_overlap)  # ìµœì†Œ 1ì ì´ìƒ ì§„í–‰ ë³´ì¥
            
            # ì‹œì‘ ìœ„ì¹˜ê°€ í…ìŠ¤íŠ¸ ëì„ ë„˜ì–´ê°€ëŠ” ê²½ìš° ì¤‘ì§€
            if start >= text_length:
                break
        
        # ë¬´í•œ ë£¨í”„ ê²€ì‚¬
        if iteration_count >= max_iterations:
            print("Warning: Max iterations reached in chunking, potential infinite loop avoided")
        
        # ì¤‘ë³µ ì²­í¬ ì œê±°
        unique_chunks = []
        seen = set()
        for chunk in chunks:
            # ì§§ì€ chunkëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
            if len(chunk) < 100:
                unique_chunks.append(chunk)
            else:
                # ê¸´ ì²­í¬ëŠ” í•´ì‹œë¥¼ í†µí•´ ì¤‘ë³µ ê²€ì‚¬
                chunk_hash = hash(chunk)
                if chunk_hash not in seen:
                    seen.add(chunk_hash)
                    unique_chunks.append(chunk)
        
        # ğŸ”§ ENHANCED: GPU/CPU ì„±ëŠ¥ì— ë”°ë¥¸ ë™ì  ì²­í¬ ê°œìˆ˜ ì œí•œ
        if hasattr(self, 'embedding_model') and hasattr(self.embedding_model, 'hardware_profiler'):
            profile = self.embedding_model.hardware_profiler.performance_profile
            
            # GPU ì„±ëŠ¥ì— ë”°ë¥¸ ì²­í¬ ìˆ˜ ê²°ì •
            if 'professional_gpu' in profile:
                max_chunks = 200  # Tesla, A100, H100 ë“±
            elif 'flagship_gpu' in profile:
                max_chunks = 150  # RTX 5090, RX 7900 XTX ë“±
            elif 'ultra_high_end_gpu' in profile:
                max_chunks = 120  # RTX 5080, RTX 4080 ë“±
            elif 'high_end_gpu' in profile:
                max_chunks = 100  # RTX 5070, RTX 4070 ë“±
            elif 'mid_range_gpu' in profile:
                max_chunks = 80   # RTX 3060, RTX 2080 ë“±
            elif 'low_mid_gpu' in profile:
                max_chunks = 60   # RTX 2060, GTX 1660 Ti ë“±
            elif 'low_end_gpu' in profile:
                max_chunks = 50   # GTX 1650, RX 580 ë“±
            elif 'very_low_end_gpu' in profile:
                max_chunks = 40   # GTX 1050 ë“±
            elif 'high_end_cpu' in profile:
                max_chunks = 60   # ê³ ì„±ëŠ¥ CPU
            elif 'mid_range_cpu' in profile:
                max_chunks = 40   # ì¤‘ê¸‰ CPU
            else:
                max_chunks = 30   # ì €ì„±ëŠ¥ CPU
            
            # í•˜ë“œì›¨ì–´ ì„±ëŠ¥ì´ ì¢‹ë”ë¼ë„ ìµœëŒ€ 100ê°œë¡œ ì œí•œ (ì•ˆì •ì„±)
            # Use config max chunks per file - no arbitrary limits
            max_chunks_per_file = getattr(config, 'MAX_CHUNKS_PER_FILE', 1000)  # 1000
            
            print(f"Dynamic chunk processing based on {profile}: up to {max_chunks_per_file} chunks per file")
        else:
            # Fallback: use config value
            max_chunks_per_file = getattr(config, 'MAX_CHUNKS_PER_FILE', 1000)  # 1000
            print(f"Using config chunk limit: {max_chunks_per_file} chunks per file")
        
        # Process all chunks - no truncation for complete coverage
        if len(unique_chunks) > max_chunks_per_file:
            print(f"Large file detected: {len(unique_chunks)} chunks (will process all chunks)")
            # Don't truncate - process all chunks for complete coverage
        
        # Split long chunks instead of truncating to preserve all content
        safe_chunks = []
        max_chunk_length = getattr(config, 'MAX_CHUNK_LENGTH', 50000)  # 50K chars from config
        
        for chunk in unique_chunks:
            if len(chunk) > max_chunk_length:
                print(f"Long chunk detected: {len(chunk)} chars, splitting into smaller chunks")
                # Split long chunk instead of truncating to preserve content
                chunk_parts = [chunk[i:i+max_chunk_length] for i in range(0, len(chunk), max_chunk_length)]
                safe_chunks.extend(chunk_parts)
                print(f"Split into {len(chunk_parts)} parts to preserve all content")
            else:
                safe_chunks.append(chunk)
        
        unique_chunks = safe_chunks
            
        return unique_chunks
    
    def _save_vectors_to_milvus(self, vectors, chunks, chunk_file_map):
        """ë²¡í„°ì™€ ì²­í¬ ë°ì´í„°ë¥¼ Milvusì— ì €ì¥í•˜ëŠ” ìµœì í™”ëœ ë©”ì†Œë“œ (ë¬¸ìì—´ ê¸¸ì´ ì œí•œ ê°•í™”)
        ê°œë³„ ì²­í¬ ì‚½ì… ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰í•˜ë©°, ì¼ì • ìˆ˜ì¤€ì˜ ì„±ê³µë§Œìœ¼ë¡œë„ ì „ì²´ ì²˜ë¦¬ë¥¼ ì„±ê³µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
        """
        if not vectors or not chunks or not chunk_file_map or len(vectors) != len(chunks):
            return False
            
        # ì´ í•­ëª© ìˆ˜ì™€ ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì¶”ì 
        total_items = len(vectors)
        success_count = 0
        failed_count = 0
        file_chunk_indices = {}  # íŒŒì¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ì¶”ì 
        
        # ì²˜ë¦¬ ì‹œì‘ ë¡œê¹…
        logger.info(f"Starting to save {total_items} vectors to Milvus")
        
        # ê° ì²­í¬ì™€ ë²¡í„° ì²˜ë¦¬
        for i, (vector, chunk, metadata) in enumerate(zip(vectors, chunks, chunk_file_map)):
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (ë” ìì£¼ ì²´í¬)
            if i > 0 and i % 10 == 0:
                self._check_memory_usage(f"Milvus insertion {i}/{total_items}")
                logger.info(f"Progress: {i}/{total_items} items processed. Success: {success_count}, Failed: {failed_count}")
            
            try:
                rel_path = metadata["rel_path"]
                
                # íŒŒì¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ì¶”ì 
                if rel_path not in file_chunk_indices:
                    file_chunk_indices[rel_path] = 0
                chunk_index = file_chunk_indices[rel_path]
                file_chunk_indices[rel_path] += 1
                
                # íƒœê·¸ JSON ë³€í™˜ (ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ)
                try:
                    tags_json = json.dumps(metadata["tags"]) if metadata["tags"] else "[]"
                except Exception as json_error:
                    logger.warning(f"Error converting tags to JSON: {json_error}, using empty array")
                    tags_json = "[]"
                
                # ìµœëŒ€ ë¬¸ìì—´ ê¸¸ì´ (ë” ì•ˆì „í•œ ë§ˆì§„)
                MAX_STRING_LENGTH = 32000  # Milvus ì œí•œ 65535ë³´ë‹¤ ì¶©ë¶„íˆ ì•ˆì „í•˜ê²Œ ì„¤ì •
                MAX_CONTENT_LENGTH = 16000  # content í•„ë“œëŠ” ë” ì§§ê²Œ
                MAX_CHUNK_LENGTH = 16000    # chunk_text í•„ë“œë„ ë” ì§§ê²Œ
                
                # ê°•í™”ëœ ë¬¸ìì—´ ì•ˆì „ ìë¥´ê¸° í•¨ìˆ˜
                def safe_truncate(text, max_len=MAX_STRING_LENGTH):
                    if not isinstance(text, str):
                        return str(text) if text is not None else ""
                    if not text:
                        return ""
                    # UTF-8 ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œë„ í™•ì¸
                    try:
                        text_bytes = text.encode('utf-8', errors='ignore')[:max_len//2]
                        truncated = text_bytes.decode('utf-8', errors='ignore')
                        # ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì ê¸¸ì´ë„ í™•ì¸
                        return truncated[:max_len] if len(truncated) > max_len else truncated
                    except Exception as enc_error:
                        logger.warning(f"Encoding error in safe_truncate: {enc_error}, returning empty string")
                        return ""
                
                # ê° í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ì‚½ì… (ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©)
                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª… ì²˜ë¦¬
                original_path = rel_path  # ì›ë³¸ ê²½ë¡œ ì €ì¥
                
                # ì•ˆì „í•œ íŒŒì¼ ê²½ë¡œ ìƒì„± (í•œê¸€ ë° íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ ê°•í™”)
                file_dir = os.path.dirname(rel_path)
                file_name = os.path.basename(rel_path)
                
                # 1. íŠ¹ìˆ˜ ë¬¸ì ë° ê³µë°±ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
                # ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì ‘ë‘ì‚¬ ì¶”ê°€, íŠ¹ìˆ˜ ë¬¸ìëŠ” ASCIIë¡œ ë³€í™˜ ì‹œë„
                try:
                    # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    if re.match(r'^\d', file_name):
                        # ì ‘ë‘ì‚¬ ì¶”ê°€
                        safe_file_name = f"file_{file_name}"
                    else:
                        safe_file_name = file_name
                    
                    # íŠ¹ìˆ˜ ë¬¸ìê°€ ìˆëŠ”ì§€ í™•ì¸
                    if re.search(r'[^\w\-\. ]', safe_file_name):
                        # URL ì¸ì½”ë”©ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
                        # í•œê¸€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ìœ„í—˜í•œ íŠ¹ìˆ˜ ë¬¸ìë§Œ ì²˜ë¦¬
                        safe_file_name = safe_file_name.replace('\\', '_').replace('/', '_').replace(':', '_')\
                                                    .replace('*', '_').replace('?', '_').replace('"', '_')\
                                                    .replace('<', '_').replace('>', '_').replace('|', '_')\
                                                    .replace('\t', '_').replace('\n', '_')
                    
                    # ìµœì¢… ì•ˆì „ ê²½ë¡œ ìƒì„±
                    safe_path = os.path.join(file_dir, safe_file_name)
                    logger.debug(f"Created safe path: {safe_path} from original: {rel_path}")
                except Exception as path_error:
                    # ê²½ë¡œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ê²½ë¡œ ì‚¬ìš©
                    logger.warning(f"Error creating safe path: {path_error}, using original path")
                    safe_path = rel_path
                
                # ì•ˆì „í•œ ì œëª© ìƒì„± (íŒŒì¼ ê²½ë¡œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
                original_title = metadata.get("title", "")
                
                try:
                    # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    if original_title and re.match(r'^\d', original_title):
                        safe_title = f"Title_{original_title}"
                    else:
                        safe_title = original_title
                    
                    # íŠ¹ìˆ˜ ë¬¸ìê°€ ìˆëŠ”ì§€ í™•ì¸
                    if safe_title and re.search(r'[^\w\-\. ]', safe_title):
                        # URL ì¸ì½”ë”©ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
                        # í•œê¸€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ìœ„í—˜í•œ íŠ¹ìˆ˜ ë¬¸ìë§Œ ì²˜ë¦¬
                        safe_title = safe_title.replace('\\', '_').replace('/', '_').replace(':', '_')\
                                               .replace('*', '_').replace('?', '_').replace('"', '_')\
                                               .replace('<', '_').replace('>', '_').replace('|', '_')\
                                               .replace('\t', '_').replace('\n', '_')
                    
                    logger.debug(f"Created safe title: {safe_title} from original: {original_title}")
                except Exception as title_error:
                    # ì œëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì œëª© ì‚¬ìš©
                    logger.warning(f"Error creating safe title: {title_error}, using original title")
                    safe_title = original_title
                
                # í˜„ì¬ íŒŒì¼ ì •ë³´ ì„¤ì • (ë¡œê¹…ìš©)
                current_file = os.path.basename(rel_path)
                
                # ê²½ë¡œ ì •ë³´ë¥¼ path í•„ë“œì— ì €ì¥
                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª…ì€ safe_pathë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ê²°
                # ì¤‘ìš”: original_path í•„ë“œë„ í¬í•¨ (ìŠ¤í‚¤ë§ˆ ìš”êµ¬ì‚¬í•­)
                single_data = {
                    # "id" í•„ë“œëŠ” ì œê±°ë¨ - Milvusì—ì„œ ìë™ ìƒì„±ë¨
                    "path": safe_truncate(safe_path, 500),  # ì•ˆì „í•œ ê²½ë¡œ ì‚¬ìš©
                    "original_path": safe_truncate(original_path, 500),  # ì›ë³¸ ê²½ë¡œ ì¶”ê°€ - ìŠ¤í‚¤ë§ˆ ìš”êµ¬ì‚¬í•­
                    "title": safe_truncate(safe_title, 500),  # ì•ˆì „í•œ ì œëª© ì‚¬ìš©
                    # ì²« ë²ˆì§¸ ì²­í¬ì¼ ë•Œë§Œ ì „ì²´ ë‚´ìš© ì €ì¥, ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ë¬¸ìì—´
                    # ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ content í‚¤ì— ì ‘ê·¼ (ê¸°ë³¸ê°’ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©)
                    "content": safe_truncate(metadata.get("content", ""), MAX_CONTENT_LENGTH) if chunk_index == 0 else "",
                    "chunk_text": safe_truncate(chunk, MAX_CHUNK_LENGTH),  # chunk_text ê¸¸ì´ ì œí•œ ê°•í™”
                    "chunk_index": chunk_index,
                    "file_type": safe_truncate(metadata.get("file_ext", ""), 10),
                    "tags": safe_truncate(tags_json, 1000),
                    "created_at": safe_truncate(metadata.get("created_at", ""), 30),
                    "updated_at": safe_truncate(metadata.get("updated_at", ""), 30),
                    "vector": vector
                }
                
                # ê°•í™”ëœ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (ì•ˆì „ ì¥ì¹˜)
                valid_data = True
                for key, value in single_data.items():
                    if key != "vector" and isinstance(value, str):
                        # ëª¨ë“  ë¬¸ìì—´ í•„ë“œì— ëŒ€í•´ ê°•ì œ ê¸¸ì´ ì œí•œ
                        if key == "content":
                            max_field_len = MAX_CONTENT_LENGTH
                        elif key == "chunk_text":
                            max_field_len = MAX_CHUNK_LENGTH
                        else:
                            max_field_len = MAX_STRING_LENGTH
                        
                        if len(value) > max_field_len:
                            logger.warning(f"Field {key} too long ({len(value)} chars), forcing truncation to {max_field_len}")
                            single_data[key] = value[:max_field_len]
                
                # FINAL SAFETY: ëª¨ë“  ë¬¸ìì—´ì´ ì•ˆì „í•œ ê¸¸ì´ì¸ì§€ ìµœì¢… í™•ì¸
                for key, value in single_data.items():
                    if key != "vector" and isinstance(value, str) and len(value) > 16000:
                        logger.warning(f"EMERGENCY: Field {key} still too long after all checks ({len(value)} chars), emergency truncation")
                        single_data[key] = value[:10000]  # ì‘ê¸‰ ì²˜ì¹˜ - ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ 10Kë¡œ ì œí•œ
                
                # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ ê°œì„  (ì½¤ë§ˆ, ê´´í˜¸, ì¸ìš©ë¶€í˜¸ ë“±)
                sanitized_data = {}
                for key, value in single_data.items():
                    if key == "vector":
                        sanitized_data[key] = value
                    elif isinstance(value, str):
                        # ë¬¸ìì—´ í•„ë“œì˜ ê²½ìš° íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
                        if key in ["path", "title", "original_path"]:  # original_path í•„ë“œ ì¶”ê°€
                            # ê²½ë¡œì™€ ì œëª©ì€ ì¤‘ìš”í•˜ë¯€ë¡œ ì¸ì½”ë”© ë¬¸ì œ í™•ì¸
                            try:
                                # Milvusì—ì„œ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì‹ì— ì¤‘ìš”í•œ íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í•‘
                                escaped_value = value.replace("\\", "\\\\").replace("'", "\\'").replace('"', '\\"')
                                sanitized_data[key] = escaped_value
                            except Exception as esc_error:
                                logger.warning(f"Error escaping special chars in {key}: {esc_error}, using original value")
                                sanitized_data[key] = value
                        else:
                            # ë‹¤ë¥¸ ë¬¸ìì—´ í•„ë“œëŠ” ê¸°ë³¸ ì²˜ë¦¬
                            sanitized_data[key] = value
                    else:
                        sanitized_data[key] = value
                
                # ì¶”ê°€ ê°€ëŠ¥ì„± ê²€ì‚¬ - original_path í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€
                if "original_path" not in sanitized_data and "path" in sanitized_data:
                    # ë°˜ë“œì‹œ original_pathê°€ í¬í•¨ë˜ë„ë¡ ë³´ì¥ (ìŠ¤í‚¤ë§ˆ ìš”êµ¬ì‚¬í•­)
                    sanitized_data["original_path"] = sanitized_data.get("path", "")  # path ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                    logger.debug(f"Added missing original_path field (schema requirement)")
                    
                # í™•ì¸ìš© ë¡œê¹…
                logger.debug(f"Final fields ready for insertion: {list(sanitized_data.keys())}")
                if "original_path" not in sanitized_data:
                    logger.warning(f"WARNING: original_path field is still missing after all fixes")
                else:
                    logger.debug(f"original_path field is present with value: '{sanitized_data['original_path'][:30]}...'")
                    
                # ì¤‘ìš”: id í•„ë“œê°€ ìˆìœ¼ë©´ ì œê±° (Milvusì—ì„œ ìë™ ê´€ë¦¬ë¨)
                if 'id' in sanitized_data:
                    logger.debug(f"Removing 'id' field from sanitized_data to prevent DataNotMatchException")
                    del sanitized_data['id']
                
                # ì´ë¯¸ì§€ ì°¸ì¡° íƒì§€ ë° ì œê±° (![[...]] í˜•íƒœ ì²˜ë¦¬)
                if 'chunk_text' in sanitized_data and isinstance(sanitized_data['chunk_text'], str):
                    # ì´ë¯¸ì§€ ì°¸ì¡° ì œê±° ë° ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
                    image_pattern = re.compile(r'!\[\[Pasted image [^\]]+\]\]')
                    sanitized_data['chunk_text'] = image_pattern.sub(' [IMAGE] ', sanitized_data['chunk_text'])
                    
                    # ë§Œì•½ contentë„ ìˆë‹¤ë©´ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                    if 'content' in sanitized_data and isinstance(sanitized_data['content'], str):
                        sanitized_data['content'] = image_pattern.sub(' [IMAGE] ', sanitized_data['content'])
                
                # í•œê¸€ ë° íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
                for key in ['path', 'title', 'original_path', 'chunk_text', 'content']:
                    if key in sanitized_data and isinstance(sanitized_data[key], str):
                        # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì¶”ê°€ ë¡œê¹…
                        if any('\u3131' <= c <= '\ud7a3' for c in sanitized_data[key]):
                            logger.debug(f"Field {key} contains Korean characters")
                            
                            # ê¸¸ì´ ì œí•œ ì ìš© - í•œê¸€ì€ ì¼ë°˜ì ìœ¼ë¡œ ë” ë§ì€ ë°”ì´íŠ¸ë¥¼ ì°¨ì§€í•¨
                            max_length = min(len(sanitized_data[key]), 800 if key in ['chunk_text', 'content'] else 500)
                            if len(sanitized_data[key]) > max_length:
                                sanitized_data[key] = sanitized_data[key][:max_length]
                                logger.debug(f"Truncated {key} with Korean content to {max_length} characters")
                
                # íŠ¹ìˆ˜ ë¬¸ìë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš° ì¶”ê°€ ë¡œê¹…
                has_special_chars = False
                for key, value in sanitized_data.items():
                    if key != "vector" and isinstance(value, str) and any(c in value for c in "'\"()[]{},;"):
                        has_special_chars = True
                        logger.debug(f"Field {key} contains special characters that might need careful handling")
                
                        try:
                            encoded_bytes = value.encode('utf-8')
                            byte_len = len(encoded_bytes)
                            # íŠ¹ì • ë²”ìœ„ì˜ ë°”ì´íŠ¸ ê°’ ì¶œë ¥ (ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìœ„ì¹˜ í™•ì¸)
                            if byte_len > 100:
                                sample_bytes = encoded_bytes[:50] + b'...' + encoded_bytes[-50:]
                                logger.debug(f"  Field {key} encoding (byte length: {byte_len}): {sample_bytes}")
                        except Exception as enc_error:
                            logger.warning(f"  Field {key} has encoding issues: {enc_error}")

                # 4. ì‚½ì… ì‹œë„ ì „ Milvus ë¬¸ì„œ í™•ì¸
                try:
                    # ì½œë ‰ì…˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ìš”ì²­
                    schema = self.milvus_manager.collection.schema
                    field_names = [field.name for field in schema.fields]
                    logger.debug(f"  Milvus schema fields: {field_names}")

                    # ìŠ¤í‚¤ë§ˆì— ì—†ëŠ” í•„ë“œ ì°¾ê¸°
                    extra_fields = [key for key in sanitized_data.keys() if key not in field_names]
                    if extra_fields:
                        logger.warning(f"  Fields not in schema: {extra_fields} - these might cause errors")

                        # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
                        for field in extra_fields:
                            if field in sanitized_data:
                                del sanitized_data[field]
                                logger.debug(f"Removed extra field '{field}' not in schema")

                        # ìŠ¤í‚¤ë§ˆì— ìˆì§€ë§Œ ë°ì´í„°ì— ì—†ëŠ” í•„ë“œ ì°¾ê¸°
                        missing_fields = [name for name in field_names if name not in sanitized_data and name != 'vector']
                        if missing_fields:
                            logger.warning(f"  Missing fields from schema: {missing_fields}")

                            # í•„ìˆ˜ í•„ë“œ ì¶”ê°€ (ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©)
                            for field in missing_fields:
                                if field != "vector":  # ë²¡í„° í•„ë“œëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                                    sanitized_data[field] = ""
                                    logger.debug(f"Added missing field '{field}' required by schema")
                except Exception as schema_error:
                    logger.warning(f"  Could not verify schema compatibility: {schema_error}")

                # 5. ì‚½ì… ì‹œë„ (ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§)
                logger.debug(f"  Attempting to insert data for {current_file}...")

                # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ - ìµœëŒ€ 5íšŒë¡œ ì¦ê°€
                max_retries = 5
                retry_count = 0
                last_error = None

                # ë°ì´í„° ì „ì²˜ë¦¬: ê²½ë¡œ ë° íŒŒì¼ ì´ë¦„ íŠ¹ë³„ ì²˜ë¦¬
                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª…ì— ì ‘ë‘ì‚¬ ì¶”ê°€
                if 'path' in sanitized_data and isinstance(sanitized_data['path'], str):
                    # ê²½ë¡œì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                    filename = os.path.basename(sanitized_data['path'])
                    if filename and filename[0].isdigit():
                        # ì›ë˜ ê²½ë¡œ ë³´ì¡´
                        if 'original_path' not in sanitized_data:
                            sanitized_data['original_path'] = sanitized_data['path']
                        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª…ì— ì ‘ë‘ì‚¬ ì¶”ê°€
                        sanitized_data['path'] = os.path.join(
                            os.path.dirname(sanitized_data['path']),
                            f"file_{filename}"
                        )
                        logger.debug(f"Added prefix to numeric filename: {filename} -> file_{filename}")
                
                # ì œëª©ì´ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                if 'title' in sanitized_data and isinstance(sanitized_data['title'], str):
                    if sanitized_data['title'] and sanitized_data['title'][0].isdigit():
                        sanitized_data['title'] = f"Title_{sanitized_data['title']}"
                        logger.debug(f"Added prefix to numeric title: {sanitized_data['title']}")
                
                # ì´ë¯¸ì§€ ì°¸ì¡° íŒ¨í„´ ê°ì§€ ë° ì •ë¦¬ (![[Pasted image...]])
                image_pattern = re.compile(r'!\[\[(Pasted image[^\]]+)\]\]')
                for key, value in list(sanitized_data.items()):
                    if key != "vector" and isinstance(value, str) and '![[' in value:
                        sanitized_data[key] = image_pattern.sub(r'Image: \1', value)
                        logger.debug(f"Sanitized image references in field '{key}'")
                
                # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë° íŠ¹ìˆ˜ ë¬¸ì ì¸ì½”ë”© ë¬¸ì œ ì²˜ë¦¬ - ëª¨ë“  ë¬¸ìì—´ í•„ë“œì— ëŒ€í•´ ì¶”ê°€ ì²˜ë¦¬
                for key, value in list(sanitized_data.items()):
                    if key != "vector" and isinstance(value, str):
                        # í•œêµ­ì–´ íŠ¹ìˆ˜ ì²˜ë¦¬: ê¸¸ì´ ì œí•œ ì ìš© (ë°”ì´íŠ¸ ê¸°ì¤€)
                        if any(ord(c) > 127 for c in value):
                            # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ë°”ì´íŠ¸ ê¸¸ì´ ê³„ì‚° ë° ì œí•œ
                            try:
                                byte_length = len(value.encode('utf-8'))
                                max_bytes = 2000  # Milvus ê¶Œì¥ ìµœëŒ€ ë°”ì´íŠ¸ ìˆ˜
                                
                                if byte_length > max_bytes:
                                    # ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°
                                    truncated = ''
                                    current_bytes = 0
                                    for char in value:
                                        char_bytes = len(char.encode('utf-8'))
                                        if current_bytes + char_bytes <= max_bytes - 3:  # ì—¬ìœ  ê³µê°„ í™•ë³´
                                            truncated += char
                                            current_bytes += char_bytes
                                        else:
                                            break
                                    sanitized_data[key] = truncated + '...'
                                    logger.debug(f"Korean text in '{key}' truncated from {byte_length} to {current_bytes} bytes")
                            except UnicodeEncodeError as enc_err:
                                logger.warning(f"Korean encoding issue with {key}, applying special handling: {enc_err}")
                                # ì¸ì½”ë”© ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                                sanitized_data[key] = ''.join(c if ord(c) < 128 else '?' for c in value[:200]) + '...'
                        
                        # ì¼ë°˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ë° ë¬¸ì œ ì²˜ë¦¬
                        try:
                            # í…ìŠ¤íŠ¸ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
                            encoded = value.encode('utf-8')
                        except UnicodeEncodeError as enc_err:
                            # ì¸ì½”ë”© ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° asciië¡œ í•„í„°ë§
                            logger.warning(f"Encoding issue with {key}, sanitizing: {enc_err}")
                            sanitized_data[key] = value.encode('ascii', 'ignore').decode('ascii')

                while retry_count < max_retries:
                    try:
                        start_time = time.time()
                        
                        # ì œì¼ ë¬˜í•˜ê³  ê°œì„ ëœ ë°©ë²•ìœ¼ë¡œ ì‹œë„
                        if retry_count == 0:
                            # ì²« ë²ˆì§¸ ì‹œë„: ì •ì œëœ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‹œë„
                            result = self.milvus_manager.insert_data(sanitized_data)
                        # ë‘ ë²ˆì§¸ ì‹œë„: ì¼ë¶€ í•„ë“œ ê°„ì†Œí™” ë° YAML ë¬¸ì œ í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                        elif retry_count == 1:
                            # ì¤‘ìš”í•˜ì§€ ì•Šì€ í•„ë“œ ì œê±° í›„ ì¬ì‹œë„
                            minimal_data = dict(sanitized_data)
                            for field in ['tags', 'created_at', 'updated_at']:
                                if field in minimal_data:
                                    del minimal_data[field]
                            
                            # YAML í”„ë¡ íŠ¸ë§¤í„° ê´€ë ¨ íŠ¹ìˆ˜ ë¬¸ì ë¬¸ì œ ì²˜ë¦¬
                            if 'title' in minimal_data and isinstance(minimal_data['title'], str):
                                # ì½œë¡ ì´ í¬í•¨ëœ ì œëª© ì²˜ë¦¬
                                if ':' in minimal_data['title']:
                                    minimal_data['title'] = minimal_data['title'].replace(':', ' - ')
                                    logger.debug(f"Replaced colons in title with hyphens")
                                # ë”°ì˜´í‘œ ì²˜ë¦¬
                                if '"' in minimal_data['title'] or "'" in minimal_data['title']:
                                    minimal_data['title'] = minimal_data['title'].replace('"', '').replace("'", '')
                                    logger.debug(f"Removed quotes from title")
                            
                            result = self.milvus_manager.insert_data(minimal_data)
                        # ì„¸ ë²ˆì§¸ ì‹œë„: ë…¸ì´ì¦ˆê°€ ìˆëŠ” í•„ë“œ í‘œì¤€í™” ë° Excalidraw íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
                        elif retry_count == 2:
                            # ëª¨ë“  ë¬¸ìì—´ í•„ë“œ ë” ê°•ë ¥í•˜ê²Œ ì •ì œ
                            ultra_safe_data = dict(sanitized_data)
                            for key, value in ultra_safe_data.items():
                                if key != "vector" and isinstance(value, str):
                                    # ì•ˆì „í•œ ë¬¸ìë§Œ ìœ ì§€ (ë” ê´€ëŒ€í•œ ë²„ì „ìœ¼ë¡œ ìˆ˜ì •)
                                    if 'excalidraw' in value.lower():
                                        # Excalidraw íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
                                        logger.debug(f"Applying special handling for Excalidraw content in {key}")
                                        ultra_safe_data[key] = f"Excalidraw drawing {self.next_id}"
                                    else:                                  
                                        # í•œê¸€ê³¼ ì˜ì–´ ë° ê¸°ë³¸ ë¬¸ì¥ ë¶€í˜¸ ìœ ì§€, ë‚˜ë¨¸ì§€ íŠ¹ìˆ˜ë¬¸ì ì¹˜í™˜
                                        ultra_safe_data[key] = re.sub(r'[^\w\-\. ,;:\(\)\[\]ê°€-í£]', '_', value)[:200]
                            result = self.milvus_manager.insert_data(ultra_safe_data)
                        # ë„¤ ë²ˆì§¸ ì‹œë„: ìƒ‰ì¸ ì›ë³¸ íŒŒì¼ì„ ìµœì†Œí•œ í•„ë“œë¡œë§Œ êµ¬ì„±
                        elif retry_count == 3:
                            # í•„ìˆ˜ í•„ë“œë§Œì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì‚½ì… ì‹œë„ - í•œê¸€ ì§€ì› ê°•í™”
                            # ì›ë³¸ ê²½ë¡œ ë³´ì¡´
                            safe_path = f"safe_path_{self.next_id}"
                            
                            # ì›ë³¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì•ˆì „í•œ ê°’ ìƒì„±
                            if original_path:
                                safe_original = original_path[:100]
                            else:
                                # í˜„ì¬ íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ì¶œ ì‹œë„
                                if current_file and isinstance(current_file, str):
                                    safe_original = current_file[:100]
                                else:
                                    safe_original = f"fallback_path_{self.next_id}"
                            
                            # ì²­í¬ í…ìŠ¤íŠ¸ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            if isinstance(chunk, str):
                                # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                                if any(ord(c) > 127 for c in chunk):
                                    safe_chunk = ''.join(c for c in chunk[:50] if ord(c) < 1000) + '...'
                                else:
                                    safe_chunk = chunk[:100]
                            else:
                                safe_chunk = f"Safe chunk text {self.next_id}"
                            
                            fallback_data = {
                                "path": safe_path,
                                "original_path": safe_original,
                                "title": f"Safe Title {self.next_id}",
                                "chunk_text": safe_chunk,
                                "chunk_index": chunk_index,
                                "vector": vector
                            }
                            result = self.milvus_manager.insert_data(fallback_data)
                        # ë§ˆì§€ë§‰ ì‹œë„: ê³ ì • ê°’ ì‚¬ìš©
                        else:
                            # ê³ ì • ê°’ì„ ì‚¬ìš©í•œ ê°€ì¥ ì•ˆì „í•œ ì‚½ì… ì‹œë„
                            emergency_data = {
                                "path": f"emergency_path_{self.next_id}",
                                "original_path": f"emergency_original_path_{self.next_id}",
                                "title": f"Emergency Title {self.next_id}",
                                "chunk_text": f"Emergency chunk {self.next_id}",
                                "chunk_index": 0,
                                "vector": vector
                            }
                            result = self.milvus_manager.insert_data(emergency_data)
                            
                        end_time = time.time()

                        # ì„±ê³µ ì‹œ ì¶”ê°€ ì •ë³´ ë¡œê¹…
                        success_count += 1
                        if retry_count > 0:
                            logger.info(f"Successfully inserted data for file: {current_file} after {retry_count+1} attempts (took {end_time - start_time:.2f}s)")
                        else:
                            logger.info(f"Successfully inserted data for file: {current_file} (took {end_time - start_time:.2f}s)")

                        logger.debug(f"  Insert result: {result}")
                        break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ë¹„í™œì„±í™”
                    except Exception as insert_error:
                        retry_count += 1
                        last_error = insert_error

                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹… ê°œì„ 
                        error_type = type(insert_error).__name__
                        error_message = str(insert_error)

                        logger.warning(f"Insert attempt {retry_count}/{max_retries} failed: {error_type} - {error_message[:100]}...")

                        # ì¬ì‹œë„ ì „ ì¶”ê°€ ì¡°ì¹˜ (ì˜¤ë¥˜ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì ìš©)
                        if "schema" in error_message.lower() or "DataNotMatchException" in error_type:
                            # ìŠ¤í‚¤ë§ˆ ë¬¸ì œì¸ ê²½ìš° ê¸°ë¡ ë° ë‹¤ìŒ ì‹œë„ì— ëŒ€ë¹„
                            logger.debug(f"Schema issue detected, will try alternative approach in next retry")
                            # ì•„ë¬´ ì²˜ë¦¬ë„ í•˜ì§€ ì•ŠìŒ - ë‹¤ìŒ ì‹œë„ì—ì„œ ë‹¤ë¥¸ ì „ëµ ì‚¬ìš©
                        elif "timeout" in error_message.lower() or "connection" in error_message.lower():
                            # ì—°ê²° ë¬¸ì œì¸ ê²½ìš° ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                            time.sleep(1.0)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                            logger.debug(f"Connection issue, waiting before retry {retry_count}")
                        else:
                            # ê¸°íƒ€ ì˜¤ë¥˜ì— ëŒ€í•œ ë¡œê¹…
                            logger.debug(f"General error in retry {retry_count}, will use more aggressive sanitization in next attempt")

                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´ ì˜¤ë¥˜ ì²˜ë¦¬
                        if retry_count >= max_retries:
                            failed_count += 1
                            logger.error(f"Failed to insert data for {current_file} after {max_retries} attempts")
                            # ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´ ì¶”ê°€ ë¡œê¹…
                            logger.error(f"Final error: {error_type} - {error_message}")

                # 6. ì¼ì • ê°œìˆ˜ë§ˆë‹¤ flush - ë©”ëª¨ë¦¬ ê´€ë¦¬
                if success_count % 10 == 0:
                    try:
                        flush_start = time.time()
                        self.milvus_manager.collection.flush()
                        flush_end = time.time()
                        logger.debug(f"Successfully flushed after {success_count} insertions (took {flush_end - flush_start:.2f}s)")
                    except Exception as flush_error:
                        logger.warning(f"Non-critical flush error (continuing): {flush_error}")
                    
                # ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ì¸ ê²½ìš°
                else:  # valid_dataê°€ Falseì¸ ê²½ìš°
                    failed_count += 1
                    logger.warning(f"Skipping invalid data for item {self.next_id} (data validation failed)")
            
            except Exception as overall_error:  # ì „ì²´ ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜ˆì™¸
                # ì‚½ì… ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ í•­ëª©ì€ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                failed_count += 1
                error_type = type(overall_error).__name__
                error_message = str(overall_error)
                
                # current_fileì€ ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨
                    
                # 1. ê¸°ë³¸ ì˜¤ë¥˜ ì •ë³´ ë¡œê¹…
                logger.error(f"Failed to insert data for file: {current_file}")
                logger.error(f"Error type: {error_type}, Message: {error_message}")
                
                # 2. ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ì™€ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ë¡œê¹…
                import traceback
                stack_trace = traceback.format_exc()
                logger.error(f"Exception stack trace:\n{stack_trace}")
                
                # 3. ì˜¤ë¥˜ ë¶„ì„
                if "DataNotMatchException" in error_type or "schema" in error_message.lower():
                    logger.error("This appears to be a schema mismatch error. Check if field definitions match Milvus schema.")
                    # í•„ë“œ ì •ë³´ ì¶œë ¥
                    logger.error("Field values that might be causing the error:")
                    for key, value in sanitized_data.items():
                        if key != "vector":
                            value_type = type(value).__name__
                            value_preview = str(value)[:50] + "..." if isinstance(value, str) and len(str(value)) > 50 else value
                            logger.error(f"  Field '{key}' ({value_type}): {value_preview}")
                
                elif "timeout" in error_message.lower() or "connection" in error_message.lower():
                    logger.error("This appears to be a connection or timeout issue with Milvus.")
                    # ì—°ê²° ì •ë³´ ë¡œê¹…
                    logger.error(f"Milvus connection info: {self.milvus_manager.host}:{self.milvus_manager.port}")
                
                elif "quota" in error_message.lower() or "limit" in error_message.lower():
                    logger.error("This appears to be a quota or limit exceeded error in Milvus.")
                    # ì¼ë¶€ ë°ì´í„° í¬ê¸° ì¶œë ¥
                    for key, value in sanitized_data.items():
                        if key != "vector" and isinstance(value, str):
                            logger.error(f"  Field '{key}' length: {len(value)} chars")
                    
                    # 4. ì „ì²´ ë°ì´í„° ì •ë³´ ë””ë²„ê¹… ë¡œê¹…
                    logger.error("Complete data details for debugging:")
                    for field_name, field_value in sanitized_data.items():
                        if field_name != "vector":
                            value_type = type(field_value).__name__
                            value_preview = str(field_value)[:100] + "..." if isinstance(field_value, str) and len(str(field_value)) > 100 else field_value
                            logger.error(f"  Field '{field_name}' ({value_type}): {value_preview}")
                    
                    # 5. ë²¡í„° ì •ë³´ ë¡œê¹…
                    vector = sanitized_data.get("vector", [])
                    logger.error(f"  Vector dimension: {len(vector)}")
                    
                    # 6. ë¬¸ìì—´ í•„ë“œì˜ ë°”ì´íŠ¸ í¬ê¸° í™•ì¸ (ì¸ì½”ë”© ë¬¸ì œ ì§„ë‹¨)
                    for key, value in sanitized_data.items():
                        if key != "vector" and isinstance(value, str):
                            try:
                                encoded_bytes = value.encode('utf-8')
                                logger.error(f"  Field '{key}' byte length: {len(encoded_bytes)}")
                                
                                # ìœ„í—˜í•œ ë¬¸ì ì°¾ê¸°
                                for i, char in enumerate(value[:100]):
                                    if ord(char) > 127 or char in '\\\'"`<>{}[]':
                                        logger.error(f"  Field '{key}' contains potentially problematic character '{char}' (ord={ord(char)}) at position {i}")
                            except Exception as enc_error:
                                logger.error(f"  Field '{key}' encoding error: {enc_error}")
                    
                    # 7. ì˜¤ë¥˜ ì •ë³´ ìì„¸íˆ ê¸°ë¡í•˜ì§€ë§Œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰
                    logger.debug(f"Continuing with next item despite insertion error for item {self.next_id}")
                
                # ID ì¦ê°€ (í•­ìƒ ì¦ê°€í•´ì•¼ ì¤‘ë³µ ID ë°©ì§€)
                self.next_id += 1
                
            except Exception as item_error:
                # í•­ëª© ìì²´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒí•´ë„ ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ê³„ì† ì§„í–‰
                failed_count += 1
                logger.error(f"Error processing item {i}/{total_items}: {item_error}", exc_info=True)
                # IDëŠ” í•­ìƒ ì¦ê°€ (ì•ˆì „ì¥ì¹˜)
                self.next_id += 1
        
        # ìµœì¢… flush ì‹œë„
        try:
            self.milvus_manager.collection.flush()
            logger.info("Final flush completed successfully")
        except Exception as final_flush_error:
            logger.warning(f"Error during final flush (non-critical): {final_flush_error}")
        
        # ìµœì¢… ê²°ê³¼ ë¡œê¹…
        success_rate = (success_count / total_items) * 100 if total_items > 0 else 0
        logger.info(f"Vector insertion complete. Total: {total_items}, Success: {success_count}, Failed: {failed_count}, Success Rate: {success_rate:.1f}%")
        
        # ì„±ê³µë¥  50% ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        # ë˜ëŠ” ì ì–´ë„ í•˜ë‚˜ì˜ í•­ëª©ì´ ì„±ê³µí–ˆê³  ì‹¤íŒ¨ê°€ ì ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        success_threshold = 0.5  # 50% ì„±ê³µë¥  ì„ê³„ê°’
        min_success_count = 1    # ìµœì†Œ ì„±ê³µ í•­ëª© ìˆ˜
        
        if (total_items > 0 and success_count / total_items >= success_threshold) or \
           (success_count >= min_success_count and success_count > failed_count):
            logger.info(f"Vector insertion considered successful with {success_rate:.1f}% success rate")
            return True
        else:
            logger.warning(f"Vector insertion considered failed with only {success_rate:.1f}% success rate")
            return False
    
    def _fast_decision_engine(self, file_path, file_mtime, existing_mtime, file_size):
        """OPTIMIZATION: 3-Tier fast decision making for file processing"""
        rel_path = os.path.relpath(file_path, self.vault_path)
        
        # Cache key for this file
        cache_key = f"{rel_path}:{file_mtime}:{file_size}"
        
        # Check cache first
        if cache_key in self.verification_cache:
            cached_result = self.verification_cache[cache_key]
            print(f"{Fore.BLUE}[CACHED] {rel_path}: {cached_result['decision']} ({cached_result['reason']}){Style.RESET_ALL}")
            return cached_result['decision'], cached_result['reason']
        
        # Calculate time difference
        time_diff = abs(file_mtime - existing_mtime) if existing_mtime > 0 else float('inf')
        
        decision = None
        reason = ""
        
        # TIER 1: Lightning Fast Decisions (90%+ of files)
        if time_diff > self.FAST_PROCESS_THRESHOLD:
            # File definitely modified - immediate process
            decision = "PROCESS"
            reason = f"definitely modified (time_diff: {time_diff:.2f}s)"
            print(f"{Fore.GREEN}[FAST-PROCESS] {rel_path}: {reason}{Style.RESET_ALL}")
            
        elif time_diff < self.FAST_SKIP_THRESHOLD:
            # File very likely unchanged - immediate skip (low risk)
            decision = "SKIP"
            reason = f"very likely unchanged (time_diff: {time_diff:.2f}s)"
            print(f"{Fore.CYAN}[FAST-SKIP] {rel_path}: {reason}{Style.RESET_ALL}")
            
        else:
            # TIER 2: Smart Batch Check for ambiguous cases
            decision = "VERIFY"
            reason = f"ambiguous timestamp (time_diff: {time_diff:.2f}s) - needs verification"
            print(f"{Fore.YELLOW}[NEED-VERIFY] {rel_path}: {reason}{Style.RESET_ALL}")
        
        # Cache the result
        result = {"decision": decision, "reason": reason}
        self.verification_cache[cache_key] = result
        
        return decision, reason

    def _batch_existence_check(self, suspect_files):
        """OPTIMIZATION: Batch check multiple files at once"""
        if not suspect_files:
            return {}
            
        try:
            # Build batch query for multiple files
            paths = [os.path.relpath(fp, self.vault_path) for fp, _ in suspect_files]
            path_conditions = " or ".join([f"path == '{path}'" for path in paths])
            
            # Single query to check all suspect files
            results = self.milvus_manager.query(
                expr=f"({path_conditions})",
                output_fields=["path", "id"],
                limit=len(paths) * 10  # Assume max 10 chunks per file
            )
            
            # Count chunks per file
            file_chunk_counts = {}
            for result in results:
                path = result.get("path")
                if path:
                    file_chunk_counts[path] = file_chunk_counts.get(path, 0) + 1
            
            print(f"{Fore.CYAN}[BATCH-CHECK] Verified {len(suspect_files)} files in single query{Style.RESET_ALL}")
            
            return file_chunk_counts
            
        except Exception as e:
            print(f"{Fore.YELLOW}[WARNING] Batch check failed: {e}{Style.RESET_ALL}")
            return {}
    
    def _normalize_timestamp(self, timestamp_value):
        """Normalize timestamp to float for reliable comparison"""
        try:
            if timestamp_value is None:
                return 0.0
            
            if isinstance(timestamp_value, (int, float)):
                return float(timestamp_value)
            
            if isinstance(timestamp_value, str):
                # Handle empty strings
                if not timestamp_value.strip():
                    return 0.0
                
                # Try direct float conversion
                try:
                    return float(timestamp_value)
                except ValueError:
                    # Try parsing as datetime string if needed
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp_value.replace('Z', '+00:00'))
                        return dt.timestamp()
                    except:
                        print(f"{Fore.YELLOW}[WARNING] Could not parse timestamp: {timestamp_value}{Style.RESET_ALL}")
                        return 0.0
            
            print(f"{Fore.YELLOW}[WARNING] Unknown timestamp type: {type(timestamp_value)}{Style.RESET_ALL}")
            return 0.0
            
        except Exception as e:
            print(f"{Fore.YELLOW}[WARNING] Error normalizing timestamp {timestamp_value}: {e}{Style.RESET_ALL}")
            return 0.0
    
    def process_updated_files(self):
        """ë³¼íŠ¸ì˜ ìƒˆë¡œìš´ íŒŒì¼ ë˜ëŠ” ìˆ˜ì •ëœ íŒŒì¼ë§Œ ì²˜ë¦¬ + ì‚­ì œëœ íŒŒì¼ ì •ë¦¬ (ì¦ë¶„ ì„ë² ë”©) - ENHANCED VERSION"""
        print(f"\n{Fore.CYAN}[DEBUG] Starting FIXED process_updated_files with deleted file cleanup{Style.RESET_ALL}")
        print(f"Processing new/modified files AND cleaning up deleted files in {self.vault_path}")
        
        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.vault_path):
            error_msg = f"Error: Obsidian vault path not found: {self.vault_path}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
        
        print(f"\n{Fore.CYAN}[DEBUG] Vault path exists: {self.vault_path}{Style.RESET_ALL}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ í™•ì¸
        try:
            test_file = os.path.join(self.vault_path, "test_access.txt")
            with open(test_file, 'w') as f:
                f.write("test access")
            os.remove(test_file)
            print(f"{Fore.CYAN}[DEBUG] File system access test passed{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error: Cannot write to vault directory: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False
            return 0
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
        self.embedding_in_progress = True
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": time.time(),
            "current_file": "",
            "estimated_time_remaining": "Calculating...",
            "percentage": 0,
            "is_full_reindex": False,  # ì¦ë¶„ ì„ë² ë”©ì€ ì „ì²´ ì¬ì¸ë±ì‹±ì´ ì•„ë‹˜
            "cpu_percent": 0,
            "memory_percent": 0,
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì§„í–‰ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.embedding_progress["processed_size"] = 0
        self.embedding_progress["processed_files"] = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° ì „ Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Checking Milvus connection...{Style.RESET_ALL}")
            test_query = self.milvus_manager.query("id >= 0", limit=1)
            print(f"\n{Fore.CYAN}[DEBUG] Milvus connection successful. Query result: {test_query}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error connecting to Milvus: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Testing embedding model...{Style.RESET_ALL}")
            test_vector = self.embedding_model.get_embedding("Test embedding model")
            print(f"\n{Fore.CYAN}[DEBUG] Embedding model OK, vector dimension: {len(test_vector)}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error with embedding model: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # Get existing file information - IMPROVED VERSION (more robust)
        existing_files_info = {}
        
        try:
            print(f"{Fore.CYAN}[DEBUG] Querying existing files from Milvus (intelligent batch sizing)...{Style.RESET_ALL}")
            # Use MilvusManager's intelligent batch sizing
            max_limit = self.milvus_manager._get_optimal_query_limit()
            offset = 0
            
            while True:
                # Get only path and updated_at for timestamp comparison
                results = self.milvus_manager.query(
                    output_fields=["path", "updated_at"],
                    limit=max_limit,
                    offset=offset,
                    expr="id >= 0"
                )
                
                if not results:
                    break
                    
                for doc in results:
                    path = doc.get("path")
                    updated_at = doc.get('updated_at')
                    
                    if path and path not in existing_files_info:
                        # Store normalized timestamp
                        normalized_timestamp = self._normalize_timestamp(updated_at)
                        existing_files_info[path] = normalized_timestamp
                
                offset += max_limit
                if len(results) < max_limit:
                    break
                
                # Memory management
                gc.collect()
                
            print(f"{Fore.CYAN}[DEBUG] Found {len(existing_files_info)} unique files in DB{Style.RESET_ALL}")
            
        except Exception as e:
            print(f"Warning: Error fetching existing files: {e}")
        
        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ ë° ì‚­ì œëœ íŒŒì¼ íƒì§€
        print("Scanning files for changes and detecting deleted files...")
        files_to_process = []
        skipped_count = 0
        total_files_count = 0
        total_files_size = 0
        new_or_modified_count = 0
        new_or_modified_size = 0
        
        # í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œì˜ íŒŒì¼ë“¤ ìˆ˜ì§‘
        fs_files = set()
        
        print(f"{Fore.CYAN}[DEBUG] Walking through directory: {self.vault_path}{Style.RESET_ALL}")
        
        for root, _, files in os.walk(self.vault_path):
            # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
            if os.path.basename(root).startswith(('.', '_')):
                print(f"{Fore.CYAN}[DEBUG] Skipping hidden directory: {root}{Style.RESET_ALL}")
                continue
                
            for file in files:
                # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                    total_files_count += 1
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, self.vault_path)
                    
                    # íŒŒì¼ ì‹œìŠ¤í…œ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                    fs_files.add(rel_path)
                    
                    try:
                        # íŒŒì¼ í¬ê¸° ë° ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
                        file_size = os.path.getsize(full_path)
                        total_files_size += file_size
                        file_mtime = os.path.getmtime(full_path)
                        
                        # OPTIMIZED: Use fast decision engine
                        file_mtime = os.path.getmtime(full_path)
                        existing_mtime = self._normalize_timestamp(existing_files_info.get(rel_path, 0))
                        
                        decision, reason = self._fast_decision_engine(full_path, file_mtime, existing_mtime, file_size)
                        
                        if decision == "PROCESS":
                            new_or_modified_count += 1
                            new_or_modified_size += file_size
                            
                            if rel_path in existing_files_info:
                                self.milvus_manager.mark_for_deletion(rel_path)
                            
                            files_to_process.append((full_path, file_size))
                            
                        elif decision == "SKIP":
                            skipped_count += 1
                            
                        elif decision == "VERIFY":
                            # Will be handled in batch verification below
                            files_to_process.append((full_path, file_size))
                    except Exception as e:
                        print(f"Warning: Error checking file {rel_path}: {e}")
        
        # ENHANCED: ì‚­ì œëœ íŒŒì¼ íƒì§€ ë° ì •ë¦¬
        print(f"\n{Fore.MAGENTA}[DELETED FILES CLEANUP] Detecting deleted files...{Style.RESET_ALL}")
        deleted_files = set(existing_files_info.keys()) - fs_files
        
        if deleted_files:
            print(f"{Fore.YELLOW}Found {len(deleted_files)} deleted files:{Style.RESET_ALL}")
            
            # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ í‘œì‹œ
            display_count = min(5, len(deleted_files))
            for i, file_path in enumerate(list(deleted_files)[:display_count]):
                print(f"{Fore.YELLOW}  {i+1}. {file_path}{Style.RESET_ALL}")
            
            if len(deleted_files) > display_count:
                print(f"{Fore.YELLOW}  ... and {len(deleted_files) - display_count} more files{Style.RESET_ALL}")
            
            print(f"\n{Fore.CYAN}Starting automatic cleanup of deleted files...{Style.RESET_ALL}")
            
            try:
                # ì‚­ì œëœ íŒŒì¼ë“¤ì˜ ì„ë² ë”© ì •ë¦¬
                cleanup_count = self.cleanup_deleted_embeddings(list(deleted_files))
                
                if cleanup_count > 0:
                    print(f"{Fore.GREEN}âœ… Successfully cleaned up {cleanup_count} deleted files{Style.RESET_ALL}")
                else:
                    print(f"{Fore.YELLOW}âš ï¸ No deleted files were cleaned up{Style.RESET_ALL}")
                    
            except Exception as e:
                print(f"{Fore.RED}Error during deleted files cleanup: {e}{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}âœ… No deleted files found - database is in sync{Style.RESET_ALL}")
        
        # ì‚­ì œ í‘œì‹œëœ íŒŒì¼ë“¤ ì¼ê´„ ì‚­ì œ (ìˆ˜ì •ëœ íŒŒì¼ë“¤ì˜ ì´ì „ ë²„ì „)
        self.milvus_manager.execute_pending_deletions()
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸ - ì´ ê°’ë“¤ì´ ì „ì²´ ì§„í–‰ë„ì˜ ë¶„ëª¨ê°€ ë¨
        self.embedding_progress["total_files"] = new_or_modified_count
        self.embedding_progress["total_size"] = new_or_modified_size
        
        # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
        print(f"\n{Fore.CYAN}[SUMMARY] File processing summary:{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Total files scanned: {total_files_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] New or modified files: {new_or_modified_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Skipped files (unchanged): {skipped_count}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}[DEBUG] Deleted files cleaned up: {len(deleted_files)}{Style.RESET_ALL}")
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì •ë³´ ì¶œë ¥
        print(f"Total files to process: {new_or_modified_count} files ({new_or_modified_size/(1024*1024):.2f} MB)")
        
        # OPTIMIZATION: Performance summary
        total_scanned = total_files_count
        if total_scanned > 0:
            print(f"\n{Fore.CYAN}[PERFORMANCE SUMMARY]{Style.RESET_ALL}")
            print(f"Files scanned: {total_scanned}")
            print(f"Processing decisions: {len(files_to_process)}/{total_scanned} ({len(files_to_process)/total_scanned*100:.1f}%)")
            print(f"Skipped decisions: {skipped_count}/{total_scanned} ({skipped_count/total_scanned*100:.1f}%)")
            print(f"Cache entries: {len(self.verification_cache)}")
        
        # ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        if not files_to_process:
            print(f"{Fore.GREEN}[INFO] No new or modified files found. Nothing to process.{Style.RESET_ALL}")
            if len(deleted_files) > 0:
                print(f"{Fore.GREEN}[INFO] However, {len(deleted_files)} deleted files were cleaned up.{Style.RESET_ALL}")
            self.embedding_in_progress = False
            return len(deleted_files)  # Return count of cleaned up files
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
        self.start_monitoring()
        
        try:
            # ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            current_batch_size = self._check_system_resources()
            
            # íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
            processed_count = self._process_file_batch(files_to_process, current_batch_size)
            
            # ì„ë² ë”© ì™„ë£Œ í‘œì‹œ
            self.embedding_in_progress = False
            print(f"\n{Fore.GREEN}[SUCCESS] Incremental embedding & deleted cleanup completed successfully!{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Processed {processed_count} new/modified files{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Cleaned up {len(deleted_files)} deleted files{Style.RESET_ALL}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return processed_count + len(deleted_files)
            
        except Exception as e:
            error_msg = f"Error in process_updated_files: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ë² ë”© ì¤‘ì§€
            self.embedding_in_progress = False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return 0
        finally:
            # í•­ìƒ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.stop_monitoring()
            print(f"\n{Fore.CYAN}[DEBUG] Exiting process_updated_files{Style.RESET_ALL}")
    
    def process_all_files(self):
        """ë³¼íŠ¸ì˜ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë° ì„±ëŠ¥ ê°œì„ )
        ì „ì²´ ì„ë² ë”© - ëª¨ë“  íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹"""
        print(f"\n{Fore.CYAN}[DEBUG] Starting process_all_files (FULL REINDEXING){Style.RESET_ALL}")
        print(f"Processing all files in {self.vault_path} - All files will be reprocessed")
        
        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.vault_path):
            error_msg = f"Error: Obsidian vault path not found: {self.vault_path}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
        
        print(f"\n{Fore.CYAN}[DEBUG] Vault path exists: {self.vault_path}{Style.RESET_ALL}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ í™•ì¸
        try:
            test_file = os.path.join(self.vault_path, "test_access.txt")
            with open(test_file, 'w') as f:
                f.write("test access")
            os.remove(test_file)
            print(f"{Fore.CYAN}[DEBUG] File system access test passed{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error: Cannot write to vault directory: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False
            return 0
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
        self.embedding_in_progress = True
        self.embedding_progress = {
            "total_files": 0,
            "processed_files": 0,
            "total_size": 0,
            "processed_size": 0,
            "start_time": time.time(),
            "current_file": "",
            "estimated_time_remaining": "Calculating...",
            "percentage": 0,
            "is_full_reindex": True,
            "cpu_percent": 0,
            "memory_percent": 0,
            "current_batch_size": self.dynamic_batch_size
        }
        
        # ì§„í–‰ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.embedding_progress["processed_size"] = 0
        self.embedding_progress["processed_files"] = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° ì „ Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Checking Milvus connection...{Style.RESET_ALL}")
            test_query = self.milvus_manager.query("id >= 0", limit=1)
            print(f"\n{Fore.CYAN}[DEBUG] Milvus connection successful. Query result: {test_query}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error connecting to Milvus: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            print(f"\n{Fore.CYAN}[DEBUG] Testing embedding model...{Style.RESET_ALL}")
            test_vector = self.embedding_model.get_embedding("Test embedding model")
            print(f"\n{Fore.CYAN}[DEBUG] Embedding model OK, vector dimension: {len(test_vector)}{Style.RESET_ALL}")
        except Exception as e:
            error_msg = f"Error with embedding model: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            self.embedding_in_progress = False  # ë°˜ë“œì‹œ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
            return 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚°
        print("Calculating total files and size...")
        total_files_count = 0
        total_files_size = 0
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ê³„ì‚°
        for root, _, files in os.walk(self.vault_path):
            # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
            if os.path.basename(root).startswith(('.', '_')):
                continue
                
            for file in files:
                # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                    total_files_count += 1
                    file_path = os.path.join(root, file)
                    try:
                        file_size = os.path.getsize(file_path)
                        total_files_size += file_size
                    except Exception as e:
                        print(f"Error getting file size for {file_path}: {e}")
        
        # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸ - ì´ ê°’ë“¤ì´ ì „ì²´ ì§„í–‰ë„ì˜ ë¶„ëª¨ê°€ ë¨
        self.embedding_progress["total_files"] = total_files_count
        self.embedding_progress["total_size"] = total_files_size
        print(f"Found {total_files_count} files with total size of {total_files_size/(1024*1024):.2f} MB")
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
        self.start_monitoring()
        
        try:
            # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            print(f"\n{Fore.CYAN}[DEBUG] Collecting files to process...{Style.RESET_ALL}")
            
            # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            existing_files_info = {}
            try:
                print(f"{Fore.CYAN}[DEBUG] Querying existing files from Milvus (intelligent batch sizing)...{Style.RESET_ALL}")
                # Use MilvusManager's intelligent batch sizing
                max_limit = self.milvus_manager._get_optimal_query_limit()
                offset = 0
                
                while True:
                    results = self.milvus_manager.query(
                        output_fields=["path", "updated_at"],
                        limit=max_limit,
                        offset=offset,
                        expr="id >= 0"
                    )
                    
                    if not results:
                        break
                        
                    for doc in results:
                        path = doc.get("path")
                        if path and path not in existing_files_info:
                            existing_files_info[path] = doc.get('updated_at')
                    
                    offset += max_limit
                    if len(results) < max_limit:
                        break
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬
                    gc.collect()
            except Exception as e:
                print(f"Warning: Error fetching existing files: {e}")
            
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            print("Scanning files...")
            files_to_process = []
            skipped_count = 0
            
            # ì´ë¯¸ ë¯¸ë¦¬ ê³„ì‚°í•œ ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸°ë¥¼ ì‚¬ìš©
            # ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
            total_files = self.embedding_progress["total_files"]
            total_size = self.embedding_progress["total_size"]
            
            print(f"{Fore.CYAN}[DEBUG] Walking through directory: {self.vault_path}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Total files to process: {total_files} files ({total_size/(1024*1024):.2f} MB){Style.RESET_ALL}")
            file_count = 0
            
            for root, _, files in os.walk(self.vault_path):
                # ë””ë ‰í† ë¦¬ë³„ ë¡œê·¸ ì¶”ê°€
                print(f"{Fore.CYAN}[DEBUG] Scanning directory: {root}{Style.RESET_ALL}")
                
                # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
                if os.path.basename(root).startswith(('.', '_')):
                    print(f"{Fore.CYAN}[DEBUG] Skipping hidden directory: {root}{Style.RESET_ALL}")
                    continue
                    
                for file in files:
                    # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                    if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                        file_count += 1
                        # íŒŒì¼ ê²½ë¡œ
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, self.vault_path)
                        print(f"{Fore.CYAN}[DEBUG] Found file {file_count}: {rel_path}{Style.RESET_ALL}")
                        
                        # ë¬¸ì œê°€ ìˆëŠ” íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
                        if "(shorter version(2)).md" in full_path:
                            print(f"Found problematic file: {full_path}")
                            # ì´ íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
                            try:
                                # íŒŒì¼ ë‚´ìš© í™•ì¸
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                
                                # íŒŒì¼ ë‚´ìš©ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ê²½ìš° ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìê°€ ë§ì€ ê²½ìš°
                                if content.count('$~') > 10:
                                    print("File contains many math placeholders, cleaning...")
                                    # íŒŒì¼ ì •ë¦¬ ë° ì €ì¥
                                    content = re.sub(r'\$~\$\n+', '\n', content)
                                    content = re.sub(r'\n{3,}', '\n\n', content)
                                    content = content.rstrip()
                                    
                                    # ì •ë¦¬ëœ íŒŒì¼ ì €ì¥
                                    with open(full_path + ".cleaned", 'w', encoding='utf-8') as f:
                                        f.write(content)
                                    print(f"Cleaned file saved to {full_path}.cleaned")
                            except Exception as e:
                                print(f"Error analyzing problematic file: {e}")
                        
                        # ì „ì²´ ì„ë² ë”©ì—ì„œëŠ” ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬ (ìˆ˜ì • ì‹œê°„ ë¹„êµ ì—†ìŒ)
                        try:
                            # ì„ë² ë”© ì§„í–‰ ì •ë³´ì— ì „ì²´ ì¬ì²˜ë¦¬ ëª¨ë“œ í‘œì‹œ (ì´ë¯¸ main.pyì—ì„œ ì„¤ì •ë¨)
                            
                            # PDF íŒŒì¼ ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸ (í•„ìš”ì— ë”°ë¼ ê±´ë„ˆë›° ìˆ˜ ìˆìŒ)
                            if file.lower().endswith('.pdf') and getattr(config, 'SKIP_PDF_IN_FULL_EMBEDDING', False):
                                print(f"Skipping PDF in full embedding: {rel_path}")
                                skipped_count += 1
                                continue
                                
                            # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì‚­ì œ í‘œì‹œ
                            if rel_path in existing_files_info:
                                print(f"Reprocessing existing file: {rel_path}")
                                self.milvus_manager.mark_for_deletion(rel_path)
                        except Exception as e:
                            print(f"Warning: Error checking file {rel_path}: {e}")
                        
                        # íŒŒì¼ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                        file_size = os.path.getsize(full_path)
                        
                        # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                        files_to_process.append((full_path, file_size))
                        
                        # íŒŒì¼ ëª©ë¡ì´ ë„ˆë¬´ ì»¤ì§€ë©´ ì¤‘ê°„ì— ì²˜ë¦¬
                        if len(files_to_process) >= 10000:
                            print(f"Reached 10,000 files, processing batch...")
                            current_batch_size = self._check_system_resources()
                            self._process_file_batch(files_to_process, current_batch_size)
                            files_to_process = []
                            
                            # ë©”ëª¨ë¦¬ ì •ë¦¬
                            gc.collect()
                            self.embedding_model.clear_cache()
            
            # ì‚­ì œ í‘œì‹œëœ íŒŒì¼ë“¤ë§Œ ì¼ê´„ ì‚­ì œ
            # ì „ì²´ ì¬ìƒ‰ì¸ ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ main.pyì—ì„œ recreate_choiceì— ë”°ë¼ ì»´ë ‰ì…˜ì„ ì¬ìƒì„±í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì‚­ì œ ì‘ì—…ì„ í•˜ì§€ ì•ŠìŒ
            self.milvus_manager.execute_pending_deletions()
            
            # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
            print(f"{Fore.CYAN}[DEBUG] Files found in this scan: {file_count}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Files to process: {len(files_to_process)}{Style.RESET_ALL}")
            print(f"{Fore.CYAN}[DEBUG] Skipped files: {skipped_count}{Style.RESET_ALL}")
            
            # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì •ë³´ ì¶œë ¥
            print(f"Total files to process: {total_files} files ({total_size/(1024*1024):.2f} MB)")
            
            # ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
            if not files_to_process:
                print(f"{Fore.YELLOW}[WARNING] No files found to process. Creating test file for demonstration.{Style.RESET_ALL}")
                # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
                test_dir = os.path.join(self.vault_path, "test")
                os.makedirs(test_dir, exist_ok=True)
                test_file = os.path.join(test_dir, "test_file.md")
                
                # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‘ì„±
                with open(test_file, "w", encoding="utf-8") as f:
                    f.write("# Test File\n\nThis is a test file for embedding process demonstration.\n\n## Section 1\n\nSome content here.\n\n## Section 2\n\nMore content here.")
                
                # íŒŒì¼ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                file_size = os.path.getsize(test_file)
                
                # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                files_to_process.append((test_file, file_size))
                
                # ì „ì²´ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì—…ë°ì´íŠ¸
                self.embedding_progress["total_files"] = 1
                self.embedding_progress["total_size"] = file_size
                
                print(f"Created test file: {test_file} ({file_size} bytes)")
            
            # íŒŒì¼ ë°°ì¹˜ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
            if files_to_process:
                first_five = files_to_process[:5]
                print(f"{Fore.CYAN}[DEBUG] First 5 files in batch: {[os.path.basename(fp) for fp, _ in first_five]}{Style.RESET_ALL}")
            
            # ë‚¨ì€ íŒŒì¼ ì²˜ë¦¬
            if files_to_process:
                print(f"{Fore.CYAN}[DEBUG] Processing files...{Style.RESET_ALL}")
                current_batch_size = self._check_system_resources()
                processed_count = self._process_file_batch(files_to_process, current_batch_size)
                print(f"{Fore.CYAN}[DEBUG] Batch processing completed. Processed {processed_count} files.{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}[WARNING] No files to process after filtering!{Style.RESET_ALL}")
                # íŒŒì¼ì´ ì—†ì–´ë„ ì„ë² ë”© ì§„í–‰ ìƒíƒœë¥¼ ìœ ì§€í•˜ì—¬ ì§„í–‰ë°”ê°€ í‘œì‹œë˜ë„ë¡ í•¨
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            self.embedding_model.clear_cache()
            
            print(f"Successfully processed files, skipped {skipped_count} unchanged files")
            
            # ëª…ì‹œì ìœ¼ë¡œ ì„±ê³µ ì½”ë“œ ë°˜í™˜
            return 1
            
        except Exception as e:
            error_msg = f"Error in process_all_files: {e}"
            print(f"\n{Fore.RED}{error_msg}{Style.RESET_ALL}")
            import traceback
            print(f"\n{Fore.RED}Stack trace:\n{traceback.format_exc()}{Style.RESET_ALL}")
            if hasattr(self, 'monitor') and hasattr(self.monitor, 'add_error_log'):
                self.monitor.add_error_log(error_msg)
            return 0
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.stop_monitoring()
            
            # ì„ë² ë”© ì§„í–‰ ìƒíƒœ ì™„ë£Œ
            self.embedding_in_progress = False
            self.embedding_progress["percentage"] = 100
            print(f"\n{Fore.CYAN}[DEBUG] Exiting process_all_files with status: {0 if 'error_msg' in locals() else 1}{Style.RESET_ALL}")
    
    def _process_file_batch(self, files_to_process, batch_size):
        """íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§"""
        if not files_to_process:
            print(f"{Fore.YELLOW}[DEBUG] No files to process in batch.{Style.RESET_ALL}")
            return 0
            
        processed_count = 0
        total_files = len(files_to_process)
        total_size = sum(file_size for _, file_size in files_to_process)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ì—…ë°ì´íŠ¸
        print(f"{Fore.CYAN}[DEBUG] Processing batch: {total_files} files in this batch ({total_size/(1024*1024):.2f} MB){Style.RESET_ALL}")
        
        # ì„ë² ë”© ì§„í–‰ ìƒíƒœ í™•ì¸
        self.embedding_in_progress = True
        
        # íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
        first_5_files = [os.path.basename(fp) for fp, _ in files_to_process[:5]]
        logger.debug(f"First 5 files in batch: {first_5_files}")
        print(f"{Fore.CYAN}[DEBUG] First 5 files in batch: {first_5_files}{Style.RESET_ALL}")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        total_size_mb = total_size / (1024 * 1024)
        logger.info(f"Starting batch processing of {total_files} files (total size: {total_size_mb:.2f} MB)")
        with tqdm(total=total_size_mb, desc="Indexing files", unit="MB", ncols=100) as pbar:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, total_files, batch_size):
                batch = files_to_process[i:i+batch_size]
                batch_number = i//batch_size+1
                total_batches = (total_files+batch_size-1)//batch_size
                
                logger.debug(f"Processing batch {batch_number}/{total_batches} with {len(batch)} files")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                self._check_memory_usage(f"Before processing batch {batch_number}/{total_batches}")
                
                # ë°°ì¹˜ ì²˜ë¦¬
                for file_item in batch:
                    try:
                        # íŒŒì¼ ê²½ë¡œì™€ í¬ê¸° ë¶„ë¦¬
                        file_path, file_size = file_item
                        
                        # í˜„ì¬ ì²˜ë¦¬ì¤‘ì¸ íŒŒì¼ í‘œì‹œ
                        rel_path = os.path.relpath(file_path, self.vault_path)
                        logger.debug(f"Processing file: {rel_path}")
                        self.embedding_progress["current_file"] = rel_path
                        self.embedding_progress["processed_files"] += 1
                        
                        # íŒŒì¼ í¬ê¸°ë¥¼ ì¶”ê°€í•˜ê³  processed_sizeëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                        # ì¤‘ìš”: process_fileê°€ ì´ë¯¸ processed_sizeë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                        # ëŒ€ì‹  ì´ë¯¸ ì¶”ê°€ë˜ì–´ ìˆëŠ”ì§€ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸ ì„¤ì •
                        self._processed_this_file = True
                        
                        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
                        success = self.process_file(file_path)
                        if success:
                            logger.debug(f"Successfully processed file: {rel_path}")
                            processed_count += 1
                        else:
                            logger.warning(f"Failed to process file: {rel_path}")
                            
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        file_size_mb = file_size / (1024 * 1024)
                        pbar.update(file_size_mb)
                        
                    except Exception as e:
                        logger.error(f"Error processing file in batch: {rel_path}: {e}", exc_info=True)
                        print(f"Error processing file in batch: {e}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                logger.debug("Running garbage collection and clearing model cache")
                gc.collect()
                self.embedding_model.clear_cache()
        
        return processed_count
    
    def detect_deleted_files(self):
        """ì‚­ì œëœ íŒŒì¼ íƒì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        from colorama import Fore, Style
        
        logger.info("Starting deleted files detection with intelligent batch sizing")
        print(f"{Fore.CYAN}Scanning Milvus database for file paths (intelligent batch sizing)...{Style.RESET_ALL}")
        
        # 1. Milvusì—ì„œ ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)
        db_files = set()
        # Use MilvusManager's intelligent batch sizing
        max_limit = self.milvus_manager._get_optimal_query_limit()
        logger.debug(f"Using optimal query limit of {max_limit} for batch queries")
        offset = 0
        total_db_files = 0
        
        try:
            logger.info("Starting pagination query of Milvus database for file paths")
            while True:
                logger.debug(f"Querying batch with offset {offset}, limit {max_limit}")
                results = self.milvus_manager.query(
                    expr="id >= 0",
                    output_fields=["path"],
                    limit=max_limit,
                    offset=offset
                )
                
                if not results:
                    logger.debug("No more results returned from query")
                    break
                    
                # Check for paths with special characters
                paths_with_special_chars = 0
                
                for doc in results:
                    path = doc.get("path")
                    if path and path not in db_files:
                        # Check for special characters that might cause issues
                        has_special_chars = any(c in path for c in "'\"()[]{},;")
                        if has_special_chars:
                            logger.debug(f"Found path with special characters: {path}")
                            paths_with_special_chars += 1
                            
                        db_files.add(path)
                        total_db_files += 1
                
                if paths_with_special_chars > 0:
                    logger.info(f"Batch contains {paths_with_special_chars} paths with special characters")
                        
                offset += max_limit
                if len(results) < max_limit:
                    logger.debug(f"Received {len(results)} results, which is less than limit {max_limit}. Pagination complete.")
                    break
                    
                # ì§„í–‰ìƒí™© í‘œì‹œ
                if total_db_files % 1000 == 0 and total_db_files > 0:
                    logger.info(f"Found {total_db_files} files in database so far")
                    print(f"{Fore.CYAN}Found {total_db_files} files in database so far...{Style.RESET_ALL}")
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if total_db_files % 5000 == 0:
                    logger.debug("Running garbage collection for memory management")
                    gc.collect()
                
        except Exception as e:
            logger.error(f"Error querying Milvus database: {e}", exc_info=True)
            print(f"{Fore.RED}Error querying Milvus database: {e}{Style.RESET_ALL}")
            return []
            
        logger.info(f"Found {len(db_files)} unique files in Milvus database")
        print(f"{Fore.GREEN}Found {len(db_files)} unique files in Milvus database{Style.RESET_ALL}")
        
        # 2. í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº”
        logger.info("Starting file system scan for comparison with database")
        print(f"{Fore.CYAN}Scanning file system...{Style.RESET_ALL}")
        fs_files = set()
        total_fs_files = 0
        special_char_files = 0
        
        try:
            for root, _, files in os.walk(self.vault_path):
                # ìˆ¨ê²¨ì§„ í´ë” ê±´ë„ˆë›°ê¸°
                if os.path.basename(root).startswith(('.', '_')):
                    logger.debug(f"Skipping hidden directory: {root}")
                    continue
                    
                for file in files:
                    # ë§ˆí¬ë‹¤ìš´ê³¼ PDFë§Œ ì²˜ë¦¬
                    if file.endswith(('.md', '.pdf')) and not file.startswith('.'):
                        full_path = os.path.join(root, file)
                        rel_path = os.path.relpath(full_path, self.vault_path)
                        
                        # Check for special characters that might cause issues
                        has_special_chars = any(c in rel_path for c in "'\"()[]{},;")
                        if has_special_chars:
                            logger.debug(f"Found file system path with special characters: {rel_path}")
                            special_char_files += 1
                            
                        fs_files.add(rel_path)
                        total_fs_files += 1
                        
                        # ì§„í–‰ìƒí™© í‘œì‹œ
                        if total_fs_files % 1000 == 0:
                            logger.info(f"Found {total_fs_files} files in file system so far")
                            print(f"{Fore.CYAN}Scanned {total_fs_files} files in file system...{Style.RESET_ALL}")
                            
                # Occasional garbage collection
                if total_fs_files % 10000 == 0 and total_fs_files > 0:
                    logger.debug("Running garbage collection during file system scan")
                    gc.collect()
                            
        except Exception as e:
            logger.error(f"Error scanning file system: {e}", exc_info=True)
            print(f"{Fore.RED}Error scanning file system: {e}{Style.RESET_ALL}")
            return []
            
        if special_char_files > 0:
            logger.info(f"Found {special_char_files} files with special characters in file system")
            
        logger.info(f"File system scan complete - found {total_fs_files} files")
        print(f"{Fore.GREEN}Found {len(fs_files)} files in file system{Style.RESET_ALL}")
        
        # 3. ì‚­ì œëœ íŒŒì¼ ì°¾ê¸°
        logger.info("Comparing database files with file system to identify deleted files")
        deleted_files = db_files - fs_files
        
        if deleted_files:
            logger.info(f"Found {len(deleted_files)} files that exist in database but not in file system")
            
            # Check for special characters in deleted files paths
            special_chars_in_deleted = [p for p in deleted_files if any(c in p for c in "'\"()[]{},;")]
            if special_chars_in_deleted:
                logger.info(f"Deleted files include {len(special_chars_in_deleted)} paths with special characters")
                logger.debug(f"Sample of deleted files with special characters: {special_chars_in_deleted[:5]}")
                
            print(f"{Fore.YELLOW}Found {len(deleted_files)} deleted files{Style.RESET_ALL}")
        else:
            logger.info("No deleted files found")
            print(f"{Fore.GREEN}No deleted files found{Style.RESET_ALL}")
        
        return list(deleted_files)
    
    def cleanup_deleted_embeddings(self, deleted_files):
        """ì‚­ì œëœ íŒŒì¼ë“¤ì˜ embedding ì œê±°"""
        from colorama import Fore, Style
        
        if not deleted_files:
            logger.info("No files to clean up")
            print(f"{Fore.GREEN}No files to clean up{Style.RESET_ALL}")
            return 0
        
        # Check for files with special characters that might need careful handling
        special_char_files = [p for p in deleted_files if any(c in p for c in "'\"()[]{},;")]
        if special_char_files:
            logger.info(f"Cleanup includes {len(special_char_files)} files with special characters")
            logger.debug(f"Sample of files with special characters: {special_char_files[:5]}")
            
        logger.info(f"Starting cleanup of {len(deleted_files)} deleted files")
        print(f"{Fore.CYAN}Starting cleanup of {len(deleted_files)} deleted files...{Style.RESET_ALL}")
        
        success_count = 0
        error_count = 0
        
        try:
            logger.debug("Marking files for deletion")
            # ë°°ì¹˜ ì‚­ì œë¥¼ ìœ„í•´ pending_deletionsì— ì¶”ê°€
            for file_path in deleted_files:
                try:
                    self.milvus_manager.mark_for_deletion(file_path)
                except Exception as e:
                    logger.warning(f"Error marking file for deletion: {file_path}: {e}")
            
            logger.info("Executing batch deletion of marked files")
            print(f"{Fore.CYAN}Executing batch deletion...{Style.RESET_ALL}")
            
            # ë°°ì¹˜ ì‚­ì œ ì‹¤í–‰
            self.milvus_manager.execute_pending_deletions()
            
            # ì‚­ì œ ê²°ê³¼ í™•ì¸
            logger.info("Verifying deletion results")
            print(f"{Fore.CYAN}Verifying deletion results...{Style.RESET_ALL}")
            
            # ì‚­ì œ í›„ ê²€ì¦
            remaining_files = []
            verification_batch_size = 100  # Smaller batch size for verification to prevent query issues
            
            # Process verification in smaller batches to avoid query issues with special characters
            for i in range(0, len(deleted_files), verification_batch_size):
                batch = deleted_files[i:i+verification_batch_size]
                logger.debug(f"Verifying deletion batch {i//verification_batch_size + 1}/{(len(deleted_files)+verification_batch_size-1)//verification_batch_size}")
                
                for file_path in batch:
                    try:
                        # Check for special characters that might cause issues with query expressions
                        has_special_chars = any(c in file_path for c in "'\"()[]{},;")
                        if has_special_chars:
                            logger.debug(f"Using safe query for file with special characters: {file_path}")
                            # Use a safer query approach for files with special characters
                            expr = self.milvus_manager._sanitize_query_expr(f"path == '{file_path}'")
                        else:
                            expr = f"path == '{file_path}'"
                            
                        # íŒŒì¼ì´ ì—¬ì „íˆ DBì— ìˆëŠ”ì§€ í™•ì¸
                        results = self.milvus_manager.query(
                            expr=expr,
                            output_fields=["path"],
                            limit=1
                        )
                        
                        if results:
                            logger.warning(f"File still exists after deletion attempt: {file_path}")
                            remaining_files.append(file_path)
                            error_count += 1
                        else:
                            logger.debug(f"Successfully deleted: {file_path}")
                            success_count += 1
                    except Exception as e:
                        logger.error(f"Error verifying deletion of {file_path}: {e}", exc_info=True)
                        print(f"{Fore.YELLOW}Warning: Could not verify deletion of {file_path}: {e}{Style.RESET_ALL}")
                        error_count += 1
            
            # ê²°ê³¼ ë³´ê³ 
            logger.info(f"Cleanup results: {success_count} files successfully removed, {error_count} files failed")
            print(f"\n{Fore.GREEN}Cleanup Results:{Style.RESET_ALL}")
            print(f"{Fore.GREEN}âœ… Successfully removed: {success_count} files{Style.RESET_ALL}")
            
            if error_count > 0:
                logger.warning(f"Failed to remove {error_count} files")
                print(f"{Fore.YELLOW}âš ï¸ Failed to remove: {error_count} files{Style.RESET_ALL}")
                
                if remaining_files:
                    # Log all remaining files at debug level, but show only a sample to the user
                    logger.debug(f"Files that could not be deleted: {remaining_files}")
                    
                    # Check for special characters in remaining files
                    special_chars_in_remaining = [p for p in remaining_files if any(c in p for c in "'\"()[]{},;")]
                    if special_chars_in_remaining:
                        logger.warning(f"{len(special_chars_in_remaining)} of the failed files contain special characters")
                    
                    print(f"{Fore.YELLOW}Files that could not be deleted:{Style.RESET_ALL}")
                    for file_path in remaining_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"{Fore.YELLOW}  - {file_path}{Style.RESET_ALL}")
                    if len(remaining_files) > 5:
                        logger.debug(f"Additional failed files: {remaining_files[5:]}")
                        print(f"{Fore.YELLOW}  ... and {len(remaining_files) - 5} more{Style.RESET_ALL}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            logger.debug("Running garbage collection after cleanup operation")
            gc.collect()
            
            logger.info(f"Cleanup operation completed: {success_count} files successfully removed")
            return success_count
            
        except Exception as e:
            logger.error(f"Critical error during cleanup operation: {e}", exc_info=True)
            print(f"{Fore.RED}Error during cleanup: {e}{Style.RESET_ALL}")
            import traceback
            print(f"{Fore.RED}Stack trace: {traceback.format_exc()}{Style.RESET_ALL}")
            return 0