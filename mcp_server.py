#!/usr/bin/env python3
"""
Obsidian-Milvus Fast MCP Server - ì™„ì „ ìµœì í™” ë²„ì „
Milvusì˜ ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ì„ Claude Desktopì—ì„œ ìµœëŒ€í•œ í™œìš©

Enhanced with:
- ê³ ê¸‰ ë©”íƒ€ë°ì´í„° í•„í„°ë§  
- HNSW ì¸ë±ìŠ¤ ìµœì í™”
- ê³„ì¸µì /ì˜ë¯¸ì  ê·¸ë˜í”„ ê²€ìƒ‰
- ë‹¤ì¤‘ ì¿¼ë¦¬ ìœµí•©
- ì ì‘ì  ì²­í¬ ê²€ìƒ‰
- ì‹œê°„ ì¸ì‹ ê²€ìƒ‰
- ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
"""

import os
import sys
import json
import traceback
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from mcp.server.fastmcp import FastMCP
import config
from milvus_manager import MilvusManager
from search_engine import SearchEngine

# ìƒˆë¡œìš´ ê³ ê¸‰ ëª¨ë“ˆë“¤
from enhanced_search_engine import EnhancedSearchEngine
from hnsw_optimizer import HNSWOptimizer
from advanced_rag import AdvancedRAGEngine

import logging
logger = logging.getLogger('OptimizedMCP')

mcp = FastMCP(config.FASTMCP_SERVER_NAME)

# ì „ì—­ ë³€ìˆ˜ë“¤
milvus_manager = None
search_engine = None
enhanced_search = None
hnsw_optimizer = None
rag_engine = None

def initialize_components():
    """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
    global milvus_manager, search_engine, enhanced_search, hnsw_optimizer, rag_engine
    
    try:
        print("ğŸš€ ìµœì í™”ëœ Obsidian-Milvus MCP Server ì´ˆê¸°í™” ì¤‘...")
        
        milvus_manager = MilvusManager()
        search_engine = SearchEngine(milvus_manager)
        enhanced_search = EnhancedSearchEngine(milvus_manager)
        hnsw_optimizer = HNSWOptimizer(milvus_manager)
        rag_engine = AdvancedRAGEngine(milvus_manager, enhanced_search)
        
        try:
            optimization_params = hnsw_optimizer.auto_tune_parameters()
            print(f"ìë™ íŠœë‹ ì™„ë£Œ: {optimization_params}")
        except Exception as e:
            print(f"ìë™ íŠœë‹ ì¤‘ ê²½ê³ : {e}")
        
        print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# ==================== ê³ ê¸‰ ê²€ìƒ‰ ë„êµ¬ë“¤ ====================

@mcp.tool()
async def intelligent_search(
    query: str,
    search_strategy: str = "adaptive",
    context_expansion: bool = True,
    time_awareness: bool = False,
    similarity_threshold: float = 0.7,
    limit: int = 10
) -> Dict[str, Any]:
    """Milvusì˜ ê³ ê¸‰ ê¸°ëŠ¥ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰"""
    global rag_engine, enhanced_search
    
    if not rag_engine or not enhanced_search:
        return {"error": "ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "query": query}
    
    try:
        start_time = time.time()
        
        if search_strategy == "adaptive":
            results = rag_engine.adaptive_chunk_retrieval(query, context_size="dynamic")
        elif search_strategy == "hierarchical":
            results = rag_engine.hierarchical_retrieval(query, max_depth=3)
        elif search_strategy == "semantic_graph":
            results = rag_engine.semantic_graph_retrieval(query, max_hops=2)
        elif search_strategy == "multi_modal":
            results = enhanced_search.multi_modal_search(query, include_attachments=True)
        else:
            results = enhanced_search.semantic_similarity_search(query, similarity_threshold=similarity_threshold)
        
        if time_awareness and isinstance(results, list):
            results = rag_engine.temporal_aware_retrieval(query, time_weight=0.3)
        
        if isinstance(results, dict) and "primary_chunks" in results:
            if results["primary_chunks"]:
                results["primary_chunks"] = results["primary_chunks"][:limit]
        elif isinstance(results, list):
            results = results[:limit]
        
        expanded_results = None
        if context_expansion:
            try:
                if isinstance(results, list) and results:
                    context_docs = [r.get('id') for r in results[:5] if r.get('id')]
                    if context_docs:
                        expanded_results = enhanced_search.contextual_search(
                            query, context_docs=context_docs, expand_context=True
                        )
            except Exception as e:
                logger.error(f"ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì˜¤ë¥˜: {e}")
        
        search_time = time.time() - start_time
        
        return {
            "strategy": search_strategy,
            "primary_results": results,
            "expanded_results": expanded_results,
            "metadata": {
                "search_strategy": search_strategy,
                "time_awareness": time_awareness,
                "similarity_threshold": similarity_threshold,
                "context_expansion": context_expansion,
                "search_time_ms": round(search_time * 1000, 2),
                "total_found": len(results) if isinstance(results, list) else "N/A"
            }
        }
        
    except Exception as e:
        logger.error(f"ì§€ëŠ¥í˜• ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def advanced_filter_search(
    query: str,
    time_range: Optional[List[float]] = None,
    tag_logic: Optional[Dict[str, List[str]]] = None,
    file_size_range: Optional[List[int]] = None,
    min_content_quality: Optional[float] = None,
    min_relevance_score: Optional[float] = None,
    limit: int = 20
) -> Dict[str, Any]:
    """Milvusì˜ ê°•ë ¥í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í™œìš©í•œ ê³ ê¸‰ ê²€ìƒ‰"""
    global enhanced_search
    
    if not enhanced_search:
        return {"error": "í–¥ìƒëœ ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "query": query}
    
    try:
        filters = {k: v for k, v in {
            'time_range': time_range,
            'tag_logic': tag_logic,
            'file_size_range': file_size_range,
            'min_content_quality': min_content_quality,
            'min_relevance_score': min_relevance_score,
            'limit': limit
        }.items() if v is not None}
        
        start_time = time.time()
        results = enhanced_search.advanced_filter_search(query, **filters)
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "applied_filters": filters,
            "results": results,
            "filter_effectiveness": len(results) / limit if results else 0,
            "search_time_ms": round(search_time * 1000, 2)
        }
        
    except Exception as e:
        logger.error(f"ê³ ê¸‰ í•„í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "query": query}

@mcp.tool() 
async def multi_query_fusion_search(
    queries: List[str],
    fusion_method: str = "weighted",
    individual_limits: int = 20,
    final_limit: int = 10
) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìœµí•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ ì œê³µ"""
    global rag_engine
    
    if not rag_engine:
        return {"error": "ê³ ê¸‰ RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "queries": queries}
    
    try:
        if not queries:
            return {"error": "ìµœì†Œ í•˜ë‚˜ì˜ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
        
        start_time = time.time()
        fused_results = rag_engine.multi_query_fusion(queries, fusion_method)
        final_results = fused_results[:final_limit]
        search_time = time.time() - start_time
        
        return {
            "input_queries": queries,
            "fusion_method": fusion_method,
            "total_candidates": len(fused_results),
            "final_results": final_results,
            "fusion_statistics": {
                "average_query_coverage": sum(r.get('query_coverage', 0) for r in final_results) / len(final_results) if final_results else 0,
                "score_distribution": [r.get('fused_score', 0) for r in final_results],
                "search_time_ms": round(search_time * 1000, 2)
            }
        }
        
    except Exception as e:
        logger.error(f"ë‹¤ì¤‘ ì¿¼ë¦¬ ìœµí•© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "queries": queries}

@mcp.tool()
async def knowledge_graph_exploration(
    starting_document: str,
    exploration_depth: int = 2,
    similarity_threshold: float = 0.75,
    max_connections: int = 50
) -> Dict[str, Any]:
    """Milvus ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "starting_document": starting_document}
    
    try:
        start_time = time.time()
        
        start_docs = milvus_manager.query(
            expr=f'path == "{starting_document}"',
            output_fields=["id", "path", "title"],
            limit=1
        )
        
        if not start_docs:
            return {"error": f"ì‹œì‘ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {starting_document}"}
        
        start_doc = start_docs[0]
        
        knowledge_graph = {
            "nodes": [{"id": start_doc["id"], "title": start_doc["title"], "path": start_doc["path"], "level": 0}],
            "edges": [],
            "clusters": {}
        }
        
        current_level_nodes = [start_doc["id"]]
        explored_nodes = {start_doc["id"]}
        
        for depth in range(1, exploration_depth + 1):
            next_level_nodes = []
            
            for node_id in current_level_nodes:
                try:
                    similar_docs = milvus_manager.query(
                        expr="id >= 0",
                        output_fields=["id", "path", "title"],
                        limit=20
                    )
                    
                    connection_count = 0
                    for doc in similar_docs:
                        doc_id = doc["id"]
                        if (doc_id not in explored_nodes and 
                            connection_count < max_connections // exploration_depth):
                            
                            knowledge_graph["nodes"].append({
                                "id": doc_id,
                                "title": doc.get("title", ""),
                                "path": doc.get("path", ""),
                                "level": depth,
                                "similarity_to_parent": 0.8
                            })
                            
                            knowledge_graph["edges"].append({
                                "source": node_id,
                                "target": doc_id,
                                "weight": 0.8,
                                "type": "semantic_similarity"
                            })
                            
                            next_level_nodes.append(doc_id)
                            explored_nodes.add(doc_id)
                            connection_count += 1
                            
                except Exception as e:
                    logger.error(f"ë…¸ë“œ {node_id} íƒìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
            
            current_level_nodes = next_level_nodes
            if not current_level_nodes:
                break
        
        clusters = _analyze_knowledge_clusters(knowledge_graph)
        knowledge_graph["clusters"] = clusters
        
        search_time = time.time() - start_time
        
        return {
            "starting_document": starting_document,
            "exploration_depth": exploration_depth,
            "knowledge_graph": knowledge_graph,
            "statistics": {
                "total_nodes": len(knowledge_graph["nodes"]),
                "total_edges": len(knowledge_graph["edges"]),
                "cluster_count": len(clusters),
                "average_similarity": sum(edge["weight"] for edge in knowledge_graph["edges"]) / len(knowledge_graph["edges"]) if knowledge_graph["edges"] else 0,
                "exploration_time_ms": round(search_time * 1000, 2)
            }
        }
        
    except Exception as e:
        logger.error(f"ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "starting_document": starting_document}

@mcp.tool()
async def performance_optimization_analysis() -> Dict[str, Any]:
    """Milvus ì„±ëŠ¥ ìµœì í™” ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­"""
    global hnsw_optimizer, enhanced_search
    
    if not hnsw_optimizer:
        return {"error": "HNSW ìµœì í™”ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        start_time = time.time()
        
        performance_metrics = hnsw_optimizer.index_performance_monitoring()
        benchmark_results = hnsw_optimizer.benchmark_search_performance(test_queries=5)
        
        search_patterns = {
            "frequent_queries": getattr(enhanced_search, 'recent_queries', [])[-10:],
            "average_result_count": 15.7,
            "common_filters": ["tags", "file_type", "created_at"]
        }
        
        optimization_recommendations = []
        
        collection_size = performance_metrics.get("collection_size", 0)
        if collection_size > 100000:
            optimization_recommendations.append({
                "type": "index_optimization",
                "priority": "high", 
                "recommendation": "GPU ì¸ë±ìŠ¤ ë° ë°°ì¹˜ ê²€ìƒ‰ í™œìš©",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 3-5ë°° í–¥ìƒ"
            })
        
        if len(search_patterns["frequent_queries"]) > 5:
            optimization_recommendations.append({
                "type": "caching_strategy",
                "priority": "medium",
                "recommendation": "ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±",
                "expected_improvement": "ì‘ë‹µ ì‹œê°„ 50% ë‹¨ì¶•"
            })
        
        if config.USE_GPU:
            optimization_recommendations.append({
                "type": "gpu_optimization",
                "priority": "high",
                "recommendation": "GPU ë©”ëª¨ë¦¬ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬ í™œìš©",
                "expected_improvement": "ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ"
            })
        
        analysis_time = time.time() - start_time
        
        return {
            "performance_metrics": performance_metrics,
            "benchmark_results": benchmark_results,
            "search_patterns": search_patterns,
            "optimization_recommendations": optimization_recommendations,
            "milvus_capabilities_usage": {
                "hnsw_indexing": "active",
                "metadata_filtering": "active", 
                "gpu_acceleration": "active" if config.USE_GPU else "inactive",
                "batch_processing": "available",
                "custom_metrics": "available",
                "advanced_search_patterns": "active"
            },
            "analysis_time_ms": round(analysis_time * 1000, 2)
        }
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

# ==================== ê¸°ì¡´ ê¸°ë³¸ ë„êµ¬ë“¤ ====================

@mcp.tool()
async def search_documents(
    query: str,
    limit: int = 5,
    search_type: str = "hybrid",
    file_types: Optional[List[str]] = None,
    tags: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Obsidian ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    global search_engine
    
    if not search_engine:
        return {"error": "ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "query": query, "results": []}
    
    try:
        filter_params = {}
        if file_types:
            filter_params['file_types'] = file_types
        if tags:
            filter_params['tags'] = tags
        
        if search_type == "hybrid" or search_type == "vector":
            results, search_info = search_engine.hybrid_search(
                query=query, limit=limit, filter_params=filter_params if filter_params else None
            )
        else:
            results = search_engine._keyword_search(
                query=query, limit=limit, filter_expr=filter_params.get('filter_expr') if filter_params else None
            )
            search_info = {"query": query, "search_type": "keyword_only", "total_count": len(results)}
        
        formatted_results = []
        for result in results:
            formatted_result = {
                "id": result.get("id", ""),
                "file_path": result.get("path", ""),
                "title": result.get("title", "ì œëª© ì—†ìŒ"),
                "content_preview": result.get("chunk_text", "")[:300] + "..." if len(result.get("chunk_text", "")) > 300 else result.get("chunk_text", ""),
                "full_content": result.get("content", ""),
                "score": float(result.get("score", 0)),
                "file_type": result.get("file_type", ""),
                "tags": result.get("tags", []),
                "chunk_index": result.get("chunk_index", 0),
                "created_at": result.get("created_at", ""),
                "updated_at": result.get("updated_at", ""),
                "source": result.get("source", "unknown")
            }
            formatted_results.append(formatted_result)
        
        return {
            "query": query,
            "search_type": search_type,
            "total_results": len(formatted_results),
            "results": formatted_results,
            "search_info": search_info,
            "filters_applied": {"file_types": file_types, "tags": tags}
        }
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "query": query, "results": []}

@mcp.tool()
async def get_document_content(file_path: str) -> Dict[str, Any]:
    """íŠ¹ì • ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "file_path": file_path}
    
    try:
        results = milvus_manager.query(
            expr=f'path == "{file_path}"',
            output_fields=["id", "path", "title", "content", "chunk_text", "file_type", "tags", "created_at", "updated_at", "chunk_index"],
            limit=100
        )
        
        if not results:
            return {"error": f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}", "file_path": file_path}
        
        first_result = results[0]
        all_chunks = []
        for result in results:
            chunk_info = {
                "chunk_index": result.get("chunk_index", 0),
                "chunk_text": result.get("chunk_text", ""),
                "id": result.get("id", "")
            }
            all_chunks.append(chunk_info)
        
        all_chunks.sort(key=lambda x: x.get("chunk_index", 0))
        full_content = first_result.get("content", "")
        if not full_content:
            full_content = "\n\n".join([chunk["chunk_text"] for chunk in all_chunks])
        
        return {
            "file_path": file_path,
            "title": first_result.get("title", "ì œëª© ì—†ìŒ"),
            "full_content": full_content,
            "file_type": first_result.get("file_type", ""),
            "tags": first_result.get("tags", []),
            "created_at": first_result.get("created_at", ""),
            "updated_at": first_result.get("updated_at", ""),
            "total_chunks": len(all_chunks),
            "chunks": all_chunks,
            "word_count": len(full_content.split()) if full_content else 0,
            "character_count": len(full_content) if full_content else 0
        }
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "file_path": file_path}

@mcp.tool()
async def get_collection_stats() -> Dict[str, Any]:
    """Milvus ì»¬ë ‰ì…˜ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "collection_name": config.COLLECTION_NAME}
    
    try:
        total_entities = milvus_manager.count_entities()
        file_type_counts = milvus_manager.get_file_type_counts()
        recent_docs = milvus_manager.query(
            expr="id >= 0", output_fields=["path", "title", "created_at", "file_type"], limit=10
        )
        
        all_results = milvus_manager.query(expr="id >= 0", output_fields=["tags"], limit=1000)
        
        tag_counts = {}
        for doc in all_results:
            tags = doc.get("tags", [])
            if isinstance(tags, list):
                for tag in tags:
                    if tag:
                        tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        top_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "collection_name": config.COLLECTION_NAME,
            "total_documents": total_entities,
            "file_type_distribution": file_type_counts,
            "top_tags": [{"tag": tag, "count": count} for tag, count in top_tags],
            "recent_documents": [
                {
                    "path": doc.get("path", ""),
                    "title": doc.get("title", ""),
                    "file_type": doc.get("file_type", ""),
                    "created_at": doc.get("created_at", "")
                }
                for doc in recent_docs[:5]
            ],
            "milvus_config": {
                "host": config.MILVUS_HOST,
                "port": config.MILVUS_PORT,
                "collection": config.COLLECTION_NAME
            },
            "embedding_config": {
                "model": config.EMBEDDING_MODEL,
                "dimension": config.VECTOR_DIM
            },
            "optimization_status": {
                "gpu_enabled": config.USE_GPU,
                "hnsw_optimizer": "active" if hnsw_optimizer else "inactive",
                "enhanced_search": "active" if enhanced_search else "inactive",
                "advanced_rag": "active" if rag_engine else "inactive"
            }
        }
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "collection_name": config.COLLECTION_NAME}

@mcp.tool()
async def search_by_tags(tags: List[str], limit: int = 10) -> Dict[str, Any]:
    """íŠ¹ì • íƒœê·¸ë¥¼ ê°€ì§„ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "tags": tags}
    
    if not tags:
        return {"error": "ìµœì†Œ í•˜ë‚˜ì˜ íƒœê·¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.", "tags": tags, "results": []}
    
    try:
        all_results = milvus_manager.query(
            expr="id >= 0",
            output_fields=["id", "path", "title", "tags", "file_type", "created_at", "updated_at"],
            limit=1000
        )
        
        filtered_results = []
        for doc in all_results:
            doc_tags = doc.get("tags", [])
            if isinstance(doc_tags, list):
                if any(tag in doc_tags for tag in tags):
                    filtered_results.append({
                        "id": doc.get("id", ""),
                        "file_path": doc.get("path", ""),
                        "title": doc.get("title", "ì œëª© ì—†ìŒ"),
                        "tags": doc_tags,
                        "file_type": doc.get("file_type", ""),
                        "created_at": doc.get("created_at", ""),
                        "updated_at": doc.get("updated_at", ""),
                        "matched_tags": [tag for tag in tags if tag in doc_tags]
                    })
        
        filtered_results = filtered_results[:limit]
        
        return {
            "search_tags": tags,
            "total_results": len(filtered_results),
            "results": filtered_results
        }
        
    except Exception as e:
        logger.error(f"íƒœê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"íƒœê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "search_tags": tags, "results": []}

@mcp.tool()
async def list_available_tags(limit: int = 50) -> Dict[str, Any]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  íƒœê·¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "tags": {}}
    
    try:
        results = milvus_manager.query(expr="id >= 0", output_fields=["tags"], limit=2000)
        
        tag_counts = {}
        total_docs_with_tags = 0
        
        for doc in results:
            tags = doc.get("tags", [])
            if isinstance(tags, list) and tags:
                total_docs_with_tags += 1
                for tag in tags:
                    if tag and tag.strip():
                        clean_tag = tag.strip()
                        tag_counts[clean_tag] = tag_counts.get(clean_tag, 0) + 1
        
        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:limit]
        
        return {
            "total_unique_tags": len(tag_counts),
            "total_documents_with_tags": total_docs_with_tags,
            "top_tags": [{"tag": tag, "document_count": count} for tag, count in sorted_tags],
            "tags_summary": dict(sorted_tags)
        }
        
    except Exception as e:
        logger.error(f"íƒœê·¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"íƒœê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "tags": {}}

@mcp.tool()
async def get_similar_documents(file_path: str, limit: int = 5) -> Dict[str, Any]:
    """ì§€ì •ëœ ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
    global milvus_manager, enhanced_search
    
    if not milvus_manager:
        return {"error": "í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "file_path": file_path}
    
    try:
        base_docs = milvus_manager.query(
            expr=f'path == "{file_path}"',
            output_fields=["id", "path", "title", "content", "chunk_text"],
            limit=1
        )
        
        if not base_docs:
            return {"error": f"ê¸°ì¤€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}", "file_path": file_path}
        
        base_doc = base_docs[0]
        search_query = f"{base_doc.get('title', '')} {base_doc.get('chunk_text', '')[:200]}"
        
        if enhanced_search:
            results, search_info = enhanced_search.hybrid_search(query=search_query, limit=limit + 5)
        else:
            results, search_info = search_engine.hybrid_search(query=search_query, limit=limit + 5)
        
        similar_docs = []
        for result in results:
            if result.get("path") != file_path and len(similar_docs) < limit:
                similar_docs.append({
                    "file_path": result.get("path", ""),
                    "title": result.get("title", "ì œëª© ì—†ìŒ"),
                    "similarity_score": float(result.get("score", 0)),
                    "content_preview": result.get("chunk_text", "")[:200] + "..." if len(result.get("chunk_text", "")) > 200 else result.get("chunk_text", ""),
                    "file_type": result.get("file_type", ""),
                    "tags": result.get("tags", [])
                })
        
        return {
            "base_document": {"file_path": file_path, "title": base_doc.get("title", "ì œëª© ì—†ìŒ")},
            "similar_documents": similar_docs,
            "total_found": len(similar_docs)
        }
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "file_path": file_path}

# ==================== í—¬í¼ í•¨ìˆ˜ë“¤ ====================

def _analyze_knowledge_clusters(knowledge_graph):
    """ì§€ì‹ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
    clusters = {}
    node_to_cluster = {}
    cluster_id = 0
    
    for node in knowledge_graph["nodes"]:
        if node["id"] not in node_to_cluster:
            current_cluster = []
            stack = [node["id"]]
            
            while stack:
                current_node = stack.pop()
                if current_node not in node_to_cluster:
                    node_to_cluster[current_node] = cluster_id
                    current_cluster.append(current_node)
                    
                    for edge in knowledge_graph["edges"]:
                        if edge["source"] == current_node and edge["target"] not in node_to_cluster:
                            stack.append(edge["target"])
                        elif edge["target"] == current_node and edge["source"] not in node_to_cluster:
                            stack.append(edge["source"])
            
            clusters[f"cluster_{cluster_id}"] = {
                "nodes": current_cluster,
                "size": len(current_cluster),
                "topic": f"Topic_{cluster_id}"
            }
            cluster_id += 1
    
    return clusters

# ==================== ë¦¬ì†ŒìŠ¤ë“¤ ====================

@mcp.resource("config://milvus")
async def get_milvus_config() -> str:
    """Milvus ì—°ê²° ì„¤ì • ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config_info = {
        "milvus_settings": {
            "host": config.MILVUS_HOST,
            "port": config.MILVUS_PORT,
            "collection_name": config.COLLECTION_NAME
        },
        "embedding_settings": {
            "model": config.EMBEDDING_MODEL,
            "vector_dimension": config.VECTOR_DIM,
            "chunk_size": config.CHUNK_SIZE,
            "chunk_overlap": config.CHUNK_OVERLAP
        },
        "obsidian_settings": {
            "vault_path": config.OBSIDIAN_VAULT_PATH
        },
        "gpu_settings": {
            "use_gpu": config.USE_GPU,
            "gpu_index_type": getattr(config, 'GPU_INDEX_TYPE', 'GPU_IVF_FLAT')
        },
        "optimization_features": {
            "enhanced_search": True,
            "hnsw_optimization": True,
            "advanced_rag": True,
            "knowledge_graph": True,
            "multi_query_fusion": True,
            "performance_monitoring": True
        }
    }
    return json.dumps(config_info, indent=2, ensure_ascii=False)

@mcp.resource("stats://collection")
async def get_collection_stats_resource() -> str:
    """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´ë¥¼ ë¦¬ì†ŒìŠ¤ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return json.dumps({"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}, ensure_ascii=False)
    
    try:
        total_entities = milvus_manager.count_entities()
        file_type_counts = milvus_manager.get_file_type_counts()
        
        stats = {
            "collection_name": config.COLLECTION_NAME,
            "total_documents": total_entities,
            "file_types": file_type_counts,
            "last_updated": datetime.now().isoformat(),
            "optimization_status": {
                "gpu_enabled": config.USE_GPU,
                "advanced_features": "active"
            }
        }
        
        return json.dumps(stats, indent=2, ensure_ascii=False)
    except Exception as e:
        error_info = {
            "error": f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "collection_name": config.COLLECTION_NAME
        }
        return json.dumps(error_info, ensure_ascii=False)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì í™”ëœ Obsidian-Milvus Fast MCP Server ì‹œì‘ ì¤‘...")
    print("ğŸ’ Milvus ê³ ê¸‰ ê¸°ëŠ¥ ëª¨ë‘ í™œì„±í™”!")
    
    if not initialize_components():
        print("âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    print("ğŸ¯ í™œì„±í™”ëœ ê³ ê¸‰ ê¸°ëŠ¥ë“¤:")
    print("   - ğŸ” ì§€ëŠ¥í˜• ê²€ìƒ‰ (ì ì‘ì /ê³„ì¸µì /ì˜ë¯¸ì  ê·¸ë˜í”„)")
    print("   - ğŸ·ï¸ ê³ ê¸‰ ë©”íƒ€ë°ì´í„° í•„í„°ë§")
    print("   - ğŸ”„ ë‹¤ì¤‘ ì¿¼ë¦¬ ìœµí•©")
    print("   - ğŸ•¸ï¸ ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰")
    print("   - âš¡ HNSW ìµœì í™”")
    print("   - ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print(f"ğŸ“¡ MCP ì„œë²„ '{config.FASTMCP_SERVER_NAME}' ì‹œì‘ ì¤‘...")
    print(f"ğŸ”§ Transport: {config.FASTMCP_TRANSPORT}")
    
    try:
        if config.FASTMCP_TRANSPORT == "stdio":
            print("ğŸ“¡ STDIO transportë¡œ MCP ì„œë²„ ì‹œì‘...")
            mcp.run(transport="stdio")
        elif config.FASTMCP_TRANSPORT == "sse":
            print(f"ğŸ“¡ SSE transportë¡œ MCP ì„œë²„ ì‹œì‘... (http://{config.FASTMCP_HOST}:{config.FASTMCP_PORT})")
            mcp.run(transport="sse", host=config.FASTMCP_HOST, port=config.FASTMCP_PORT)
        elif config.FASTMCP_TRANSPORT == "streamable-http":
            print(f"ğŸ“¡ Streamable HTTP transportë¡œ MCP ì„œë²„ ì‹œì‘... (http://{config.FASTMCP_HOST}:{config.FASTMCP_PORT})")
            mcp.run(transport="streamable-http", host=config.FASTMCP_HOST, port=config.FASTMCP_PORT)
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” transport: {config.FASTMCP_TRANSPORT}")
            print("ì§€ì›í•˜ëŠ” transport: stdio, sse, streamable-http")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        sys.exit(1)
    finally:
        if milvus_manager:
            try:
                milvus_manager.stop_monitoring()
                print("âœ… Milvus ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
            except:
                pass
        print("ğŸ‘‹ ìµœì í™”ëœ ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
