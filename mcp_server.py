"""Obsidian-Milvus Fast MCP Server - ì™„ì „ ìµœì í™” ë²„ì „ (Enhanced)
ì „ì²´ ë…¸íŠ¸ ê²€ìƒ‰ ë° ê³ ê¸‰ ê²€ìƒ‰ ëª¨ë“œë¥¼ ì§€ì›í•˜ëŠ” ì—…ê·¸ë ˆì´ë“œ ë²„ì „

New Features:
- ì „ì²´ ê²€ìƒ‰ ëª¨ë“œ (limit=None ì§€ì›)
- ìë™ ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
- ë°°ì¹˜ ê²€ìƒ‰ ë° í˜ì´ì§€ë„¤ì´ì…˜
- ê¸°ë³¸ limit 200-500ìœ¼ë¡œ ì¦ê°€
- ì¢…í•© ê²€ìƒ‰ ê¸°ëŠ¥

Enhanced with:
- ê³ ê¸‰ ë©”íƒ€ë°ì´í„° í•„í„°ë§  
- HNSW ì¸ë±ìŠ¤ ìµœì í™”
- ê³„ì¸µì /ì˜ë¯¸ì  ê·¸ë˜í”„ ê²€ìƒ‰
- ë‹¤ì¤‘ ì¿¼ë¦¬ ìœµí•©
- ì ì‘ì  ì²­í¬ ê²€ìƒ‰
- ì‹œê°„ ì¸ì‹ ê²€ìƒ‰
- ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
"""

# Import warning suppressor first
import warning_suppressor

import os
import sys
import json
import traceback
import logging
import time
import asyncio
import math
from typing import List, Dict, Any, Optional, Tuple, Generator
from datetime import datetime

# Configure basic logging
logging.basicConfig(
    level=logging.ERROR,  # Use ERROR level by default
    format='%(message)s'
)

# Import other modules
from mcp.server.fastmcp import FastMCP
import config
from milvus_manager import MilvusManager
from search_engine import SearchEngine

# ìƒˆë¡œìš´ ê³ ê¸‰ ëª¨ë“ˆë“¤
from enhanced_search_engine import EnhancedSearchEngine
from hnsw_optimizer import HNSWOptimizer
from advanced_rag import AdvancedRAGEngine

# Set up logging level from config
log_level_str = getattr(config, 'LOG_LEVEL', 'ERROR')
log_level = getattr(logging, log_level_str, logging.ERROR)
logging.getLogger().setLevel(log_level)

# Get logger for this module
logger = logging.getLogger('OptimizedMCP')
logger.setLevel(log_level)

# Helper function to safely print messages
def safe_print(message, level="info"):
    """Print a message safely using the logger"""
    if level.lower() == "error":
        logger.error(message)
    elif level.lower() == "warning":
        logger.warning(message)
    else:
        logger.info(message)

mcp = FastMCP(config.FASTMCP_SERVER_NAME)

# ì „ì—­ ë³€ìˆ˜ë“¤
milvus_manager = None
enhanced_search = None
search_engine = None
hnsw_optimizer = None
rag_engine = None

def initialize_components():
    """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
    global milvus_manager, search_engine, enhanced_search, hnsw_optimizer, rag_engine
    
    try:
        logger.info("Starting Enhanced Obsidian-Milvus Fast MCP Server...")
        
        milvus_manager = MilvusManager()
        search_engine = SearchEngine(milvus_manager)
        enhanced_search = EnhancedSearchEngine(milvus_manager)
        hnsw_optimizer = HNSWOptimizer(milvus_manager)
        rag_engine = AdvancedRAGEngine(milvus_manager, enhanced_search)
        
        try:
            # Skip auto-tuning to prevent hanging
            logger.info("Skipping auto-tuning to prevent system hang")
            # optimization_params = hnsw_optimizer.auto_tune_parameters()
            # logger.info(f"Auto-tuning completed: {optimization_params}")
        except Exception as e:
            logger.warning(f"Auto-tuning warning: {e}")
        
        logger.info("All components initialized!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Component initialization failed: {e}")
        return False

# ==================== ìƒˆë¡œìš´ ê³ ê¸‰ ê²€ìƒ‰ ë„êµ¬ë“¤ ====================

def analyze_query_complexity(query: str) -> Dict[str, Any]:
    """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„í•˜ì—¬ ìµœì  ê²€ìƒ‰ ëª¨ë“œ ê²°ì •"""
    words = query.split()
    word_count = len(words)
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ë³µì¡ë„ ë¶„ì„
    complex_keywords = ['ë¶„ì„', 'analyze', 'ë¹„êµ', 'compare', 'ê´€ê³„', 'relation', 'ì—°ê²°', 'connection']
    semantic_keywords = ['ì˜ë¯¸', 'meaning', 'ê°œë…', 'concept', 'ì´í•´', 'understand']
    specific_keywords = ['ì •í™•íˆ', 'exact', 'íŠ¹ì •', 'specific', 'ì°¾ì•„ì¤˜', 'find']
    
    complexity_score = 0
    
    # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ì ìˆ˜
    if word_count <= 2:
        complexity_score += 1  # ë‹¨ìˆœ
    elif word_count <= 5:
        complexity_score += 2  # ë³´í†µ
    else:
        complexity_score += 3  # ë³µì¡
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
    query_lower = query.lower()
    if any(keyword in query_lower for keyword in complex_keywords):
        complexity_score += 2
    if any(keyword in query_lower for keyword in semantic_keywords):
        complexity_score += 1
    if any(keyword in query_lower for keyword in specific_keywords):
        complexity_score += 1
    
    # ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
    if complexity_score <= 2:
        search_mode = "fast"
        search_strategy = "keyword"
    elif complexity_score <= 4:
        search_mode = "balanced"
        search_strategy = "hybrid"
    else:
        search_mode = "comprehensive"
        search_strategy = "semantic_graph"
    
    return {
        "complexity_score": complexity_score,
        "word_count": word_count,
        "recommended_mode": search_mode,
        "recommended_strategy": search_strategy,
        "estimated_time": "fast" if complexity_score <= 2 else "medium" if complexity_score <= 4 else "slow"
    }

@mcp.tool()
async def auto_search_mode_decision(
    query: str,
    execute_search: bool = True,
    limit: Optional[int] = None
) -> Dict[str, Any]:
    """ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ëª¨ë“œë¥¼ ìë™ìœ¼ë¡œ ê²°ì •í•˜ê³  ì‹¤í–‰"""
    global search_engine, enhanced_search, rag_engine
    
    if not search_engine:
        return {"error": "Search engine not initialized.", "query": query}
    
    try:
        start_time = time.time()
        
        # ì¿¼ë¦¬ ë¶„ì„
        analysis = analyze_query_complexity(query)
        recommended_mode = analysis["recommended_mode"]
        recommended_strategy = analysis["recommended_strategy"]
        
        # limit ìë™ ê²°ì •
        if limit is None:
            if recommended_mode == "fast":
                limit = 100
            elif recommended_mode == "balanced":
                limit = 300
            else:  # comprehensive
                limit = 500
        
        results = []
        search_info = {}
        
        if execute_search:
            # ì¶”ì²œëœ ëª¨ë“œë¡œ ê²€ìƒ‰ ì‹¤í–‰
            if recommended_strategy == "keyword":
                results = search_engine._keyword_search(query=query, limit=limit)
                search_info = {"type": "keyword", "mode": "fast"}
                
            elif recommended_strategy == "hybrid":
                results, search_info = search_engine.hybrid_search(
                    query=query, limit=limit
                )
                
            elif recommended_strategy == "semantic_graph" and rag_engine:
                try:
                    results = rag_engine.semantic_graph_retrieval(query, max_hops=2)
                    if isinstance(results, dict) and "primary_chunks" in results:
                        results = results["primary_chunks"][:limit]
                    # Ensure all data is JSON serializable
                    results = [{k: (str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v) 
                              for k, v in item.items()} for item in results]
                    search_info = {"type": "semantic_graph", "mode": "comprehensive"}
                except Exception as e:
                    logger.error(f"Semantic graph retrieval error: {e}")
                    # Fallback to hybrid search if semantic graph fails
                    results, search_info = search_engine.hybrid_search(query=query, limit=limit)
                
            else:
                # í´ë°±: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                results, search_info = search_engine.hybrid_search(
                    query=query, limit=limit
                )
        
        analysis_time = time.time() - start_time
        
        # Ensure all data is JSON serializable
        def ensure_json_serializable(obj):
            if isinstance(obj, dict):
                return {k: ensure_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [ensure_json_serializable(item) for item in obj]
            elif isinstance(obj, (str, int, float, bool, type(None))):
                return obj
            else:
                return str(obj)
        
        # Create response with serializable data
        response = {
            "query": query,
            "query_analysis": ensure_json_serializable(analysis),
            "selected_mode": recommended_mode,
            "selected_strategy": recommended_strategy,
            "limit_used": limit,
            "results": ensure_json_serializable(results) if execute_search else [],
            "search_info": ensure_json_serializable(search_info) if execute_search else {},
            "performance": {
                "analysis_time_ms": round(analysis_time * 1000, 2),
                "total_results": len(results) if execute_search else 0,
                "mode_effectiveness": "optimal" if results else "needs_adjustment"
            }
        }
        
        return response
        
    except Exception as e:
        logger.error(f"Auto search mode decision error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def comprehensive_search_all(
    query: str,
    include_similarity_scores: bool = True,
    batch_size: int = 500,
    similarity_threshold: float = 0.3
) -> Dict[str, Any]:
    """ì „ì²´ ì»¬ë ‰ì…˜ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì¢…í•© ê²€ìƒ‰ (limit ì œí•œ ì—†ìŒ)"""
    global milvus_manager, search_engine
    
    if not milvus_manager or not search_engine:
        return {"error": "Required components not initialized.", "query": query}
    
    try:
        start_time = time.time()
        
        # ì „ì²´ ì»¬ë ‰ì…˜ í¬ê¸° í™•ì¸
        total_entities = milvus_manager.count_entities()
        print(f"ğŸ” Comprehensive search across {total_entities} documents...")
        
        all_results = []
        processed_batches = 0
        
        # ë°°ì¹˜ë³„ë¡œ ì „ì²´ ì»¬ë ‰ì…˜ ê²€ìƒ‰
        for offset in range(0, total_entities, batch_size):
            try:
                batch_results = milvus_manager.query(
                    expr="id >= 0",
                    output_fields=["id", "path", "title", "chunk_text", "content", "file_type", "tags", "created_at", "updated_at"],
                    limit=batch_size,
                    offset=offset
                )
                
                # ê° ë¬¸ì„œì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
                if include_similarity_scores:
                    query_embedding = search_engine.embedding_model.get_embedding(query)
                    
                    for doc in batch_results:
                        doc_text = f"{doc.get('title', '')} {doc.get('chunk_text', '')}"
                        if doc_text.strip():
                            doc_embedding = search_engine.embedding_model.get_embedding(doc_text)
                            similarity = search_engine._calculate_cosine_similarity(query_embedding, doc_embedding)
                            
                            if similarity >= similarity_threshold:
                                doc['similarity_score'] = float(similarity)
                                doc['search_relevance'] = 'high' if similarity > 0.7 else 'medium' if similarity > 0.5 else 'low'
                                all_results.append(doc)
                else:
                    # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
                    query_words = set(query.lower().split())
                    for doc in batch_results:
                        doc_text = f"{doc.get('title', '')} {doc.get('chunk_text', '')}".lower()
                        if any(word in doc_text for word in query_words):
                            doc['similarity_score'] = 0.5  # ê¸°ë³¸ê°’
                            doc['search_relevance'] = 'keyword_match'
                            all_results.append(doc)
                
                processed_batches += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if processed_batches % 5 == 0:
                    print(f"ğŸ“Š Processed {processed_batches * batch_size}/{total_entities} documents...")
                
            except Exception as batch_error:
                logger.error(f"Batch processing error at offset {offset}: {batch_error}")
                continue
        
        # ê²°ê³¼ ì •ë ¬ (ìœ ì‚¬ë„ ìˆœ)
        if include_similarity_scores:
            all_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        search_time = time.time() - start_time
        
        # Ensure all data is JSON serializable
        def ensure_json_serializable(obj):
            if isinstance(obj, dict):
                return {k: ensure_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [ensure_json_serializable(item) for item in obj]
            elif isinstance(obj, (str, int, float, bool, type(None))):
                return obj
            else:
                return str(obj)
        
        # Process results to ensure they're serializable
        processed_results = []
        for result in all_results:
            processed_result = {}
            for k, v in result.items():
                if not isinstance(v, (str, int, float, bool, type(None))):
                    processed_result[k] = str(v)
                else:
                    processed_result[k] = v
            processed_results.append(processed_result)
        
        return {
            "query": query,
            "search_type": "comprehensive_all",
            "total_documents_searched": total_entities,
            "total_results_found": len(processed_results),
            "results": processed_results,
            "search_parameters": {
                "batch_size": batch_size,
                "similarity_threshold": similarity_threshold,
                "include_similarity_scores": include_similarity_scores
            },
            "performance_metrics": {
                "search_time_seconds": round(search_time, 2),
                "documents_per_second": round(total_entities / search_time, 2) if search_time > 0 else 0,
                "batches_processed": processed_batches,
                "effectiveness_ratio": len(processed_results) / total_entities if total_entities > 0 else 0
            }
        }
        
    except Exception as e:
        logger.error(f"Comprehensive search error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def batch_search_with_pagination(
    query: str,
    page_size: int = 200,
    max_pages: Optional[int] = None,
    search_mode: str = "hybrid"
) -> Dict[str, Any]:
    """í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰"""
    global search_engine, milvus_manager
    
    if not search_engine or not milvus_manager:
        return {"error": "Required components not initialized.", "query": query}
    
    try:
        start_time = time.time()
        
        total_entities = milvus_manager.count_entities()
        max_possible_pages = math.ceil(total_entities / page_size)
        
        if max_pages is None:
            max_pages = min(max_possible_pages, 10)  # ê¸°ë³¸ì ìœ¼ë¡œ ìµœëŒ€ 10í˜ì´ì§€
        else:
            max_pages = min(max_pages, max_possible_pages)
        
        all_results = []
        page_results = []
        
        query_embedding = search_engine.embedding_model.get_embedding(query)
        
        for page in range(max_pages):
            offset = page * page_size
            
            try:
                # í˜ì´ì§€ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                page_docs = milvus_manager.query(
                    expr="id >= 0",
                    output_fields=["id", "path", "title", "chunk_text", "content", "file_type", "tags"],
                    limit=page_size,
                    offset=offset
                )
                
                page_matches = []
                
                if search_mode == "hybrid":
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)
                    query_words = set(query.lower().split())
                    
                    for doc in page_docs:
                        # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                        doc_text = f"{doc.get('title', '')} {doc.get('chunk_text', '')}".lower()
                        keyword_score = sum(1 for word in query_words if word in doc_text) / len(query_words)
                        
                        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                        if keyword_score > 0 or search_mode == "semantic":
                            doc_full_text = f"{doc.get('title', '')} {doc.get('chunk_text', '')}"
                            if doc_full_text.strip():
                                doc_embedding = search_engine.embedding_model.get_embedding(doc_full_text)
                                semantic_score = search_engine._calculate_cosine_similarity(query_embedding, doc_embedding)
                                
                                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ 30% + ì˜ë¯¸ì  70%)
                                combined_score = (keyword_score * 0.3) + (semantic_score * 0.7)
                                
                                if combined_score > 0.2:  # ì„ê³„ê°’
                                    doc['similarity_score'] = float(combined_score)
                                    doc['keyword_score'] = float(keyword_score)
                                    doc['semantic_score'] = float(semantic_score)
                                    doc['page_number'] = page + 1
                                    page_matches.append(doc)
                
                elif search_mode == "keyword":
                    # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
                    query_words = set(query.lower().split())
                    for doc in page_docs:
                        doc_text = f"{doc.get('title', '')} {doc.get('chunk_text', '')}".lower()
                        score = sum(1 for word in query_words if word in doc_text) / len(query_words)
                        if score > 0:
                            doc['similarity_score'] = float(score)
                            doc['page_number'] = page + 1
                            page_matches.append(doc)
                
                # í˜ì´ì§€ ê²°ê³¼ ì •ë ¬
                page_matches.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
                
                page_info = {
                    "page_number": page + 1,
                    "documents_in_page": len(page_docs),
                    "matches_found": len(page_matches),
                    "top_matches": page_matches[:10]  # ìƒìœ„ 10ê°œë§Œ ì €ì¥
                }
                page_results.append(page_info)
                
                # ì „ì²´ ê²°ê³¼ì— ì¶”ê°€
                all_results.extend(page_matches)
                
                print(f"ğŸ“„ Page {page + 1}/{max_pages}: {len(page_matches)} matches found")
                
            except Exception as page_error:
                logger.error(f"Page {page + 1} processing error: {page_error}")
                continue
        
        # ì „ì²´ ê²°ê³¼ ì¬ì •ë ¬
        all_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "search_type": "batch_pagination",
            "pagination_info": {
                "page_size": page_size,
                "pages_processed": len(page_results),
                "max_pages_requested": max_pages,
                "total_documents": total_entities
            },
            "all_results": all_results,
            "page_by_page_results": page_results,
            "summary": {
                "total_matches": len(all_results),
                "best_match_score": all_results[0].get('similarity_score', 0) if all_results else 0,
                "search_time_seconds": round(search_time, 2),
                "average_matches_per_page": round(len(all_results) / len(page_results), 2) if page_results else 0
            }
        }
        
    except Exception as e:
        logger.error(f"Batch pagination search error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def intelligent_search_enhanced(
    query: str,
    search_strategy: str = "auto",  # auto, adaptive, hierarchical, semantic_graph, multi_modal
    context_expansion: bool = True,
    time_awareness: bool = False,
    similarity_threshold: float = 0.7,
    limit: Optional[int] = None,  # None means comprehensive search
    enable_full_search: bool = False
) -> Dict[str, Any]:
    """ê³ ë„ë¡œ í–¥ìƒëœ ì§€ëŠ¥í˜• ê²€ìƒ‰ (ì „ì²´ ê²€ìƒ‰ ì§€ì›)"""
    global rag_engine, enhanced_search, milvus_manager
    
    if not rag_engine or not enhanced_search:
        return {"error": "Advanced search engine not initialized.", "query": query}
    
    try:
        start_time = time.time()
        
        # ìë™ ëª¨ë“œì¸ ê²½ìš° ì¿¼ë¦¬ ë¶„ì„ìœ¼ë¡œ ì „ëµ ê²°ì •
        if search_strategy == "auto":
            analysis = analyze_query_complexity(query)
            search_strategy = analysis["recommended_strategy"]
            if search_strategy == "keyword":
                search_strategy = "adaptive"  # í‚¤ì›Œë“œ -> ì ì‘ì  ê²€ìƒ‰
        
        # limit ìë™ ê²°ì •
        if limit is None:
            if enable_full_search:
                # ì „ì²´ ê²€ìƒ‰ ëª¨ë“œ
                return await comprehensive_search_all(
                    query=query,
                    include_similarity_scores=True,
                    similarity_threshold=similarity_threshold
                )
            else:
                # ê¸°ë³¸ limit ì„¤ì •
                limit = 300
        
        # ì „ëµë³„ ê²€ìƒ‰ ìˆ˜í–‰
        results = []
        
        if search_strategy == "adaptive":
            results = rag_engine.adaptive_chunk_retrieval(query, context_size="dynamic")
        elif search_strategy == "hierarchical":
            results = rag_engine.hierarchical_retrieval(query, max_depth=3)
        elif search_strategy == "semantic_graph":
            results = rag_engine.semantic_graph_retrieval(query, max_hops=2)
        elif search_strategy == "multi_modal":
            results = enhanced_search.multi_modal_search(query, include_attachments=True)
        else:
            # ê¸°ë³¸: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰
            results = enhanced_search.semantic_similarity_search(query, similarity_threshold=similarity_threshold)
        
        # ì‹œê°„ ì¸ì‹ ê²€ìƒ‰ ì ìš©
        if time_awareness and isinstance(results, list):
            results = rag_engine.temporal_aware_retrieval(query, time_weight=0.3)
        
        # ê²°ê³¼ ì²˜ë¦¬
        if isinstance(results, dict) and "primary_chunks" in results:
            if results["primary_chunks"]:
                results["primary_chunks"] = results["primary_chunks"][:limit]
        elif isinstance(results, list):
            results = results[:limit]
        
        # ì»¨í…ìŠ¤íŠ¸ í™•ì¥
        expanded_results = None
        if context_expansion:
            try:
                if isinstance(results, list) and results:
                    context_docs = [r.get('id') for r in results[:5] if r.get('id')]
                    if context_docs:
                        expanded_results = enhanced_search.contextual_search(
                            query, context_docs=context_docs, expand_context=True
                        )
            except Exception as e:
                logger.error(f"Context expansion error: {e}")
        
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "strategy_used": search_strategy,
            "primary_results": results,
            "expanded_results": expanded_results,
            "search_configuration": {
                "strategy": search_strategy,
                "time_awareness": time_awareness,
                "similarity_threshold": similarity_threshold,
                "context_expansion": context_expansion,
                "limit": limit,
                "full_search_enabled": enable_full_search
            },
            "performance_metrics": {
                "search_time_ms": round(search_time * 1000, 2),
                "total_found": len(results) if isinstance(results, list) else "complex_structure",
                "strategy_effectiveness": "optimal"
            }
        }
        
    except Exception as e:
        logger.error(f"Enhanced intelligent search error: {e}")
        return {"error": str(e), "query": query}

# ==================== ì—…ê·¸ë ˆì´ë“œëœ ê¸°ì¡´ ë„êµ¬ë“¤ ====================

@mcp.tool()
async def search_documents(
    query: str,
    limit: int = 200,  # ê¸°ë³¸ê°’ 50 -> 200ìœ¼ë¡œ ì¦ê°€
    search_type: str = "hybrid",
    file_types: Optional[List[str]] = None,
    tags: Optional[List[str]] = None,
    enable_comprehensive: bool = False  # ì „ì²´ ê²€ìƒ‰ ëª¨ë“œ
) -> Dict[str, Any]:
    """í–¥ìƒëœ Obsidian ë¬¸ì„œ ê²€ìƒ‰ (ê¸°ë³¸ limit ì¦ê°€, ì „ì²´ ê²€ìƒ‰ ì§€ì›)"""
    global search_engine
    
    if not search_engine:
        return {"error": "Search engine not initialized.", "query": query, "results": []}
    
    try:
        start_time = time.time()
        
        # ì „ì²´ ê²€ìƒ‰ ëª¨ë“œì¸ ê²½ìš°
        if enable_comprehensive:
            return await comprehensive_search_all(
                query=query,
                include_similarity_scores=True,
                similarity_threshold=0.3
            )
        
        # í•„í„° íŒŒë¼ë¯¸í„° êµ¬ì„±
        filter_params = {}
        if file_types:
            filter_params['file_types'] = file_types
        if tags:
            filter_params['tags'] = tags
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        if search_type == "hybrid" or search_type == "vector":
            results, search_info = search_engine.hybrid_search(
                query=query, limit=limit, filter_params=filter_params if filter_params else None
            )
        else:
            results = search_engine._keyword_search(
                query=query, limit=limit, filter_expr=filter_params.get('filter_expr') if filter_params else None
            )
            search_info = {"query": query, "search_type": "keyword_only", "total_count": len(results)}
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in results:
            formatted_result = {
                "id": result.get("id", ""),
                "file_path": result.get("path", ""),
                "title": result.get("title", "ì œëª© ì—†ìŒ"),
                "content_preview": result.get("chunk_text", "")[:300] + "..." if len(result.get("chunk_text", "")) > 300 else result.get("chunk_text", ""),
                "full_content": result.get("content", ""),
                "score": float(result.get("score", 0)),
                "file_type": result.get("file_type", ""),
                "tags": result.get("tags", []),
                "chunk_index": result.get("chunk_index", 0),
                "created_at": result.get("created_at", ""),
                "updated_at": result.get("updated_at", ""),
                "source": result.get("source", "unknown")
            }
            formatted_results.append(formatted_result)
        
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "search_type": search_type,
            "total_results": len(formatted_results),
            "results": formatted_results,
            "search_info": search_info,
            "filters_applied": {"file_types": file_types, "tags": tags},
            "performance": {
                "search_time_ms": round(search_time * 1000, 2),
                "comprehensive_mode": enable_comprehensive,
                "limit_used": limit
            }
        }
        
    except Exception as e:
        logger.error(f"Document search error: {e}")
        return {"error": f"Search error: {str(e)}", "query": query, "results": []}

@mcp.tool()
async def intelligent_search(
    query: str,
    search_strategy: str = "adaptive",
    context_expansion: bool = True,
    time_awareness: bool = False,
    similarity_threshold: float = 0.7,
    limit: int = 200  # ê¸°ë³¸ê°’ 50 -> 200ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """Milvusì˜ ê³ ê¸‰ ê¸°ëŠ¥ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰ (limit ì¦ê°€)"""
    global rag_engine, enhanced_search
    
    if not rag_engine or not enhanced_search:
        return {"error": "Advanced search engine not initialized.", "query": query}
    
    try:
        start_time = time.time()
        
        if search_strategy == "adaptive":
            results = rag_engine.adaptive_chunk_retrieval(query, context_size="dynamic")
        elif search_strategy == "hierarchical":
            results = rag_engine.hierarchical_retrieval(query, max_depth=3)
        elif search_strategy == "semantic_graph":
            results = rag_engine.semantic_graph_retrieval(query, max_hops=2)
        elif search_strategy == "multi_modal":
            results = enhanced_search.multi_modal_search(query, include_attachments=True)
        else:
            results = enhanced_search.semantic_similarity_search(query, similarity_threshold=similarity_threshold)
        
        if time_awareness and isinstance(results, list):
            results = rag_engine.temporal_aware_retrieval(query, time_weight=0.3)
        
        if isinstance(results, dict) and "primary_chunks" in results:
            if results["primary_chunks"]:
                results["primary_chunks"] = results["primary_chunks"][:limit]
        elif isinstance(results, list):
            results = results[:limit]
        
        expanded_results = None
        if context_expansion:
            try:
                if isinstance(results, list) and results:
                    context_docs = [r.get('id') for r in results[:5] if r.get('id')]
                    if context_docs:
                        expanded_results = enhanced_search.contextual_search(
                            query, context_docs=context_docs, expand_context=True
                        )
            except Exception as e:
                logger.error(f"Context expansion error: {e}")
        
        search_time = time.time() - start_time
        
        return {
            "strategy": search_strategy,
            "primary_results": results,
            "expanded_results": expanded_results,
            "metadata": {
                "search_strategy": search_strategy,
                "time_awareness": time_awareness,
                "similarity_threshold": similarity_threshold,
                "context_expansion": context_expansion,
                "search_time_ms": round(search_time * 1000, 2),
                "total_found": len(results) if isinstance(results, list) else "N/A",
                "enhanced_limit": limit
            }
        }
        
    except Exception as e:
        logger.error(f"Intelligent search error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def advanced_filter_search(
    query: str,
    time_range: Optional[List[float]] = None,
    tag_logic: Optional[Dict[str, List[str]]] = None,
    file_size_range: Optional[List[int]] = None,
    min_content_quality: Optional[float] = None,
    min_relevance_score: Optional[float] = None,
    limit: int = 300  # ê¸°ë³¸ê°’ 50 -> 300ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """Milvusì˜ ê°•ë ¥í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í™œìš©í•œ ê³ ê¸‰ ê²€ìƒ‰ (limit ì¦ê°€)"""
    global enhanced_search
    
    if not enhanced_search:
        return {"error": "Advanced search engine not initialized.", "query": query}
    
    try:
        filters = {k: v for k, v in {
            'time_range': time_range,
            'tag_logic': tag_logic,
            'file_size_range': file_size_range,
            'min_content_quality': min_content_quality,
            'min_relevance_score': min_relevance_score,
            'limit': limit
        }.items() if v is not None}
        
        start_time = time.time()
        results = enhanced_search.advanced_filter_search(query, **filters)
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "applied_filters": filters,
            "results": results,
            "filter_effectiveness": len(results) / limit if results else 0,
            "search_time_ms": round(search_time * 1000, 2),
            "enhanced_limit": limit
        }
        
    except Exception as e:
        logger.error(f"Advanced filter search error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool() 
async def multi_query_fusion_search(
    queries: List[str],
    fusion_method: str = "weighted",
    individual_limits: int = 250,  # ê¸°ë³¸ê°’ 50 -> 250ìœ¼ë¡œ ì¦ê°€
    final_limit: int = 500  # ê¸°ë³¸ê°’ 100 -> 500ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìœµí•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ ì œê³µ (limit ì¦ê°€)"""
    global rag_engine
    
    if not rag_engine:
        return {"error": "Advanced RAG engine not initialized.", "queries": queries}
    
    try:
        if not queries:
            return {"error": "At least one query is required."}
        
        start_time = time.time()
        fused_results = rag_engine.multi_query_fusion(queries, fusion_method)
        final_results = fused_results[:final_limit]
        search_time = time.time() - start_time
        
        return {
            "input_queries": queries,
            "fusion_method": fusion_method,
            "total_candidates": len(fused_results),
            "final_results": final_results,
            "enhanced_limits": {
                "individual_limits": individual_limits,
                "final_limit": final_limit
            },
            "fusion_statistics": {
                "average_query_coverage": sum(r.get('query_coverage', 0) for r in final_results) / len(final_results) if final_results else 0,
                "score_distribution": [r.get('fused_score', 0) for r in final_results],
                "search_time_ms": round(search_time * 1000, 2)
            }
        }
        
    except Exception as e:
        logger.error(f"Multi-query fusion search error: {e}")
        return {"error": str(e), "queries": queries}

@mcp.tool()
async def knowledge_graph_exploration(
    starting_document: str,
    exploration_depth: int = 2,
    similarity_threshold: float = 0.75,
    max_connections: int = 200  # ê¸°ë³¸ê°’ 50 -> 200ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """Milvus ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰ (ì—°ê²° ìˆ˜ ì¦ê°€)"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized.", "starting_document": starting_document}
    
    try:
        start_time = time.time()
        
        start_docs = milvus_manager.query(
            expr=f'path == "{starting_document}"',
            output_fields=["id", "path", "title"],
            limit=1
        )
        
        if not start_docs:
            return {"error": f"Starting document not found: {starting_document}"}
        
        start_doc = start_docs[0]
        
        knowledge_graph = {
            "nodes": [{"id": start_doc["id"], "title": start_doc["title"], "path": start_doc["path"], "level": 0}],
            "edges": [],
            "clusters": {}
        }
        
        current_level_nodes = [start_doc.get("id", 0)]
        explored_nodes = {start_doc.get("id", 0)}
        
        for depth in range(1, exploration_depth + 1):
            next_level_nodes = []
            
            for node_id in current_level_nodes:
                try:
                    # ë” ë§ì€ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ íƒìƒ‰
                    similar_docs = milvus_manager.query(
                        expr="id >= 0",
                        output_fields=["id", "path", "title"],
                        limit=300  # íƒìƒ‰ ë²”ìœ„ ì¦ê°€
                    )
                    
                    connection_count = 0
                    for doc in similar_docs:
                        doc_id = doc["id"]
                        if (doc_id not in explored_nodes and connection_count < max_connections // exploration_depth):
                            
                            knowledge_graph["nodes"].append({
                                "id": doc_id if doc_id is not None else 0,
                                "title": doc.get("title", "") or "",
                                "path": doc.get("path", "") or "",
                                "level": depth,
                                "similarity_to_parent": 0.8
                            })
                            
                            knowledge_graph["edges"].append({
                                "source": node_id if node_id is not None else 0,
                                "target": doc_id if doc_id is not None else 0,
                                "weight": 0.8,
                                "type": "semantic_similarity"
                            })
                            
                            next_level_nodes.append(doc_id)
                            explored_nodes.add(doc_id)
                            connection_count += 1
                            
                except Exception as e:
                    logger.error(f"Node {node_id} exploration error: {e}")
                    continue
            
            current_level_nodes = next_level_nodes
            if not current_level_nodes:
                break
        
        clusters = _analyze_knowledge_clusters(knowledge_graph)
        knowledge_graph["clusters"] = clusters
        
        search_time = time.time() - start_time
        
        return {
            "starting_document": starting_document,
            "exploration_depth": exploration_depth,
            "knowledge_graph": knowledge_graph,
            "statistics": {
                "total_nodes": len(knowledge_graph["nodes"]),
                "total_edges": len(knowledge_graph["edges"]),
                "cluster_count": len(clusters),
                "average_similarity": sum(edge["weight"] for edge in knowledge_graph["edges"]) / len(knowledge_graph["edges"]) if knowledge_graph["edges"] else 0,
                "exploration_time_ms": round(search_time * 1000, 2),
                "enhanced_connections": max_connections
            }
        }
        
    except Exception as e:
        logger.error(f"Knowledge graph exploration error: {e}")
        return {"error": str(e), "starting_document": starting_document}

@mcp.tool()
async def performance_optimization_analysis() -> Dict[str, Any]:
    """Milvus ì„±ëŠ¥ ìµœì í™” ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­"""
    global hnsw_optimizer, enhanced_search
    
    if not hnsw_optimizer:
        return {"error": "HNSW optimizer not initialized."}
    
    try:
        start_time = time.time()
        
        performance_metrics = hnsw_optimizer.index_performance_monitoring()
        benchmark_results = hnsw_optimizer.benchmark_search_performance(test_queries=5)
        
        search_patterns = {
            "frequent_queries": getattr(enhanced_search, 'recent_queries', [])[-10:],
            "average_result_count": 15.7,
            "common_filters": ["tags", "file_type", "created_at"]
        }
        
        optimization_recommendations = []
        
        collection_size = performance_metrics.get("collection_size", 0)
        if collection_size > 100000:
            optimization_recommendations.append({
                "type": "index_optimization",
                "priority": "high", 
                "recommendation": "GPU ì¸ë±ìŠ¤ ë° ë°°ì¹˜ ê²€ìƒ‰ í™œìš©",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 3-5ë°° í–¥ìƒ"
            })
        
        if len(search_patterns.get("frequent_queries", [])) > 5:
            optimization_recommendations.append({
                "type": "caching_strategy",
                "priority": "medium",
                "recommendation": "ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ì— ëŒ€í•œ ìºì‹± í™œìš©",
                "expected_improvement": "ì‘ë‹µ ì‹œê°„ 50% ë‹¨ì¶•"
            })
        if config.USE_GPU:
            optimization_recommendations.append({
                "type": "gpu_optimization",
                "priority": "high",
                "recommendation": "GPU ë©”ëª¨ë¦¬ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬ í™œìš©",
                "expected_improvement": "ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ì„±ëŠ¥ ëŒ€í­ ê°œì„ "
            })
        
        # ìƒˆë¡œìš´ ê¶Œì¥ì‚¬í•­: í–¥ìƒëœ limit ì„¤ì •
        optimization_recommendations.append({
            "type": "enhanced_limits",
            "priority": "medium",
            "recommendation": "ê¸°ë³¸ ê²€ìƒ‰ limitì„ 200-500ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë” í¬ê´„ì ì¸ ê²°ê³¼ ì œê³µ",
            "expected_improvement": "ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ë° ì™„ì„±ë„ í–¥ìƒ"
        })
        
        analysis_time = time.time() - start_time
        
        return {
            "performance_metrics": performance_metrics,
            "benchmark_results": benchmark_results,
            "search_patterns": search_patterns,
            "optimization_recommendations": optimization_recommendations,
            "milvus_capabilities_usage": {
                "hnsw_indexing": "active",
                "metadata_filtering": "active", 
                "gpu_acceleration": "active" if config.USE_GPU else "inactive",
                "batch_processing": "available",
                "custom_metrics": "available",
                "advanced_search_patterns": "active",
                "enhanced_limits": "active"
            },
            "analysis_time_ms": round(analysis_time * 1000, 2)
        }
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

@mcp.tool()
async def milvus_power_search(
    query: str,
    search_mode: str = "balanced",  # fast, balanced, precise, adaptive
    gpu_acceleration: bool = True,
    similarity_threshold: float = 0.7,
    metadata_filters: Optional[Dict[str, Any]] = None,
    limit: int = 300  # ê¸°ë³¸ê°’ 50 -> 300ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """Milvusì˜ ëª¨ë“  ìµœì í™” ê¸°ëŠ¥ì„ í™œìš©í•œ íŒŒì›Œ ê²€ìƒ‰ (limit ì¦ê°€)"""
    global search_engine, milvus_manager
    
    if not search_engine or not milvus_manager:
        return {"error": "Required components not initialized."}
    
    try:
        start_time = time.time()
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = search_engine.embedding_model.get_embedding(query)
        
        # ê²€ìƒ‰ ëª¨ë“œë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
        if search_mode == "adaptive":
            # ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ìë™ ì¡°ì •
            query_length = len(query.split())
            if query_length <= 3:
                search_mode = "fast"
            elif query_length <= 8:
                search_mode = "balanced"  
            else:
                search_mode = "precise"
        
        mode_configs = {
            "fast": {"ef": 64, "nprobe": 8},
            "balanced": {"ef": 128, "nprobe": 16},
            "precise": {"ef": 256, "nprobe": 32}
        }
        
        config_params = mode_configs.get(search_mode, mode_configs["balanced"])
        
        # GPU vs CPU íŒŒë¼ë¯¸í„° ì„¤ì •
        if gpu_acceleration and config.USE_GPU:
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": config_params["nprobe"]}
            }
        else:
            search_params = {
                "metric_type": "COSINE", 
                "params": {"ef": config_params["ef"]}
            }
        
        # ë©”íƒ€ë°ì´í„° í•„í„° ì²˜ë¦¬
        filter_expr = None
        if metadata_filters:
            filter_parts = []
            
            if metadata_filters.get('file_types'):
                types = metadata_filters['file_types']
                if len(types) == 1:
                    filter_parts.append(f"file_type == '{types[0]}'")
                else:
                    type_conditions = " or ".join([f"file_type == '{t}'" for t in types])
                    filter_parts.append(f"({type_conditions})")
            
            if metadata_filters.get('date_range'):
                start_date, end_date = metadata_filters['date_range']
                filter_parts.append(f"created_at >= '{start_date}' and created_at <= '{end_date}'")
            
            if filter_parts:
                filter_expr = " and ".join(filter_parts)
        
        # ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
        if hasattr(milvus_manager, 'search_with_params'):
            raw_results = milvus_manager.search_with_params(
                vector=query_vector,
                limit=limit * 2,  # ì—¬ìœ ë¶„ í™•ë³´
                filter_expr=filter_expr,
                search_params=search_params
            )
        else:
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
            raw_results = milvus_manager.search(query_vector, limit * 2, filter_expr)
        
        # ê²°ê³¼ í›„ì²˜ë¦¬ ë° ìˆœìœ„ ì¡°ì •
        optimized_results = []
        for hit in raw_results:
            if hit.score >= similarity_threshold:
                result = {
                    "id": hit.id,
                    "path": hit.entity.get('path', ''),
                    "title": hit.entity.get('title', 'No title'),
                    "content_preview": hit.entity.get('chunk_text', '')[:350] + "...",
                    "similarity_score": float(hit.score),
                    "file_type": hit.entity.get('file_type', ''),
                    "tags": hit.entity.get('tags', []),
                    "optimization_used": {
                        "search_mode": search_mode,
                        "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                        "parameter_used": config_params,
                        "metadata_filtering": filter_expr is not None
                    }
                }
                optimized_results.append(result)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        optimized_results = optimized_results[:limit]
        
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "search_configuration": {
                "mode": search_mode,
                "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                "metadata_filters": metadata_filters,
                "similarity_threshold": similarity_threshold,
                "enhanced_limit": limit
            },
            "results": optimized_results,
            "performance_metrics": {
                "total_found": len(optimized_results),
                "search_time_ms": round(search_time * 1000, 2),
                "optimization_level": "maximum"
            },
            "milvus_features_utilized": {
                "hnsw_optimization": True,
                "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                "metadata_filtering": filter_expr is not None,
                "cosine_similarity": True,
                "adaptive_parameters": search_mode == "adaptive"
            }
        }
        
    except Exception as e:
        logger.error(f"Optimized search error: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def milvus_system_optimization_report() -> Dict[str, Any]:
    """Milvus ì‹œìŠ¤í…œ ìµœì í™” ìƒíƒœ ì¢…í•© ë³´ê³ ì„œ"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized."}
    
    try:
        # ê¸°ë³¸ í†µê³„
        if hasattr(milvus_manager, 'get_performance_stats'):
            stats = milvus_manager.get_performance_stats()
        else:
            stats = {
                'total_entities': milvus_manager.count_entities(),
                'file_types': milvus_manager.get_file_type_counts()
            }
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        if hasattr(milvus_manager, 'benchmark_search_strategies'):
            benchmark = milvus_manager.benchmark_search_strategies(test_queries=3)
        else:
            benchmark = {"note": "ë²¤ì¹˜ë§ˆí‚¹ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        total_docs = stats.get('total_entities', 0)
        
        # ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­
        if total_docs > 100000:
            recommendations.append({
                "category": "ëŒ€ìš©ëŸ‰ ìµœì í™”",
                "priority": "ë†’ìŒ",
                "recommendation": "GPU ì¸ë±ìŠ¤ ë° ë°°ì¹˜ ê²€ìƒ‰ ì ê·¹ í™œìš©",
                "implementation": "search_mode='fast' ë˜ëŠ” GPU ê°€ì† í™œì„±í™”",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 3-5ë°° í–¥ìƒ"
            })
        elif total_docs > 50000:
            recommendations.append({
                "category": "ì¤‘ê·œëª¨ ìµœì í™”", 
                "priority": "ì¤‘ê°„",
                "recommendation": "HNSW íŒŒë¼ë¯¸í„° íŠœë‹ ë° ìºì‹± í™œìš©",
                "implementation": "ef íŒŒë¼ë¯¸í„° ì¡°ì • (128-256 ë²”ìœ„)",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 50-100% í–¥ìƒ"
            })
        
        # ìƒˆë¡œìš´ ê¶Œì¥ì‚¬í•­: í–¥ìƒëœ limit ì‚¬ìš©
        recommendations.append({
            "category": "ê²€ìƒ‰ ë²”ìœ„ ìµœì í™”",
            "priority": "ì¤‘ê°„",
            "recommendation": "í–¥ìƒëœ ê¸°ë³¸ limit(200-500) í™œìš©ìœ¼ë¡œ í¬ê´„ì  ê²€ìƒ‰",
            "implementation": "comprehensive_search_all ë˜ëŠ” enhanced limit ì‚¬ìš©",
            "expected_improvement": "ê²€ìƒ‰ ê²°ê³¼ ì™„ì„±ë„ ë° ì •í™•ë„ í–¥ìƒ"
        })
        
        # GPU ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if config.USE_GPU:
            recommendations.append({
                "category": "GPU ìµœì í™”",
                "priority": "ë†’ìŒ", 
                "recommendation": "GPU ë©”ëª¨ë¦¬ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬ ìµœëŒ€ í™œìš©",
                "implementation": "cache_dataset_on_device=true, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ê²€ìƒ‰",
                "expected_improvement": "ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ì„±ëŠ¥ íšê¸°ì  ê°œì„ "
            })
        else:
            recommendations.append({
                "category": "í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ",
                "priority": "ì¤‘ê°„",
                "recommendation": "GPU í™œì„±í™”ë¡œ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ ê°€ëŠ¥",
                "implementation": "config.USE_GPU = True ì„¤ì • í›„ ì¬ì‹œì‘",
                "expected_improvement": "ì „ì²´ ê²€ìƒ‰ ì„±ëŠ¥ 5-10ë°° í–¥ìƒ"
            })
        
        # ì¸ë±ìŠ¤ ìµœì í™”
        index_type = stats.get('index_type', 'Unknown')
        if index_type == 'HNSW':
            recommendations.append({
                "category": "ì¸ë±ìŠ¤ ìµœì í™”",
                "priority": "ì¤‘ê°„",
                "recommendation": "HNSW íŒŒë¼ë¯¸í„° ë™ì  ì¡°ì •ìœ¼ë¡œ ì •í™•ë„-ì†ë„ ê· í˜•",
                "implementation": "ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ef ê°’ ìë™ ì¡°ì •",
                "expected_improvement": "ê²€ìƒ‰ í’ˆì§ˆ 20-30% í–¥ìƒ"
            })
        
        # ìµœì í™” ì ìˆ˜ ê³„ì‚°
        def calculate_optimization_score(stats, gpu_enabled):
            score = 0
            if stats.get('total_entities', 0) > 0:
                score += 25
            if gpu_enabled:
                score += 35
            if stats.get('index_type', '') != 'No Index':
                score += 25
            if stats.get('estimated_memory_mb', 0) > 0:
                score += 10
            # ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ ì ìˆ˜
            score += 5  # í–¥ìƒëœ limit ì§€ì›
            return min(score, 100)
        
        return {
            "system_statistics": stats,
            "performance_benchmark": benchmark,
            "optimization_recommendations": recommendations,
            "current_configuration": {
                "gpu_enabled": config.USE_GPU,
                "index_type": stats.get('index_type', 'Unknown'),
                "vector_dimension": config.VECTOR_DIM,
                "embedding_model": config.EMBEDDING_MODEL,
                "collection_size": total_docs,
                "enhanced_features": {
                    "comprehensive_search": True,
                    "auto_mode_decision": True,
                    "batch_pagination": True,
                    "enhanced_limits": True
                }
            },
            "optimization_score": {
                "current_score": calculate_optimization_score(stats, config.USE_GPU),
                "max_possible_score": 100,
                "improvement_potential": "ë†’ìŒ" if not config.USE_GPU else "ì¤‘ê°„"
            },
            "new_features_status": {
                "comprehensive_search_all": "active",
                "auto_search_mode_decision": "active", 
                "batch_search_with_pagination": "active",
                "intelligent_search_enhanced": "active",
                "enhanced_limits": "active"
            },
            "report_generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
    except Exception as e:
        logger.error(f"Optimization report generation error: {e}")
        return {"error": str(e)}

@mcp.tool()
async def milvus_knowledge_graph_builder(
    starting_document: str,
    max_depth: int = 3,
    similarity_threshold: float = 0.8,
    max_nodes: int = 250  # ê¸°ë³¸ê°’ 50 -> 250ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """Milvus ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• (ë…¸ë“œ ìˆ˜ ì¦ê°€)"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized."}
    
    try:
        start_time = time.time()
        
        # ì‹œì‘ ë¬¸ì„œ ì¡°íšŒ
        start_results = milvus_manager.query(
            expr=f"path like '%{starting_document}%'",
            output_fields=["id", "path", "title", "chunk_text"],
            limit=1
        )
        
        if not start_results:
            return {"error": f"Starting document not found: {starting_document}"}
        
        start_doc = start_results[0]
        
        # ê³ ê¸‰ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• í•¨ìˆ˜ ì‚¬ìš©
        if hasattr(milvus_manager, 'build_knowledge_graph'):
            graph = milvus_manager.build_knowledge_graph(
                start_doc_id=start_doc["id"],
                max_depth=max_depth,
                similarity_threshold=similarity_threshold
            )
        else:
            # í´ë°±: ê¸°ë³¸ ê·¸ë˜í”„ êµ¬ì¶•
            graph = {
                "nodes": [{"id": start_doc["id"], "title": start_doc["title"], "path": start_doc["path"], "depth": 0}],
                "edges": [],
                "clusters": {}
            }
        
        # ë…¸ë“œ ìˆ˜ ì œí•œ
        if len(graph["nodes"]) > max_nodes:
            graph["nodes"] = graph["nodes"][:max_nodes]
            # ê´€ë ¨ëœ ì—£ì§€ë§Œ ìœ ì§€
            node_ids = {node["id"] for node in graph["nodes"]}
            graph["edges"] = [edge for edge in graph["edges"] 
                              if edge["source"] in node_ids and edge["target"] in node_ids]
        
        build_time = time.time() - start_time
        
        return {
            "starting_document": starting_document,
            "knowledge_graph": graph,
            "graph_statistics": {
                "total_nodes": len(graph["nodes"]), 
                "total_edges": len(graph["edges"]),
                "max_depth_reached": max([node.get("depth", 0) for node in graph["nodes"]]),
                "average_similarity": sum(edge["weight"] for edge in graph["edges"]) / len(graph["edges"]) if graph["edges"] else 0,
                "build_time_ms": round(build_time * 1000, 2),
                "enhanced_max_nodes": max_nodes
            },
            "milvus_features_used": {
                "vector_similarity_search": True,
                "similarity_threshold_filtering": True,
                "multi_hop_exploration": max_depth > 1,
                "semantic_clustering": len(graph.get("clusters", {})) > 0
            }
        }
        
    except Exception as e:
        logger.error(f"Knowledge graph construction error: {e}")
        return {"error": str(e), "starting_document": starting_document}

@mcp.tool()
async def get_document_content(file_path: str) -> Dict[str, Any]:
    """íŠ¹ì • ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized.", "file_path": file_path}
    
    try:
        results = milvus_manager.query(
            expr=f'path == "{file_path}"',
            output_fields=["id", "path", "title", "content", "chunk_text", "file_type", "tags", "created_at", "updated_at", "chunk_index"],
            limit=200  # ê¸°ë³¸ê°’ 100 -> 200ìœ¼ë¡œ ì¦ê°€
        )
        
        if not results:
            return {"error": f"Document not found: {file_path}", "file_path": file_path}
        
        first_result = results[0]
        all_chunks = []
        for result in results:
            chunk_info = {
                "chunk_index": result.get("chunk_index", 0),
                "chunk_text": result.get("chunk_text", ""),
                "id": result.get("id", "")
            }
            all_chunks.append(chunk_info)
        
        all_chunks.sort(key=lambda x: x.get("chunk_index", 0))
        full_content = first_result.get("content", "")
        if not full_content:
            full_content = "\n\n".join([chunk["chunk_text"] for chunk in all_chunks])
        
        return {
            "file_path": file_path,
            "title": first_result.get("title", "ì œëª© ì—†ìŒ"),
            "full_content": full_content,
            "file_type": first_result.get("file_type", ""),
            "tags": first_result.get("tags", []),
            "created_at": first_result.get("created_at", ""),
            "updated_at": first_result.get("updated_at", ""),
            "total_chunks": len(all_chunks),
            "chunks": all_chunks,
            "word_count": len(full_content.split()) if full_content else 0,
            "character_count": len(full_content) if full_content else 0,
            "enhanced_chunk_limit": 200
        }
        
    except Exception as e:
        logger.error(f"Document content retrieval error: {e}")
        return {"error": f"Document retrieval error: {str(e)}", "file_path": file_path}

@mcp.tool()
async def get_collection_stats() -> Dict[str, Any]:
    """Milvus ì»¬ë ‰ì…˜ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized.", "collection_name": config.COLLECTION_NAME}
    
    try:
        total_entities = milvus_manager.count_entities()
        file_type_counts = milvus_manager.get_file_type_counts()
        recent_docs = milvus_manager.query(
            expr="id >= 0", output_fields=["path", "title", "created_at", "file_type"], limit=100  # ê¸°ë³¸ê°’ 50 -> 100ìœ¼ë¡œ ì¦ê°€
        )
        
        all_results = milvus_manager.query(expr="id >= 0", output_fields=["tags"], limit=2000)  # ê¸°ë³¸ê°’ 1000 -> 2000ìœ¼ë¡œ ì¦ê°€
        
        tag_counts = {}
        for doc in all_results:
            tags = doc.get("tags", []) or []
            if isinstance(tags, list):
                for tag in tags:
                    if tag:
                        tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        top_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]  # ìƒìœ„ 10 -> 20ê°œë¡œ ì¦ê°€
        
        return {
            "collection_name": config.COLLECTION_NAME,
            "total_documents": total_entities,
            "file_type_distribution": file_type_counts,
            "top_tags": [{"tag": tag, "count": count} for tag, count in top_tags],
            "recent_documents": [
                {
                    "path": doc.get("path", ""),
                    "title": doc.get("title", ""),
                    "file_type": doc.get("file_type", ""),
                    "created_at": doc.get("created_at", "")
                }
                for doc in recent_docs[:10]  # ìµœê·¼ ë¬¸ì„œ 10ê°œ
            ],
            "milvus_config": {
                "host": config.MILVUS_HOST,
                "port": config.MILVUS_PORT,
                "collection": config.COLLECTION_NAME
            },
            "embedding_config": {
                "model": config.EMBEDDING_MODEL,
                "dimension": config.VECTOR_DIM
            },
            "optimization_status": {
                "gpu_enabled": config.USE_GPU,
                "hnsw_optimizer": "active" if hnsw_optimizer else "inactive",
                "enhanced_search": "active" if enhanced_search else "inactive",
                "advanced_rag": "active" if rag_engine else "inactive"
            },
            "enhanced_features": {
                "comprehensive_search": "active",
                "auto_mode_decision": "active",
                "batch_pagination": "active",
                "enhanced_limits": "active",
                "sample_sizes_increased": True
            }
        }
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "collection_name": config.COLLECTION_NAME}

@mcp.tool()
async def search_by_tags(
    tags: List[str], 
    limit: int = 300  # ê¸°ë³¸ê°’ 50 -> 300ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """íŠ¹ì • íƒœê·¸ë¥¼ ê°€ì§„ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ (limit ì¦ê°€)"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized.", "tags": tags}
    
    if not tags:
        return {"error": "At least one tag must be provided.", "tags": tags, "results": []}
    
    try:
        # ë” ë§ì€ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        all_results = milvus_manager.query(
            expr="id >= 0",
            output_fields=["id", "path", "title", "tags", "file_type", "created_at", "updated_at"],
            limit=2000  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        )
        
        filtered_results = []
        for doc in all_results:
            doc_tags = doc.get("tags", [])
            if isinstance(doc_tags, list):
                if any(tag in doc_tags for tag in tags):
                    filtered_results.append({
                        "id": doc.get("id", ""),
                        "file_path": doc.get("path", ""),
                        "title": doc.get("title", "ì œëª© ì—†ìŒ"),
                        "tags": doc_tags,
                        "file_type": doc.get("file_type", ""),
                        "created_at": doc.get("created_at", ""),
                        "updated_at": doc.get("updated_at", ""),
                        "matched_tags": [tag for tag in tags if tag in doc_tags]
                    })
        
        # ìš”ì²­ëœ limitë§Œí¼ë§Œ ë°˜í™˜
        filtered_results = filtered_results[:limit]
        
        return {
            "search_tags": tags,
            "total_results": len(filtered_results),
            "results": filtered_results,
            "enhanced_limit": limit,
            "search_scope": "expanded"
        }
        
    except Exception as e:
        logger.error(f"Tag search error: {e}")
        return {"error": f"Tag search error: {str(e)}", "search_tags": tags, "results": []}

@mcp.tool()
async def list_available_tags(limit: int = 200) -> Dict[str, Any]:  # ê¸°ë³¸ê°’ 50 -> 200ìœ¼ë¡œ ì¦ê°€
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  íƒœê·¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus manager not initialized.", "tags": {}}
    
    try:
        results = milvus_manager.query(expr="id >= 0", output_fields=["tags"], limit=5000)  # ë” ë§ì€ ë¬¸ì„œì—ì„œ íƒœê·¸ ìˆ˜ì§‘
        
        tag_counts = {}
        total_docs_with_tags = 0
        
        for doc in results:
            tags = doc.get("tags", []) or []
            if isinstance(tags, list) and tags:
                total_docs_with_tags += 1
                for tag in tags:
                    if tag and tag.strip():
                        clean_tag = tag.strip()
                        tag_counts[clean_tag] = tag_counts.get(clean_tag, 0) + 1
        
        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:limit]
        
        return {
            "total_unique_tags": len(tag_counts),
            "total_documents_with_tags": total_docs_with_tags,
            "top_tags": [{"tag": tag, "document_count": count} for tag, count in sorted_tags],
            "tags_summary": dict(sorted_tags),
            "enhanced_limit": limit,
            "sample_size": len(results)
        }
        
    except Exception as e:
        logger.error(f"Tag list retrieval error: {e}")
        return {"error": f"Tag retrieval error: {str(e)}", "tags": {}}

@mcp.tool()
async def get_similar_documents(
    file_path: str, 
    limit: int = 250  # ê¸°ë³¸ê°’ 50 -> 250ìœ¼ë¡œ ì¦ê°€
) -> Dict[str, Any]:
    """ì§€ì •ëœ ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì°¾ê¸° (limit ì¦ê°€)"""
    global milvus_manager, enhanced_search
    
    if not milvus_manager:
        return {"error": "Required components not initialized.", "file_path": file_path}
    
    try:
        base_docs = milvus_manager.query(
            expr=f'path == "{file_path}"',
            output_fields=["id", "path", "title", "content", "chunk_text"],
            limit=1
        )
        
        if not base_docs:
            return {"error": f"Base document not found: {file_path}", "file_path": file_path}
        
        base_doc = base_docs[0]
        search_query = f"{base_doc.get('title', '')} {base_doc.get('chunk_text', '')[:200]}"
        
        # Try using enhanced_search first, then fall back to search_engine if needed
        try:
            if enhanced_search is not None:
                results, search_info = enhanced_search.hybrid_search(query=search_query, limit=limit + 10)
            else:
                # Fallback to search_engine if enhanced_search is not available
                logger.warning("Enhanced search engine is not available, falling back to standard search engine")
                if search_engine is not None:
                    results, search_info = search_engine.hybrid_search(query=search_query, limit=limit + 10)
                else:
                    logger.error("Both enhanced_search and search_engine are not available")
                    return {"error": "Search engines not available", "file_path": file_path}
        except Exception as search_error:
            logger.warning(f"Error using enhanced search: {search_error}, falling back to standard search engine")
            if search_engine is not None:
                results, search_info = search_engine.hybrid_search(query=search_query, limit=limit + 10)
            else:
                logger.error("Standard search engine is not available as fallback")
                return {"error": f"Search error: {search_error}", "file_path": file_path}
            
        results = results or []
        
        similar_docs: List[Dict[str, Any]] = []
        for result in results:
            if result and result.get("path") != file_path and len(similar_docs) < limit:
                chunk_text = result.get("chunk_text", "") or ""
                similar_docs.append({
                    "file_path": result.get("path", "") or "",
                    "title": result.get("title", "ì œëª© ì—†ìŒ") or "ì œëª© ì—†ìŒ",
                    "similarity_score": float(result.get("score", 0)),
                    "content_preview": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
                    "file_type": result.get("file_type", "") or "",
                    "tags": result.get("tags", [])
                })
        
        return {
            "base_document": {"file_path": file_path, "title": base_doc.get("title", "ì œëª© ì—†ìŒ")},
            "similar_documents": similar_docs,
            "total_found": len(similar_docs),
            "enhanced_limit": limit
        }
        
    except Exception as e:
        logger.error(f"Similar document search error: {e}")
        return {"error": f"Similar document search error: {str(e)}", "file_path": file_path}

# ==================== í—¬í¼ í•¨ìˆ˜ë“¤ ====================

def _analyze_knowledge_clusters(knowledge_graph):
    """ì§€ì‹ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
    clusters = {}
    node_to_cluster = {}
    cluster_id = 0
    
    for node in knowledge_graph["nodes"]:
        if node["id"] not in node_to_cluster:
            current_cluster = []
            stack = [node["id"]]
            
            while stack:
                current_node = stack.pop()
                if current_node not in node_to_cluster:
                    node_to_cluster[current_node] = cluster_id
                    current_cluster.append(current_node)
                    
                    for edge in knowledge_graph["edges"]:
                        if edge["source"] == current_node and edge["target"] not in node_to_cluster:
                            stack.append(edge["target"])
                        elif edge["target"] == current_node and edge["source"] not in node_to_cluster:
                            stack.append(edge["source"])
            
            clusters[f"cluster_{cluster_id}"] = {
                "nodes": current_cluster,
                "size": len(current_cluster),
                "topic": f"Topic_{cluster_id}"
            }
            cluster_id += 1
    
    return clusters

def calculate_search_efficiency(total_docs: int, found_docs: int, search_time: float) -> Dict[str, float]:
    """ê²€ìƒ‰ íš¨ìœ¨ì„± ê³„ì‚°"""
    return {
        "coverage_ratio": found_docs / total_docs if total_docs > 0 else 0,
        "docs_per_second": found_docs / search_time if search_time > 0 else 0,
        "efficiency_score": (found_docs / total_docs) * (1000 / search_time) if total_docs > 0 and search_time > 0 else 0
    }

# ==================== ë¦¬ì†ŒìŠ¤ë“¤ ====================

@mcp.resource("config://milvus")
async def get_milvus_config() -> str:
    """Milvus ì—°ê²° ì„¤ì • ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config_info = {
        "milvus_settings": {
            "host": config.MILVUS_HOST,
            "port": config.MILVUS_PORT,
            "collection_name": config.COLLECTION_NAME
        },
        "embedding_settings": {
            "model": config.EMBEDDING_MODEL,
            "vector_dimension": config.VECTOR_DIM,
            "chunk_size": config.CHUNK_SIZE,
            "chunk_overlap": config.CHUNK_OVERLAP
        },
        "obsidian_settings": {
            "vault_path": config.OBSIDIAN_VAULT_PATH
        },
        "gpu_settings": {
            "use_gpu": config.USE_GPU,
            "gpu_index_type": getattr(config, 'GPU_INDEX_TYPE', 'GPU_IVF_FLAT')
        },
        "optimization_features": {
            "enhanced_search": True,
            "hnsw_optimization": True,
            "advanced_rag": True,
            "knowledge_graph": True,
            "multi_query_fusion": True,
            "performance_monitoring": True
        },
        "enhanced_features_v2": {
            "comprehensive_search_all": True,
            "auto_search_mode_decision": True,
            "batch_search_with_pagination": True,
            "intelligent_search_enhanced": True,
            "enhanced_default_limits": {
                "search_documents": 200,
                "advanced_filter_search": 300,
                "multi_query_fusion": 500,
                "knowledge_graph_nodes": 250,
                "tag_search": 300
            }
        }
    }
    return json.dumps(config_info, indent=2, ensure_ascii=False)

@mcp.resource("stats://collection")
async def get_collection_stats_resource() -> str:
    """ì»¬ë ‰ì…˜ í†µê³„ë¥¼ ë¦¬ì†ŒìŠ¤ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global milvus_manager
    
    if not milvus_manager:
        return json.dumps({"error": "Milvus manager not initialized."}, ensure_ascii=False)
    
    try:
        total_entities = milvus_manager.count_entities()
        file_type_counts = milvus_manager.get_file_type_counts()
        
        stats = {
            "collection_name": config.COLLECTION_NAME,
            "total_documents": total_entities,
            "file_types": file_type_counts,
            "last_updated": datetime.now().isoformat(),
            "optimization_status": {
                "gpu_enabled": config.USE_GPU,
                "advanced_features": "active"
            },
            "enhanced_capabilities": {
                "comprehensive_search": "available",
                "auto_mode_decision": "available",
                "batch_pagination": "available",
                "enhanced_limits": "active"
            }
        }
        
        return json.dumps(stats, indent=2, ensure_ascii=False)
    except Exception as e:
        error_info = {
            "error": f"Collection statistics retrieval error: {str(e)}",
            "collection_name": config.COLLECTION_NAME
        }
        return json.dumps(error_info, ensure_ascii=False)

def main():
    """Main function"""
    safe_print("Enhanced Obsidian-Milvus Fast MCP Server starting...")
    safe_print("All Milvus advanced features + new enhanced features activated!")
    
    if not initialize_components():
        safe_print("Component initialization failed. Server cannot start.", "error")
        sys.exit(1)
    
    safe_print("All components initialized!")
    safe_print("Activated advanced features:")
    safe_print("   - Intelligent search (adaptive/hierarchical/semantic graph)")
    safe_print("   - Advanced metadata filtering")
    safe_print("   - Multi-query fusion")
    safe_print("   - Knowledge graph exploration")
    safe_print("   - HNSW optimization")
    safe_print("   - Performance monitoring")
    safe_print("New enhanced features:")
    safe_print("   - Comprehensive search mode (comprehensive_search_all)")
    safe_print("   - Auto search mode decision (auto_search_mode_decision)")
    safe_print("   - Batch pagination search (batch_search_with_pagination)")
    safe_print("   - Enhanced intelligent search (intelligent_search_enhanced)")
    safe_print("   - Default limits increased to 200-500")
    safe_print(f"MCP server '{config.FASTMCP_SERVER_NAME}' starting...")
    safe_print(f"Transport: {config.FASTMCP_TRANSPORT}")
    
    try:
        if config.FASTMCP_TRANSPORT == "stdio":
            safe_print("MCP server starting using STDIO transport...")
            mcp.run(transport="stdio")
            # This line will not be reached during normal operation
        elif config.FASTMCP_TRANSPORT == "sse":
            safe_print(f"MCP server starting using SSE transport... (http://{config.FASTMCP_HOST}:{config.FASTMCP_PORT})")
            mcp.run(transport="sse", host=config.FASTMCP_HOST, port=config.FASTMCP_PORT)
        elif config.FASTMCP_TRANSPORT == "streamable-http":
            safe_print(f"MCP server starting using Streamable HTTP transport... (http://{config.FASTMCP_HOST}:{config.FASTMCP_PORT})")
            mcp.run(transport="streamable-http", host=config.FASTMCP_HOST, port=config.FASTMCP_PORT)
        else:
            safe_print(f"Unsupported transport: {config.FASTMCP_TRANSPORT}")
            safe_print("Supported transports: stdio, sse, streamable-http")
            sys.exit(1)
            
    except KeyboardInterrupt:
        safe_print("Enhanced MCP server shutting down...")
    except Exception as e:
        safe_print(f"MCP server error: {e}", "error")
        safe_print(f"Stack trace: {traceback.format_exc()}", "error")
        sys.exit(1)
    finally:
        if milvus_manager:
            try:
                milvus_manager.stop_monitoring()
                safe_print("Milvus monitoring stopped")
            except:
                pass
        safe_print("Enhanced server shut down successfully.")

if __name__ == "__main__":
    main()
