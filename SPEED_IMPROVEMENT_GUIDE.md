# ğŸš€ Embedding ì²˜ë¦¬ ì†ë„ ê°œì„  ê°€ì´ë“œ

## ğŸ“‹ **ë¬¸ì œ ë¶„ì„ ê²°ê³¼**

í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œ embedding ì²˜ë¦¬ê°€ ë§¤ìš° ëŠë¦° ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### ğŸ” **ì£¼ìš” ë³‘ëª© ì§€ì **
1. **ê°œë³„ íŒŒì¼ ì²˜ë¦¬**: ê° íŒŒì¼ì˜ ì²­í¬ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ embedding ìƒì„±
2. **ì‘ì€ ë°°ì¹˜ í¬ê¸°**: ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°ê°€ 32-128ë¡œ ë„ˆë¬´ ì‘ìŒ
3. **ê³¼ë„í•œ ëª¨ë‹ˆí„°ë§**: 2-3ì´ˆë§ˆë‹¤ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬ë¡œ ì²˜ë¦¬ ì¤‘ë‹¨
4. **ë³µì¡í•œ GPU ìµœì í™”**: ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” ë³µì¡í•œ ë¡œì§
5. **ë©”ëª¨ë¦¬ ë‹¨í¸í™”**: ì¦ì€ í…ì„œ ìƒì„±/ì‚­ì œë¡œ ì¸í•œ GPU ë©”ëª¨ë¦¬ ë‹¨í¸í™”

---

## âš¡ **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…**

### **1ë‹¨ê³„: ì†ë„ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬**
```bash
python embedding_speed_fix.py
```
- ì„ íƒ 1: í˜„ì¬ ì†ë„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
- ì„ íƒ 2: ìë™ ì„¤ì • ìµœì í™” ì ìš©

### **2ë‹¨ê³„: ì„¤ì • ìµœì í™”**
```bash
python config_optimizer.py
```
- ë°°ì¹˜ í¬ê¸°ë¥¼ 32 â†’ 2000ìœ¼ë¡œ ì¦ê°€ (62ë°°)
- ë©”ëª¨ë¦¬ ì²´í¬ ê°„ê²©ì„ 2ì´ˆ â†’ 30ì´ˆë¡œ ë³€ê²½
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì„ 70% â†’ 95%ë¡œ ì¦ê°€

### **3ë‹¨ê³„: ìµœì í™”ëœ í”„ë¡œì„¸ì„œ ì‚¬ìš©**
```bash
python obsidian_processor_optimized.py
```
- ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
- ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì„ë² ë”© ì ìš©

---

## ğŸ”§ **ìˆ˜ë™ ìµœì í™” ë°©ë²•**

### **A. config.py ìˆ˜ì •**
ê¸°ì¡´ ì„¤ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•˜ì„¸ìš”:

```python
# ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€
EMBEDDING_BATCH_SIZE = 2000  # ê¸°ì¡´: 32
BATCH_SIZE = 2000            # ê¸°ì¡´: 64

# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¦ê°€
GPU_MEMORY_FRACTION = 0.95   # ê¸°ì¡´: 0.7

# ë©”ëª¨ë¦¬ ì²´í¬ ê°„ê²© ì¦ê°€ (ì„±ëŠ¥ í–¥ìƒ)
MEMORY_CHECK_INTERVAL = 30   # ê¸°ì¡´: 2

# ë³µì¡í•œ ìµœì í™” ë¹„í™œì„±í™”
DISABLE_COMPLEX_GPU_OPTIMIZATION = True
FAST_MODE = True
```

### **B. embeddings.py ìˆ˜ì •**
`get_embeddings_batch()` ë©”ì„œë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìµœì í™”:

```python
def get_embeddings_batch_fast(self, texts, batch_size=2000):
    """ìµœì í™”ëœ ê³ ì† ë°°ì¹˜ ì„ë² ë”©"""
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        with torch.no_grad():
            vectors = self.model.encode(
                batch,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_tensor=False,
                normalize_embeddings=True,
                device=str(self.device)
            )
        
        embeddings.extend(vectors)
        
        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
        if i > 0 and i % (batch_size * 10) == 0:
            gc.collect()
            if 'cuda' in str(self.device):
                torch.cuda.empty_cache()
    
    return embeddings
```

### **C. obsidian_processor.py ìµœì í™”**
ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ì„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€ê²½:

```python
def process_all_files_batch(self):
    # 1ë‹¨ê³„: ëª¨ë“  ì²­í¬ ìˆ˜ì§‘
    all_chunks = []
    all_metadata = []
    
    for file_path in files:
        chunks, metadata = self._extract_chunks_from_file(file_path)
        all_chunks.extend(chunks)
        all_metadata.extend([metadata] * len(chunks))
    
    # 2ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì„ë² ë”©
    vectors = self.embedding_model.get_embeddings_batch_fast(all_chunks)
    
    # 3ë‹¨ê³„: ì¼ê´„ ì €ì¥
    self._save_vectors_batch(vectors, all_chunks, all_metadata)
```

---

## ğŸ“Š **ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**

| í•­ëª© | ê¸°ì¡´ | ìµœì í™” í›„ | í–¥ìƒë¥  |
|------|------|-----------|--------|
| ë°°ì¹˜ í¬ê¸° | 32 | 2000 | **62ë°°** |
| ì²˜ë¦¬ ì†ë„ | 1-5 texts/sec | 50-200 texts/sec | **10-40ë°°** |
| GPU ì‚¬ìš©ë¥  | 30-50% | 80-95% | **2-3ë°°** |
| ë©”ëª¨ë¦¬ ì²´í¬ | 2ì´ˆë§ˆë‹¤ | 30ì´ˆë§ˆë‹¤ | **15ë°° ê°ì†Œ** |
| ì „ì²´ ì‹œê°„ | 100% | 20-30% | **70-80% ë‹¨ì¶•** |

---

## ğŸ¯ **ì¶”ê°€ ìµœì í™” ì˜µì…˜**

### **1. ë” ë¹ ë¥¸ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©**
```python
# í˜„ì¬: sentence-transformers/all-MiniLM-L6-v2 (ëŠë¦¬ì§€ë§Œ ì •í™•)
# ëŒ€ì•ˆ 1: sentence-transformers/all-MiniLM-L12-v2 (ì†ë„ì™€ í’ˆì§ˆ ê· í˜•)
# ëŒ€ì•ˆ 2: sentence-transformers/paraphrase-MiniLM-L3-v2 (ë§¤ìš° ë¹ ë¦„)

EMBEDDING_MODEL = "sentence-transformers/paraphrase-MiniLM-L3-v2"
```

### **2. Ollama Embedding API ì‚¬ìš©**
```python
# Ollamaì˜ embedding APIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ë©´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ
import requests

def get_ollama_embedding(text):
    response = requests.post(
        'http://localhost:11434/api/embeddings',
        json={'model': 'nomic-embed-text', 'prompt': text}
    )
    return response.json()['embedding']
```

### **3. ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©**
```python
from multiprocessing import Pool
import multiprocessing as mp

def process_files_multiprocess():
    cpu_count = mp.cpu_count()
    with Pool(cpu_count // 2) as pool:  # CPU ì½”ì–´ì˜ ì ˆë°˜ ì‚¬ìš©
        results = pool.map(process_file_chunk, file_chunks)
```

### **4. PyTorch ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)**
```python
# embeddings.pyì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œ
import torch._dynamo
torch._dynamo.config.suppress_errors = True

# ëª¨ë¸ ì»´íŒŒì¼ (ì²« ì‹¤í–‰ ì‹œ ëŠë¦¬ì§€ë§Œ ì´í›„ ë§¤ìš° ë¹ ë¦„)
self.model = torch.compile(self.model)
```

---

## ğŸ› ï¸ **ë‹¨ê³„ë³„ ì ìš© ë°©ë²•**

### **Phase 1: ì¦‰ì‹œ ê°œì„  (5ë¶„ ì†Œìš”)**
1. `python config_optimizer.py` ì‹¤í–‰
2. ì„¤ì • ìµœì í™” ì ìš©
3. í”„ë¡œê·¸ë¨ ì¬ì‹œì‘

**ì˜ˆìƒ íš¨ê³¼: 3-5ë°° ì†ë„ í–¥ìƒ**

### **Phase 2: ë°°ì¹˜ ì²˜ë¦¬ ì ìš© (10ë¶„ ì†Œìš”)**
1. `obsidian_processor_optimized.py` ë³µì‚¬
2. ê¸°ì¡´ `obsidian_processor.py` ë°±ì—…
3. ìƒˆ íŒŒì¼ë¡œ êµì²´
4. `main.py`ì—ì„œ import ìˆ˜ì •

**ì˜ˆìƒ íš¨ê³¼: ì¶”ê°€ 5-10ë°° ì†ë„ í–¥ìƒ**

### **Phase 3: ê³ ê¸‰ ìµœì í™” (30ë¶„ ì†Œìš”)**
1. ë” ë¹ ë¥¸ ì„ë² ë”© ëª¨ë¸ë¡œ ë³€ê²½
2. Ollama API í™œìš© ê²€í† 
3. ë©€í‹°í”„ë¡œì„¸ì‹± ì ìš©

**ì˜ˆìƒ íš¨ê³¼: ì¶”ê°€ 2-3ë°° ì†ë„ í–¥ìƒ**

---

## ğŸ”§ **ì‹¤ì œ ì ìš© ì˜ˆì‹œ**

### **Before (ê¸°ì¡´)**
```
ì²˜ë¦¬ ì†ë„: 2 texts/sec
1000ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œê°„: 8ë¶„ 20ì´ˆ
GPU ì‚¬ìš©ë¥ : 35%
ë©”ëª¨ë¦¬ ì²´í¬: 2ì´ˆë§ˆë‹¤
```

### **After (ìµœì í™” í›„)**
```
ì²˜ë¦¬ ì†ë„: 80 texts/sec  
1000ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œê°„: 12ì´ˆ
GPU ì‚¬ìš©ë¥ : 90%
ë©”ëª¨ë¦¬ ì²´í¬: 30ì´ˆë§ˆë‹¤
```

**ê²°ê³¼: 40ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„!**

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

### **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€**
- ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ RAM/GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- ì‹œìŠ¤í…œ ì‚¬ì–‘ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¡°ì • í•„ìš”

### **ì„¤ì • ë°±ì—…**
- ëª¨ë“  ìˆ˜ì • ì „ì— ê¸°ì¡´ íŒŒì¼ ë°±ì—…
- `config_backup.py` íŒŒì¼ë¡œ ì›ë˜ ì„¤ì • ë³µì› ê°€ëŠ¥

### **í…ŒìŠ¤íŠ¸ ê¶Œì¥**
- ì†Œê·œëª¨ íŒŒì¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
- ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ì¸ í›„ ì „ì²´ ì ìš©

---

## ğŸš€ **ì¦‰ì‹œ ì‹œì‘í•˜ê¸°**

1. **ë¨¼ì € í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •:**
```bash
python embedding_speed_fix.py
# ì„ íƒ: 1 (ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸)
```

2. **ì„¤ì • ìµœì í™” ì ìš©:**
```bash
python config_optimizer.py  
# ì„ íƒ: 2 (ì„¤ì • ìµœì í™” ì ìš©)
```

3. **í”„ë¡œê·¸ë¨ ì¬ì‹œì‘ í›„ í™•ì¸:**
```bash
python main.py
# ì„ íƒ: 1 ë˜ëŠ” 2 (ì„ë² ë”© ì‹¤í–‰)
```

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼í•˜ë©´ **í˜„ì¬ ì†ë„ì˜ 5-40ë°°** ë¹ ë¥¸ embedding ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤!

---

## ğŸ“ **ë¬¸ì œ ë°œìƒ ì‹œ**

1. **ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**: ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
2. **GPU ì˜¤ë¥˜**: CPU ëª¨ë“œë¡œ ì „í™˜í•˜ê±°ë‚˜ GPU ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸
3. **ì›ë˜ ìƒíƒœ ë³µì›**: `python config_optimizer.py`ì—ì„œ ì„ íƒ 3

**í•µì‹¬: ê°œë³„ ì²˜ë¦¬ â†’ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ 10ë°° ì´ìƒ ë¹¨ë¼ì§‘ë‹ˆë‹¤!**
