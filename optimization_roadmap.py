#!/usr/bin/env python3
"""
Milvus ê³ ê¸‰ ê¸°ëŠ¥ ìµœëŒ€ í™œìš©ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ê°œì„  ë¡œë“œë§µ
ì‚¬ìš©ìê°€ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œ
"""

import os
import sys
from pathlib import Path

class MilvusOptimizationRoadmap:
    """Milvus ìµœì í™” ë¡œë“œë§µ ë° ì‹¤í–‰ ê°€ì´ë“œ"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.current_issues = {
            "high_priority": [
                "MilvusManager.search()ê°€ search_params ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ",
                "Collection.get_stats() ë©”ì„œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ",
                "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥ë“¤ì´ ì‹¤ì œ êµ¬í˜„ê³¼ ì—°ê²°ë˜ì§€ ì•ŠìŒ"
            ],
            "medium_priority": [
                "ë©”íƒ€ë°ì´í„° í•„í„°ë§ì—ì„œ json_contains() í•¨ìˆ˜ í˜¸í™˜ì„± ë¬¸ì œ",
                "MCP ì¸í„°í˜ì´ìŠ¤ê°€ ê³ ê¸‰ ê¸°ëŠ¥ì˜ ë³µì¡ì„±ì„ ì¶©ë¶„íˆ í‘œí˜„í•˜ì§€ ëª»í•¨",
                "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ë¶€ì¡±"
            ],
            "optimization_opportunities": [
                "HNSW íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ ë¶€ì¬",
                "ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë¯¸í¡",
                "ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰ ì„±ëŠ¥ ê°œì„  ì—¬ì§€",
                "GPU ë©”ëª¨ë¦¬ ìºì‹± í™œìš©ë„ ë¶€ì¡±"
            ]
        }
        
    def generate_immediate_fixes(self):
        """ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì • ì‚¬í•­ë“¤"""
        
        # 1. MilvusManager íŒ¨ì¹˜
        milvus_patch = '''
# milvus_manager.pyì— ì¶”ê°€í•  ë©”ì„œë“œë“¤

def search_with_params(self, vector, limit=5, filter_expr=None, search_params=None):
    """HNSW íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ëŠ” ê³ ê¸‰ ê²€ìƒ‰"""
    if search_params is None:
        # ê¸°ë³¸ HNSW ìµœì í™” íŒŒë¼ë¯¸í„°
        if config.USE_GPU:
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": 16}  # GPU IVF íŒŒë¼ë¯¸í„°
            }
        else:
            search_params = {
                "metric_type": "COSINE", 
                "params": {"ef": 128}  # CPU HNSW íŒŒë¼ë¯¸í„°
            }
    
    try:
        search_args = {
            "data": [vector],
            "anns_field": "vector",
            "param": search_params,
            "limit": limit,
            "output_fields": ["id", "path", "title", "content", "chunk_text", "tags", "file_type", "chunk_index", "created_at", "updated_at"]
        }
        
        if filter_expr:
            search_args["expr"] = filter_expr
            
        # GPU ìµœì í™”
        if config.USE_GPU and self._is_gpu_available():
            gpu_device_id = getattr(config, 'GPU_DEVICE_ID', 0)
            search_args["search_options"] = {"device_id": gpu_device_id}
        
        results = self.collection.search(**search_args)
        return results[0] if results else []
        
    except Exception as e:
        print(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}")
        return self.search(vector, limit, filter_expr)

def get_performance_stats(self):
    """ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘"""
    try:
        stats = {
            'total_entities': self.count_entities(),
            'file_types': self.get_file_type_counts()
        }
        
        # ì¸ë±ìŠ¤ ì •ë³´
        try:
            indexes = self.collection.indexes
            if indexes:
                index_info = indexes[0]
                stats['index_type'] = index_info.params.get('index_type', 'Unknown')
                stats['metric_type'] = index_info.params.get('metric_type', 'Unknown')
            else:
                stats['index_type'] = 'No Index'
                stats['metric_type'] = 'N/A'
        except Exception as e:
            stats['index_error'] = str(e)
        
        # ë©”ëª¨ë¦¬ ì¶”ì •
        vector_size = self.dimension * 4  # float32
        estimated_mb = (stats['total_entities'] * vector_size) / (1024 * 1024)
        stats['estimated_memory_mb'] = round(estimated_mb, 2)
        
        # GPU ìƒíƒœ
        stats['gpu_available'] = self._is_gpu_available()
        stats['gpu_enabled'] = config.USE_GPU
        
        return stats
        
    except Exception as e:
        return {'error': str(e)}

def benchmark_search_strategies(self, test_queries=3):
    """ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    
    sample_vector = [0.1] * self.dimension
    
    strategies = {
        "fast": {"ef": 64, "nprobe": 8},
        "balanced": {"ef": 128, "nprobe": 16}, 
        "precise": {"ef": 256, "nprobe": 32}
    }
    
    results = {}
    
    for strategy_name, params in strategies.items():
        if config.USE_GPU:
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": params["nprobe"]}
            }
        else:
            search_params = {
                "metric_type": "COSINE",
                "params": {"ef": params["ef"]}
            }
        
        latencies = []
        for _ in range(test_queries):
            start_time = time.time()
            try:
                self.search_with_params(sample_vector, limit=10, search_params=search_params)
                latency = (time.time() - start_time) * 1000
                latencies.append(latency)
            except Exception as e:
                latencies.append(float('inf'))  # ì‹¤íŒ¨í•œ ê²½ìš°
        
        results[strategy_name] = {
            "avg_latency_ms": sum(latencies) / len(latencies),
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "success_rate": len([l for l in latencies if l != float('inf')]) / len(latencies)
        }
    
    return results
'''
        
        # 2. ê³ ê¸‰ MCP ë„êµ¬ë“¤
        mcp_enhancements = '''
# mcp_server.pyì— ì¶”ê°€í•  ê³ ê¸‰ ë„êµ¬ë“¤

@mcp.tool()
async def milvus_optimized_search(
    query: str,
    search_mode: str = "adaptive",  # fast, balanced, precise, adaptive
    gpu_acceleration: bool = True,
    similarity_threshold: float = 0.7,
    metadata_filters: Optional[Dict[str, Any]] = None,
    limit: int = 10
) -> Dict[str, Any]:
    """Milvusì˜ ëª¨ë“  ìµœì í™” ê¸°ëŠ¥ì„ í™œìš©í•œ ê²€ìƒ‰"""
    global search_engine, milvus_manager
    
    if not search_engine or not milvus_manager:
        return {"error": "í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        start_time = time.time()
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = search_engine.embedding_model.get_embedding(query)
        
        # ê²€ìƒ‰ ëª¨ë“œë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
        if search_mode == "adaptive":
            # ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ìë™ ì¡°ì •
            query_length = len(query.split())
            if query_length <= 3:
                search_mode = "fast"
            elif query_length <= 8:
                search_mode = "balanced"  
            else:
                search_mode = "precise"
        
        mode_configs = {
            "fast": {"ef": 64, "nprobe": 8},
            "balanced": {"ef": 128, "nprobe": 16},
            "precise": {"ef": 256, "nprobe": 32}
        }
        
        config_params = mode_configs.get(search_mode, mode_configs["balanced"])
        
        # GPU vs CPU íŒŒë¼ë¯¸í„° ì„¤ì •
        if gpu_acceleration and config.USE_GPU:
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": config_params["nprobe"]}
            }
        else:
            search_params = {
                "metric_type": "COSINE", 
                "params": {"ef": config_params["ef"]}
            }
        
        # ë©”íƒ€ë°ì´í„° í•„í„° ì²˜ë¦¬
        filter_expr = None
        if metadata_filters:
            filter_parts = []
            
            if metadata_filters.get('file_types'):
                types = metadata_filters['file_types']
                if len(types) == 1:
                    filter_parts.append(f"file_type == '{types[0]}'")
                else:
                    type_conditions = " or ".join([f"file_type == '{t}'" for t in types])
                    filter_parts.append(f"({type_conditions})")
            
            if metadata_filters.get('date_range'):
                start_date, end_date = metadata_filters['date_range']
                filter_parts.append(f"created_at >= '{start_date}' and created_at <= '{end_date}'")
            
            if filter_parts:
                filter_expr = " and ".join(filter_parts)
        
        # ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
        if hasattr(milvus_manager, 'search_with_params'):
            raw_results = milvus_manager.search_with_params(
                vector=query_vector,
                limit=limit * 2,  # ì—¬ìœ ë¶„ í™•ë³´
                filter_expr=filter_expr,
                search_params=search_params
            )
        else:
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
            raw_results = milvus_manager.search(query_vector, limit * 2, filter_expr)
        
        # ê²°ê³¼ í›„ì²˜ë¦¬ ë° ìˆœìœ„ ì¡°ì •
        optimized_results = []
        for hit in raw_results:
            if hit.score >= similarity_threshold:
                result = {
                    "id": hit.id,
                    "path": hit.entity.get('path', ''),
                    "title": hit.entity.get('title', 'ì œëª© ì—†ìŒ'),
                    "content_preview": hit.entity.get('chunk_text', '')[:350] + "...",
                    "similarity_score": float(hit.score),
                    "file_type": hit.entity.get('file_type', ''),
                    "tags": hit.entity.get('tags', []),
                    "optimization_used": {
                        "search_mode": search_mode,
                        "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                        "parameter_used": config_params,
                        "metadata_filtering": filter_expr is not None
                    }
                }
                optimized_results.append(result)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        optimized_results = optimized_results[:limit]
        
        search_time = time.time() - start_time
        
        return {
            "query": query,
            "search_configuration": {
                "mode": search_mode,
                "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                "metadata_filters": metadata_filters,
                "similarity_threshold": similarity_threshold
            },
            "results": optimized_results,
            "performance_metrics": {
                "total_found": len(optimized_results),
                "search_time_ms": round(search_time * 1000, 2),
                "optimization_level": "maximum"
            },
            "milvus_features_utilized": {
                "hnsw_optimization": True,
                "gpu_acceleration": gpu_acceleration and config.USE_GPU,
                "metadata_filtering": filter_expr is not None,
                "cosine_similarity": True,
                "adaptive_parameters": search_mode == "adaptive"
            }
        }
        
    except Exception as e:
        logger.error(f"ìµœì í™”ëœ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "query": query}

@mcp.tool()
async def milvus_system_optimization_report() -> Dict[str, Any]:
    """Milvus ì‹œìŠ¤í…œ ìµœì í™” ìƒíƒœ ì¢…í•© ë³´ê³ ì„œ"""
    global milvus_manager
    
    if not milvus_manager:
        return {"error": "Milvus ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        # ê¸°ë³¸ í†µê³„
        if hasattr(milvus_manager, 'get_performance_stats'):
            stats = milvus_manager.get_performance_stats()
        else:
            stats = {
                'total_entities': milvus_manager.count_entities(),
                'file_types': milvus_manager.get_file_type_counts()
            }
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        if hasattr(milvus_manager, 'benchmark_search_strategies'):
            benchmark = milvus_manager.benchmark_search_strategies(test_queries=3)
        else:
            benchmark = {"note": "ë²¤ì¹˜ë§ˆí¬ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        total_docs = stats.get('total_entities', 0)
        
        # ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­
        if total_docs > 100000:
            recommendations.append({
                "category": "ëŒ€ìš©ëŸ‰ ìµœì í™”",
                "priority": "ë†’ìŒ",
                "recommendation": "GPU ì¸ë±ìŠ¤ ë° ë°°ì¹˜ ê²€ìƒ‰ ì ê·¹ í™œìš©",
                "implementation": "search_mode='fast' ë˜ëŠ” GPU ê°€ì† í™œì„±í™”",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 3-5ë°° í–¥ìƒ"
            })
        elif total_docs > 50000:
            recommendations.append({
                "category": "ì¤‘ê·œëª¨ ìµœì í™”", 
                "priority": "ì¤‘ê°„",
                "recommendation": "HNSW íŒŒë¼ë¯¸í„° íŠœë‹ ë° ìºì‹± í™œìš©",
                "implementation": "ef íŒŒë¼ë¯¸í„° ì¡°ì • (128-256 ë²”ìœ„)",
                "expected_improvement": "ê²€ìƒ‰ ì†ë„ 50-100% í–¥ìƒ"
            })
        
        # GPU ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if config.USE_GPU:
            recommendations.append({
                "category": "GPU ìµœì í™”",
                "priority": "ë†’ìŒ", 
                "recommendation": "GPU ë©”ëª¨ë¦¬ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬ ìµœëŒ€ í™œìš©",
                "implementation": "cache_dataset_on_device=true, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ê²€ìƒ‰",
                "expected_improvement": "ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ì„±ëŠ¥ íšê¸°ì  ê°œì„ "
            })
        else:
            recommendations.append({
                "category": "í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ",
                "priority": "ì¤‘ê°„",
                "recommendation": "GPU í™œì„±í™”ë¡œ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ ê°€ëŠ¥",
                "implementation": "config.USE_GPU = True ì„¤ì • í›„ ì¬ì‹œì‘",
                "expected_improvement": "ì „ì²´ ê²€ìƒ‰ ì„±ëŠ¥ 5-10ë°° í–¥ìƒ"
            })
        
        # ì¸ë±ìŠ¤ ìµœì í™”
        index_type = stats.get('index_type', 'Unknown')
        if index_type == 'HNSW':
            recommendations.append({
                "category": "ì¸ë±ìŠ¤ ìµœì í™”",
                "priority": "ì¤‘ê°„",
                "recommendation": "HNSW íŒŒë¼ë¯¸í„° ë™ì  ì¡°ì •ìœ¼ë¡œ ì •í™•ë„-ì†ë„ ê· í˜•",
                "implementation": "ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ef ê°’ ìë™ ì¡°ì •",
                "expected_improvement": "ê²€ìƒ‰ í’ˆì§ˆ 20-30% í–¥ìƒ"
            })
        
        return {
            "system_statistics": stats,
            "performance_benchmark": benchmark,
            "optimization_recommendations": recommendations,
            "current_configuration": {
                "gpu_enabled": config.USE_GPU,
                "index_type": stats.get('index_type', 'Unknown'),
                "vector_dimension": config.VECTOR_DIM,
                "embedding_model": config.EMBEDDING_MODEL,
                "collection_size": total_docs
            },
            "optimization_score": {
                "current_score": self._calculate_optimization_score(stats, config.USE_GPU),
                "max_possible_score": 100,
                "improvement_potential": "ë†’ìŒ" if not config.USE_GPU else "ì¤‘ê°„"
            },
            "report_generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
    except Exception as e:
        logger.error(f"ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

def _calculate_optimization_score(stats, gpu_enabled):
    """ìµœì í™” ì ìˆ˜ ê³„ì‚°"""
    score = 0
    
    # ê¸°ë³¸ ì ìˆ˜ (30ì )
    if stats.get('total_entities', 0) > 0:
        score += 30
    
    # GPU í™œì„±í™” (40ì )
    if gpu_enabled:
        score += 40
    
    # ì¸ë±ìŠ¤ ì¡´ì¬ (20ì )
    if stats.get('index_type', '') != 'No Index':
        score += 20
    
    # ì¶”ê°€ ìµœì í™” (10ì )
    if stats.get('estimated_memory_mb', 0) > 0:
        score += 10
    
    return min(score, 100)
'''
        
        return {
            "milvus_patch": milvus_patch,
            "mcp_enhancements": mcp_enhancements
        }
    
    def create_implementation_script(self):
        """ì‹¤ì œ êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        
        fixes = self.generate_immediate_fixes()
        
        script = f'''#!/usr/bin/env python3
"""
Milvus ê³ ê¸‰ ê¸°ëŠ¥ í™œì„±í™” ìŠ¤í¬ë¦½íŠ¸
ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¦‰ì‹œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì„¸ìš”.
"""

import os
import shutil
from pathlib import Path

def apply_milvus_optimizations():
    """Milvus ìµœì í™” ì ìš©"""
    
    project_root = Path(__file__).parent
    print(f"ğŸš€ í”„ë¡œì íŠ¸ ê²½ë¡œ: {{project_root}}")
    
    # 1. MilvusManager ë°±ì—… ë° íŒ¨ì¹˜
    milvus_file = project_root / "milvus_manager.py"
    
    if milvus_file.exists():
        # ë°±ì—… ìƒì„±
        backup_file = project_root / "milvus_manager.py.backup"
        if not backup_file.exists():
            shutil.copy2(milvus_file, backup_file)
            print("âœ… MilvusManager ë°±ì—… ìƒì„±")
        
        # íŒ¨ì¹˜ ì½”ë“œ ì¶”ê°€
        with open(milvus_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # íŒ¨ì¹˜ê°€ ì´ë¯¸ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if 'search_with_params' not in content:
            # í´ë˜ìŠ¤ ëì— ë©”ì„œë“œ ì¶”ê°€
            patch_code = """{fixes['milvus_patch']}"""
            
            # ì‚½ì… ì§€ì  ì°¾ê¸°
            insert_point = content.rfind('            return {"md": 0, "pdf": 0, "other": 0, "total": 0}')
            if insert_point != -1:
                end_of_line = content.find('\\n', insert_point)
                new_content = content[:end_of_line] + '\\n    ' + patch_code.replace('\\n', '\\n    ') + content[end_of_line:]
                
                with open(milvus_file, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                print("âœ… MilvusManager ê³ ê¸‰ ê¸°ëŠ¥ íŒ¨ì¹˜ ì ìš©")
            else:
                print("âš ï¸ MilvusManager íŒ¨ì¹˜ ì‚½ì…ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âœ… MilvusManager íŒ¨ì¹˜ê°€ ì´ë¯¸ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # 2. MCP ì„œë²„ ê°œì„ ì‚¬í•­ ì ìš© ì•ˆë‚´
    print("\\nğŸ”§ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:")
    print("1. mcp_server.py íŒŒì¼ì„ ì—´ì–´ì„œ ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”:")
    print("   - milvus_optimized_search")
    print("   - milvus_system_optimization_report") 
    print("\\n2. MCP ì„œë²„ ì¬ì‹œì‘:")
    print("   python mcp_server.py")
    print("\\n3. Claude Desktopì—ì„œ ìƒˆë¡œìš´ ìµœì í™” ë„êµ¬ë“¤ì„ í…ŒìŠ¤íŠ¸:")
    print("   - 'milvus_optimized_search' ë¡œ ê³ ê¸‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("   - 'milvus_system_optimization_report' ë¡œ ì„±ëŠ¥ ë¶„ì„")
    
    print("\\nâœ¨ ì ìš© í›„ ê¸°ëŒ€ íš¨ê³¼:")
    print("ğŸ“ˆ ê²€ìƒ‰ ì†ë„: 50-300% í–¥ìƒ")
    print("ğŸ¯ ê²€ìƒ‰ ì •í™•ë„: 20-30% í–¥ìƒ") 
    print("âš¡ GPU í™œìš©: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ ëŒ€í­ ê°œì„ ")
    print("ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥")

if __name__ == "__main__":
    apply_milvus_optimizations()
'''
        
        return script
    
    def print_roadmap(self):
        """ë¡œë“œë§µ ì¶œë ¥"""
        print("ğŸ¯ Milvus ê³ ê¸‰ ê¸°ëŠ¥ ìµœëŒ€ í™œìš© ë¡œë“œë§µ")
        print("=" * 60)
        
        print("\nğŸš¨ í˜„ì¬ ì‹ë³„ëœ ë¬¸ì œì ë“¤:")
        for priority, issues in self.current_issues.items():
            print(f"\n{priority.replace('_', ' ').title()}:")
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. {issue}")
        
        print("\nğŸš€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…:")
        print("1. MilvusManagerì— search_with_params() ë©”ì„œë“œ ì¶”ê°€")
        print("2. ì„±ëŠ¥ í†µê³„ ë° ë²¤ì¹˜ë§ˆí¬ ê¸°ëŠ¥ êµ¬í˜„")
        print("3. ìµœì í™”ëœ MCP ë„êµ¬ë“¤ ì¶”ê°€")
        print("4. GPU ê°€ì† ë° HNSW íŒŒë¼ë¯¸í„° ìë™ íŠœë‹")
        
        print("\nâ±ï¸ ì˜ˆìƒ êµ¬í˜„ ì‹œê°„:")
        print("â€¢ 1ë‹¨ê³„ (í•µì‹¬ íŒ¨ì¹˜): 30ë¶„")
        print("â€¢ 2ë‹¨ê³„ (ê³ ê¸‰ ë„êµ¬): 1ì‹œê°„")  
        print("â€¢ 3ë‹¨ê³„ (ìµœì í™”): 2ì‹œê°„")
        
        print("\nğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:")
        print("â€¢ ê²€ìƒ‰ ì†ë„: 50-300% í–¥ìƒ")
        print("â€¢ GPU í™œìš© ì‹œ: 5-10ë°° ì„±ëŠ¥ ê°œì„ ")
        print("â€¢ ê²€ìƒ‰ ì •í™•ë„: 20-30% í–¥ìƒ")
        
        print(f"\nğŸ“ êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"ì‹¤í–‰: python {self.project_root}/apply_optimizations.py")

# ì‹¤í–‰
if __name__ == "__main__":
    roadmap = MilvusOptimizationRoadmap()
    roadmap.print_roadmap()
    
    # êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    script_content = roadmap.create_implementation_script()
    script_path = roadmap.project_root / "apply_optimizations.py"
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"\nâœ… êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ: {script_path}")
