#!/usr/bin/env python3
"""
ì‹¤ì œ íŒŒì¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import os
import time
import torch
from embeddings import EmbeddingModel
from obsidian_processor import ObsidianProcessor
from milvus_manager import MilvusManager
import config

def test_real_file_processing():
    """ì‹¤ì œ íŒŒì¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Starting real file processing test...")
    
    # 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    print(f"CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # 2. EmbeddingModel ì´ˆê¸°í™”
    embedding_model = EmbeddingModel()
    
    # 3. ì‘ì€ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    test_dir = os.path.join(config.OBSIDIAN_VAULT_PATH, "batch_test")
    os.makedirs(test_dir, exist_ok=True)
    
    test_file = os.path.join(test_dir, "batch_test.md")
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (ì—¬ëŸ¬ ì²­í¬ê°€ ìƒì„±ë˜ë„ë¡)
    test_content = """# Test Document for Batch Processing

## Section 1: Introduction
This is a test document designed to create multiple chunks for batch processing testing. The content needs to be long enough to be split into several chunks to properly test the batch embedding functionality.

## Section 2: Technical Details
Batch processing should significantly improve performance by processing multiple text chunks simultaneously instead of one by one. This reduces the overhead of individual GPU calls and maximizes throughput.

## Section 3: Performance Expectations
We expect to see:
- 5-10x speed improvement with batch processing
- Higher GPU utilization during processing
- More efficient memory usage
- Reduced processing time per chunk

## Section 4: Implementation Details
The batch processing implementation should use the get_embeddings_batch_adaptive method which automatically adjusts batch sizes based on system capabilities and current resource usage.

## Section 5: Monitoring
We will monitor:
- GPU utilization percentage
- Memory usage patterns
- Processing time per batch
- Overall throughput improvement

## Section 6: Conclusion
This test will help us verify that the batch processing optimization is working correctly and providing the expected performance improvements.

This document contains enough content to be split into multiple chunks, allowing us to test the batch processing functionality properly.
"""
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‘ì„±
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    print(f"âœ… Test file created: {test_file}")
    
    # 4. ObsidianProcessor ì´ˆê¸°í™”
    try:
        milvus_manager = MilvusManager()
        processor = ObsidianProcessor(milvus_manager)
        print("âœ… ObsidianProcessor initialized")
    except Exception as e:
        print(f"âŒ Error initializing processor: {e}")
        return
    
    # 5. íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\nğŸš€ Testing file processing with batch embedding...")
    
    try:
        # GPU ëª¨ë‹ˆí„°ë§ ì‹œì‘
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            start_memory = torch.cuda.memory_allocated()
        
        start_time = time.time()
        
        # íŒŒì¼ ì²˜ë¦¬
        success = processor.process_file(test_file)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if torch.cuda.is_available():
            end_memory = torch.cuda.memory_allocated()
            peak_memory = torch.cuda.max_memory_allocated()
            memory_used = (peak_memory - start_memory) / 1024**2  # MB
        
        print(f"\nğŸ“Š Processing Results:")
        print(f"âœ… Success: {success}")
        print(f"â±ï¸ Processing time: {processing_time:.2f} seconds")
        
        if torch.cuda.is_available():
            print(f"ğŸ”¥ GPU memory used: {memory_used:.1f} MB")
            print(f"ğŸ“ˆ Peak GPU memory: {peak_memory / 1024**2:.1f} MB")
        
        # í˜„ì¬ ë°°ì¹˜ í¬ê¸° í™•ì¸
        if hasattr(processor.embedding_model, 'batch_optimizer'):
            batch_size = processor.embedding_model.batch_optimizer.current_batch_size
            print(f"ğŸ“¦ Current batch size: {batch_size}")
        
        # 6. ì²­í¬ë³„ ê°œë³„ ì²˜ë¦¬ì™€ ë¹„êµ
        print("\nğŸ”„ Testing individual chunk processing for comparison...")
        
        # íŒŒì¼ì—ì„œ ì²­í¬ ì¶”ì¶œ
        chunks, metadata = processor._extract_chunks_from_file(test_file)
        if chunks:
            print(f"ğŸ“„ File contains {len(chunks)} chunks")
            
            # ê°œë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            individual_vectors = []
            for chunk in chunks:
                vector = embedding_model.get_embedding(chunk)
                individual_vectors.append(vector)
            individual_time = time.time() - start_time
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            batch_vectors = embedding_model.get_embeddings_batch_adaptive(chunks)
            batch_time = time.time() - start_time
            
            print(f"\nâš¡ Performance Comparison:")
            print(f"Individual processing: {individual_time:.2f}s")
            print(f"Batch processing: {batch_time:.2f}s")
            if batch_time > 0:
                speedup = individual_time / batch_time
                print(f"ğŸš€ Speedup: {speedup:.2f}x")
            
            # ë²¡í„° í’ˆì§ˆ í™•ì¸
            if len(batch_vectors) == len(individual_vectors):
                print(f"âœ… Vector count matches: {len(batch_vectors)}")
            else:
                print(f"âŒ Vector count mismatch: batch={len(batch_vectors)}, individual={len(individual_vectors)}")
        
    except Exception as e:
        print(f"âŒ Error during processing: {e}")
        import traceback
        traceback.print_exc()
    
    # 7. ì •ë¦¬
    try:
        os.remove(test_file)
        os.rmdir(test_dir)
        print(f"ğŸ§¹ Cleanup completed")
    except:
        pass

if __name__ == "__main__":
    test_real_file_processing()
