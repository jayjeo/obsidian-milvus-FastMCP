"""
embeddings.pyìš© ì†ë„ ìµœì í™” íŒ¨ì¹˜

ì‚¬ìš©ë²•:
1. ì´ íŒŒì¼ì„ embeddings.pyì™€ ê°™ì€ í´ë”ì— ì €ì¥
2. embeddings.pyì— ë‹¤ìŒ ì½”ë“œ ì¶”ê°€:

from embedding_patch import get_embeddings_batch_optimized
EmbeddingModel.get_embeddings_batch_optimized = get_embeddings_batch_optimized
"""

import torch
import gc
import numpy as np
from tqdm import tqdm

def get_embeddings_batch_optimized(self, texts, batch_size=None):
    """ìµœì í™”ëœ ê³ ì† ë°°ì¹˜ ì„ë² ë”©"""
    if not texts:
        return []
    
    # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    if batch_size is None:
        batch_size = getattr(self, 'optimal_batch_size', 2000)
    
    print(f"ğŸš€ ê³ ì† ë°°ì¹˜ ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    clean_texts = []
    for text in texts:
        if isinstance(text, str) and text.strip():
            if len(text) > 8000:
                text = text[:8000]  # ê¸¸ì´ ì œí•œ
            clean_texts.append(text)
        else:
            clean_texts.append("empty")
    
    embeddings = []
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    if hasattr(torch, 'cuda') and torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ë°°ì¹˜ ì²˜ë¦¬
    with tqdm(total=len(clean_texts), desc="ì„ë² ë”© ì²˜ë¦¬") as pbar:
        for i in range(0, len(clean_texts), batch_size):
            batch_texts = clean_texts[i:i+batch_size]
            
            try:
                with torch.no_grad():
                    # SentenceTransformer ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
                    batch_embeddings = self.model.encode(
                        batch_texts,
                        batch_size=len(batch_texts),
                        show_progress_bar=False,  # tqdm ì‚¬ìš©
                        convert_to_tensor=False,
                        normalize_embeddings=True,
                        device=str(self.device) if hasattr(self, 'device') else None
                    )
                
                # ê²°ê³¼ ì²˜ë¦¬
                if isinstance(batch_embeddings, np.ndarray):
                    embeddings.extend(batch_embeddings.tolist())
                else:
                    embeddings.extend(batch_embeddings)
                
                pbar.update(len(batch_texts))
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (10ë°°ì¹˜ë§ˆë‹¤)
                if i > 0 and i % (batch_size * 10) == 0:
                    gc.collect()
                    if hasattr(torch, 'cuda') and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        
            except Exception as e:
                print(f"\nâš ï¸ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
                for text in batch_texts:
                    try:
                        vector = self.get_embedding(text)
                        embeddings.append(vector)
                    except:
                        # ê¸°ë³¸ ì°¨ì›ì˜ 0 ë²¡í„° ì¶”ê°€
                        embeddings.append([0] * 384)  # ê¸°ë³¸ ì°¨ì›
                pbar.update(len(batch_texts))
    
    print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„° ìƒì„±")
    return embeddings

# íŒ¨ì¹˜ ì ìš© í•¨ìˆ˜
def apply_embedding_patch():
    """ì„ë² ë”© íŒ¨ì¹˜ë¥¼ EmbeddingModelì— ì ìš©"""
    try:
        from embeddings import EmbeddingModel
        
        # ê¸°ì¡´ ë©”ì„œë“œ ë°±ì—…
        if not hasattr(EmbeddingModel, '_original_get_embeddings_batch'):
            EmbeddingModel._original_get_embeddings_batch = EmbeddingModel.get_embeddings_batch
        
        # ìµœì í™”ëœ ë©”ì„œë“œë¡œ êµì²´
        EmbeddingModel.get_embeddings_batch = get_embeddings_batch_optimized
        
        print("âœ… ì„ë² ë”© ì†ë„ íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    apply_embedding_patch()
